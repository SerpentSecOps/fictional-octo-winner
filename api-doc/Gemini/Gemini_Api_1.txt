
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: GEMINIX2SS9S6CL (sha256-54ded3e7e8257f0204a78c3a569c1f39084ff15c52bba80fbe78fcc547a232ea) | Title: Api-Key.Md]
[DocID: GEMINI192ZDUVYIX (sha256-73ac433aa2896b8c2947a7b52511df5394d652b4d0e4c5502189863ac7f5e76e) | Title: Audio.Md]
[DocID: GEMINI8TLX3AA1L (sha256-16a2f524e8595466cc5afc325dc328a1dc36057591131372401cc1099532350d) | Title: Batch-Api.Md]
[DocID: GEMINICED9JOS8H (sha256-1fd0404fabf16ac64244efa8786ea7204df4b6b9be5aa46a7942e1d82d5882d5) | Title: Billing.Md]
[DocID: GEMINI1YCWMSOV0T (sha256-b486583c670d78d3b8c6e2f089d6b5de205042e5036c21a6dc8618ae877ca485) | Title: Caching.Md]
--- END OF TOC ---

[START OF DOCUMENT: GEMINIX2SS9S6CL | Title: Api-Key.Md]

<br />

To use the Gemini API, you need an API key. This page outlines how to create and manage your keys in Google AI Studio as well as how to set up your environment to use them in your code.

## API Keys

An[API key](https://cloud.google.com/api-keys/docs/overview)is an encrypted string that you can use when calling Google Cloud APIs. You can create and manage all your Gemini API Keys from the[Google AI Studio](https://aistudio.google.com/app/apikey)**API Keys**page.

Once you have an API key, you have the following options to connect to the Gemini API:

- [Setting your API key as an environment variable](https://ai.google.dev/gemini-api/docs/api-key#set-api-env-var)
- [Providing your API key explicitly](https://ai.google.dev/gemini-api/docs/api-key#provide-api-key-explicitly)

For initial testing, you can hard code an API key, but this should only be temporary since it's not secure. You can find examples for hard coding the API key in[Providing API key explicitly](https://ai.google.dev/gemini-api/docs/api-key#provide-api-key-explicitly)section.

## Google Cloud projects

[Google Cloud projects](https://cloud.google.com/resource-manager/docs/creating-managing-projects)are fundamental to using Google Cloud services (such as the Gemini API), managing billing, and controlling collaborators and permissions. Google AI Studio provides a lightweight interface to your Google Cloud projects.

If you don't have any projects created yet, you must either create a new project or import one from Google Cloud into Google AI Studio. The**Projects** page in Google AI Studio will display all keys that have sufficient permission to use the Gemini API. Refer to the[import projects](https://ai.google.dev/gemini-api/docs/api-key#import-projects)section for instructions.

### Default project

For new users, after accepting Terms of Service, Google AI Studio creates a default Google Cloud Project and API Key, for ease of use. You can rename this project in Google AI Studio by navigating to**Projects** view in the**Dashboard** , clicking the 3 dots settings button next to a project and choosing**Rename project**. Existing users, or users who already have Google Cloud Accounts won't have a default project created.

## Import projects

Each Gemini API key is associated with a Google Cloud project. By default, Google AI Studio does not show all of your Cloud Projects. You must import the projects you want by searching for the name or project ID in the**Import Projects**dialog. To view a complete list of projects you have access to, visit the Cloud Console.

If you don't have any projects imported yet, follow these steps to import a Google Cloud project and create a key:

1. Go to[Google AI Studio](https://aistudio.google.com).
2. Open the**Dashboard**from the left side panel.
3. Select**Projects**.
4. Select the**Import projects** button in the**Projects**page.
5. Search for and select the Google Cloud project you want to import and select the**Import**button.

Once a project is imported, go to the**API Keys** page from the**Dashboard**menu and create an API key in the project you just imported.
| **Note:** For existing users, the keys are pre-populated in the imports pane based on the last 30-days of activity in AI Studio.

## Limitations

The following are limitations of managing API keys and Google Cloud projects in Google AI Studio.

- You can create a maximum of 10 project at a time from the Google AI Studio**Projects**page.
- You can name and rename projects and keys.
- The**API keys** and**Projects**pages display a maximum of 100 keys and 50 projects.
- Only API keys that have no restrictions, or are restricted to the Generative Language API are displayed.

For additional management access to your projects, visit the Google Cloud Console.

## Setting the API key as an environment variable

If you set the environment variable`GEMINI_API_KEY`or`GOOGLE_API_KEY`, the API key will automatically be picked up by the client when using one of the[Gemini API libraries](https://ai.google.dev/gemini-api/docs/libraries). It's recommended that you set only one of those variables, but if both are set,`GOOGLE_API_KEY`takes precedence.

If you're using the REST API, or JavaScript on the browser, you will need to provide the API key explicitly.

Here is how you can set your API key locally as the environment variable`GEMINI_API_KEY`with different operating systems.

### Linux/macOS - Bash

Bash is a common Linux and macOS terminal configuration. You can check if you have a configuration file for it by running the following command:

 ~/.bashrc

If the response is "No such file or directory", you will need to create this file and open it by running the following commands, or use`zsh`:

 touch ~/.bashrc
 open ~/.bashrc

Next, you need to set your API key by adding the following export command:

 export GEMINI_API_KEY=<YOUR_API_KEY_HERE>

After saving the file, apply the changes by running:

 source ~/.bashrc

### macOS - Zsh

Zsh is a common Linux and macOS terminal configuration. You can check if you have a configuration file for it by running the following command:

 ~/.zshrc

If the response is "No such file or directory", you will need to create this file and open it by running the following commands, or use`bash`:

 touch ~/.zshrc
 open ~/.zshrc

Next, you need to set your API key by adding the following export command:

 export GEMINI_API_KEY=<YOUR_API_KEY_HERE>

After saving the file, apply the changes by running:

 source ~/.zshrc

### Windows

1. Search for "Environment Variables" in the search bar.
2. Choose to modify**System Settings**. You may have to confirm you want to do this.
3. In the system settings dialog, click the button labeled**Environment Variables**.
4. Under either**User variables** (for the current user) or**System variables** (applies to all users who use the machine), click**New...**
5. Specify the variable name as`GEMINI_API_KEY`. Specify your Gemini API Key as the variable value.
6. Click**OK**to apply the changes.
7. Open a new terminal session (cmd or Powershell) to get the new variable.

## Providing the API key explicitly

In some cases, you may want to explicitly provide an API key. For example:

- You're doing a simple API call and prefer hard coding the API key.
- You want explicit control without having to rely on automatic discovery of environment variables by the Gemini API libraries
- You're using an environment where environment variables are not supported (e.g web) or you are making REST calls.

Below are examples for how you can provide an API key explicitly:

### Python

 from google import genai

 client = genai.Client(api_key="<var translate="no">YOUR_API_KEY</var>")

 response = client.models.generate_content(
 model="gemini-2.5-flash", contents="Explain how AI works in a few words"
 )
 print(response.text)

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const ai = new GoogleGenAI({ apiKey: "<var translate="no">YOUR_API_KEY</var>" });

 async function main() {
 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: "Explain how AI works in a few words",
 });
 console.log(response.text);
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "log"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, &genai.ClientConfig{
 APIKey: "<var translate="no">YOUR_API_KEY</var>",
 Backend: genai.BackendGeminiAPI,
 })
 if err != nil {
 log.Fatal(err)
 }

 result, err := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 genai.Text("Explain how AI works in a few words"),
 nil,
 )
 if err != nil {
 log.Fatal(err)
 }
 fmt.Println(result.Text())
 }

### Java

 package com.example;

 import com.google.genai.Client;
 import com.google.genai.types.GenerateContentResponse;

 public class GenerateTextFromTextInput {
 public static void main(String[] args) {
 Client client = Client.builder().apiKey("<var translate="no">YOUR_API_KEY</var>").build();

 GenerateContentResponse response =
 client.models.generateContent(
 "gemini-2.5-flash",
 "Explain how AI works in a few words",
 null);

 System.out.println(response.text());
 }
 }

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H 'Content-Type: application/json' \
 -H "x-goog-api-key: <var translate="no">YOUR_API_KEY</var>" \
 -X POST \
 -d '{
 "contents": [
 {
 "parts": [
 {
 "text": "Explain how AI works in a few words"
 }
 ]
 }
 ]
 }'

## Keep your API key secure

Treat your Gemini API key like a password. If compromised, others can use your project's quota, incur charges (if billing is enabled), and access your private data, such as files.

### Critical security rules

- **Never commit API keys to source control.**Do not check your API key into version control systems like Git.

- **Never expose API keys on the client-side.**Do not use your API key directly in web or mobile apps in production. Keys in client-side code (including our JavaScript/TypeScript libraries and REST calls) can be extracted.

### Best practices

- **Use server-side calls with API keys**The most secure way to use your API key is to call the Gemini API from a server-side application where the key can be kept confidential.

- **Use ephemeral tokens for client-side access (Live API only):** For direct client-side access to the Live API, you can use ephemeral tokens. They come with lower security risks and can be suitable for production use. Review[ephemeral tokens](https://ai.google.dev/gemini-api/docs/ephemeral-tokens)guide for more information.

- **Consider adding restrictions to your key:** You can limit a key's permissions by adding[API key restrictions](https://cloud.google.com/api-keys/docs/add-restrictions-api-keys#add-api-restrictions). This minimizes the potential damage if the key is ever leaked.

For some general best practices, you can also review this[support article](https://support.google.com/googleapi/answer/6310037).

[END OF DOCUMENT: GEMINIX2SS9S6CL]
---

[START OF DOCUMENT: GEMINI192ZDUVYIX | Title: Audio.Md]

Gemini can analyze and understand audio input, enabling use cases like the
following:

- Describe, summarize, or answer questions about audio content.
- Provide a transcription of the audio.
- Analyze specific segments of the audio.

This guide shows you how to use the Gemini API to generate a text response to
audio input.

### Before you begin

Before calling the Gemini API, ensure you have [your SDK of choice](https://ai.google.dev/gemini-api/docs/downloads)
installed, and a [Gemini API key](https://ai.google.dev/gemini-api/docs/api-key) configured and ready to use.

## Input audio

You can provide audio data to Gemini in the following ways:

- [Upload an audio file](https://ai.google.dev/gemini-api/docs/audio#upload-audio) before making a request to `generateContent`.
- [Pass inline audio data](https://ai.google.dev/gemini-api/docs/audio#inline-audio) with the request to `generateContent`.

### Upload an audio file

You can use the [Files API](https://ai.google.dev/gemini-api/docs/files) to upload an audio file.
Always use the Files API when the total request size (including the files, text
prompt, system instructions, etc.) is larger than 20 MB.

The following code uploads an audio file and then uses the file in a call to
`generateContent`.

### Python

 from google import genai

 client = genai.Client()

 myfile = client.files.upload(file="path/to/sample.mp3")

 response = client.models.generate_content(
 model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
 )

 print(response.text)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";

 const ai = new GoogleGenAI({});

 async function main() {
 const myfile = await ai.files.upload({
 file: "path/to/sample.mp3",
 config: { mimeType: "audio/mp3" },
 });

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: createUserContent([
 createPartFromUri(myfile.uri, myfile.mimeType),
 "Describe this audio clip",
 ]),
 });
 console.log(response.text);
 }

 await main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 localAudioPath := "/path/to/sample.mp3"
 uploadedFile, _ := client.Files.UploadFromPath(
 ctx,
 localAudioPath,
 nil,
 )

 parts := []*genai.Part{
 genai.NewPartFromText("Describe this audio clip"),
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }
 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

### REST

 AUDIO_PATH="path/to/sample.mp3"
 MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
 NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
 DISPLAY_NAME=AUDIO

 tmp_header_file=upload-header.tmp

 # Initial resumable request defining metadata.
 # The upload url is in the response headers dump them to a file.
 curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -D upload-header.tmp \
 -H "X-Goog-Upload-Protocol: resumable" \
 -H "X-Goog-Upload-Command: start" \
 -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
 -H "Content-Type: application/json" \
 -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

 upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
 rm "${tmp_header_file}"

 # Upload the actual bytes.
 curl "${upload_url}" \
 -H "Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Offset: 0" \
 -H "X-Goog-Upload-Command: upload, finalize" \
 --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

 file_uri=$(jq ".file.uri" file_info.json)
 echo file_uri=$file_uri

 # Now generate content using that file
 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {"text": "Describe this audio clip"},
 {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
 }]
 }' 2> /dev/null > response.json

 cat response.json
 echo

 jq ".candidates[].content.parts[].text" response.json

To learn more about working with media files, see
[Files API](https://ai.google.dev/gemini-api/docs/files).

### Pass audio data inline

Instead of uploading an audio file, you can pass inline audio data in the
request to `generateContent`:

### Python

 from google import genai
 from google.genai import types

 with open('path/to/small-sample.mp3', 'rb') as f:
 audio_bytes = f.read()

 client = genai.Client()
 response = client.models.generate_content(
 model='gemini-2.5-flash',
 contents=[
 'Describe this audio clip',
 types.Part.from_bytes(
 data=audio_bytes,
 mime_type='audio/mp3',
 )
 ]
 )

 print(response.text)

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 const ai = new GoogleGenAI({});
 const base64AudioFile = fs.readFileSync("path/to/small-sample.mp3", {
 encoding: "base64",
 });

 const contents = [
 { text: "Please summarize the audio." },
 {
 inlineData: {
 mimeType: "audio/mp3",
 data: base64AudioFile,
 },
 },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: contents,
 });
 console.log(response.text);

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 audioBytes, _ := os.ReadFile("/path/to/small-sample.mp3")

 parts := []*genai.Part{
 genai.NewPartFromText("Describe this audio clip"),
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "audio/mp3",
 Data: audioBytes,
 },
 },
 }
 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

A few things to keep in mind about inline audio data:

- The maximum request size is 20 MB, which includes text prompts, system instructions, and files provided inline. If your file's size will make the *total request size* exceed 20 MB, then use the Files API to [upload an audio file](https://ai.google.dev/gemini-api/docs/audio#upload-audio) for use in the request.
- If you're using an audio sample multiple times, it's more efficient to [upload an audio file](https://ai.google.dev/gemini-api/docs/audio#upload-audio).

## Get a transcript

To get a transcript of audio data, just ask for it in the prompt:

### Python

 from google import genai

 client = genai.Client()
 myfile = client.files.upload(file='path/to/sample.mp3')
 prompt = 'Generate a transcript of the speech.'

 response = client.models.generate_content(
 model='gemini-2.5-flash',
 contents=[prompt, myfile]
 )

 print(response.text)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";

 const ai = new GoogleGenAI({});
 const myfile = await ai.files.upload({
 file: "path/to/sample.mp3",
 config: { mimeType: "audio/mpeg" },
 });

 const result = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: createUserContent([
 createPartFromUri(myfile.uri, myfile.mimeType),
 "Generate a transcript of the speech.",
 ]),
 });
 console.log("result.text=", result.text);

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 localAudioPath := "/path/to/sample.mp3"
 uploadedFile, _ := client.Files.UploadFromPath(
 ctx,
 localAudioPath,
 nil,
 )

 parts := []*genai.Part{
 genai.NewPartFromText("Generate a transcript of the speech."),
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }
 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

## Refer to timestamps

You can refer to specific sections of an audio file using timestamps of the form
`MM:SS`. For example, the following prompt requests a transcript that

- Starts at 2 minutes 30 seconds from the beginning of the file.
- Ends at 3 minutes 29 seconds from the beginning of the file.

### Python

 # Create a prompt containing timestamps.
 prompt = "Provide a transcript of the speech from 02:30 to 03:29."

### JavaScript

 // Create a prompt containing timestamps.
 const prompt = "Provide a transcript of the speech from 02:30 to 03:29."

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 localAudioPath := "/path/to/sample.mp3"
 uploadedFile, _ := client.Files.UploadFromPath(
 ctx,
 localAudioPath,
 nil,
 )

 parts := []*genai.Part{
 genai.NewPartFromText("Provide a transcript of the speech " +
 "between the timestamps 02:30 and 03:29."),
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }
 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

## Count tokens

Call the `countTokens` method to get a count of the number of tokens in an
audio file. For example:

### Python

 from google import genai

 client = genai.Client()
 response = client.models.count_tokens(
 model='gemini-2.5-flash',
 contents=[myfile]
 )

 print(response)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";

 const ai = new GoogleGenAI({});
 const myfile = await ai.files.upload({
 file: "path/to/sample.mp3",
 config: { mimeType: "audio/mpeg" },
 });

 const countTokensResponse = await ai.models.countTokens({
 model: "gemini-2.5-flash",
 contents: createUserContent([
 createPartFromUri(myfile.uri, myfile.mimeType),
 ]),
 });
 console.log(countTokensResponse.totalTokens);

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 localAudioPath := "/path/to/sample.mp3"
 uploadedFile, _ := client.Files.UploadFromPath(
 ctx,
 localAudioPath,
 nil,
 )

 parts := []*genai.Part{
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }
 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 tokens, _ := client.Models.CountTokens(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Printf("File %s is %d tokens\n", localAudioPath, tokens.TotalTokens)
 }

## Supported audio formats

Gemini supports the following audio format MIME types:

- WAV - `audio/wav`
- MP3 - `audio/mp3`
- AIFF - `audio/aiff`
- AAC - `audio/aac`
- OGG Vorbis - `audio/ogg`
- FLAC - `audio/flac`

## Technical details about audio

- Gemini represents each second of audio as 32 tokens; for example, one minute of audio is represented as 1,920 tokens.
- Gemini can "understand" non-speech components, such as birdsong or sirens.
- The maximum supported length of audio data in a single prompt is 9.5 hours. Gemini doesn't limit the *number* of audio files in a single prompt; however, the total combined length of all audio files in a single prompt can't exceed 9.5 hours.
- Gemini downsamples audio files to a 16 Kbps data resolution.
- If the audio source contains multiple channels, Gemini combines those channels into a single channel.

## What's next

This guide shows how to generate text in response to audio data. To learn more,
see the following resources:

- [File prompting strategies](https://ai.google.dev/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
- [System instructions](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
- [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

[END OF DOCUMENT: GEMINI192ZDUVYIX]
---

[START OF DOCUMENT: GEMINI8TLX3AA1L | Title: Batch-Api.Md]

The Gemini Batch API is designed to process large volumes of requests
asynchronously at [50% of the standard cost](https://ai.google.dev/gemini-api/docs/pricing).
The target turnaround time is 24 hours, but in majority of cases, it is much
quicker.

Use Batch API for large-scale, non-urgent tasks such as data
pre-processing or running evaluations where an immediate response is not
required.

## Creating a batch job

You have two ways to submit your requests in Batch API:

- **[Inline Requests](https://ai.google.dev/gemini-api/docs/batch-api#inline-requests):** A list of [`GenerateContentRequest`](https://ai.google.dev/api/batch-mode#GenerateContentRequest) objects directly included in your batch creation request. This is suitable for smaller batches that keep the total request size under 20MB. The **output** returned from the model is a list of `inlineResponse` objects.
- **[Input File](https://ai.google.dev/gemini-api/docs/batch-api#input-file):** A [JSON Lines (JSONL)](https://jsonlines.org/) file where each line contains a complete [`GenerateContentRequest`](https://ai.google.dev/api/batch-mode#GenerateContentRequest) object. This method is recommended for larger requests. The **output** returned from the model is a JSONL file where each line is either a `GenerateContentResponse` or a status object.

### Inline requests

For a small number of requests, you can directly embed the
[`GenerateContentRequest`](https://ai.google.dev/api/batch-mode#GenerateContentRequest) objects
within your [`BatchGenerateContentRequest`](https://ai.google.dev/api/batch-mode#request-body). The
following example calls the
[`BatchGenerateContent`](https://ai.google.dev/api/batch-mode#google.ai.generativelanguage.v1beta.BatchService.BatchGenerateContent)
method with inline requests:

### Python

 from google import genai
 from google.genai import types

 client = genai.Client()

 # A list of dictionaries, where each is a GenerateContentRequest
 inline_requests = [
 {
 'contents': [{
 'parts': [{'text': 'Tell me a one-sentence joke.'}],
 'role': 'user'
 }]
 },
 {
 'contents': [{
 'parts': [{'text': 'Why is the sky blue?'}],
 'role': 'user'
 }]
 }
 ]

 inline_batch_job = client.batches.create(
 model="models/gemini-2.5-flash",
 src=inline_requests,
 config={
 'display_name': "inlined-requests-job-1",
 },
 )

 print(f"Created batch job: {inline_batch_job.name}")

### JavaScript

 import {GoogleGenAI} from '@google/genai';
 const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

 const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});

 const inlinedRequests = [
 {
 contents: [{
 parts: [{text: 'Tell me a one-sentence joke.'}],
 role: 'user'
 }]
 },
 {
 contents: [{
 parts: [{'text': 'Why is the sky blue?'}],
 role: 'user'
 }]
 }
 ]

 const response = await ai.batches.create({
 model: 'gemini-2.5-flash',
 src: inlinedRequests,
 config: {
 displayName: 'inlined-requests-job-1',
 }
 });

 console.log(response);

### REST

 curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchGenerateContent \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -X POST \
 -H "Content-Type:application/json" \
 -d '{
 "batch": {
 "display_name": "my-batch-requests",
 "input_config": {
 "requests": {
 "requests": [
 {
 "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]},
 "metadata": {
 "key": "request-1"
 }
 },
 {
 "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]},
 "metadata": {
 "key": "request-2"
 }
 }
 ]
 }
 }
 }
 }'

### Input file

For larger sets of requests, prepare a JSON Lines (JSONL) file. Each line in
this file must be a JSON object containing a user-defined key and a request
object, where the request is a valid
[`GenerateContentRequest`](https://ai.google.dev/api/batch-mode#GenerateContentRequest) object. The
user-defined key is used in the response to indicate which output is the result
of which request. For example, the request with the key defined as `request-1`
will have its response annotated with the same key name.

This file is uploaded using the [File API](https://ai.google.dev/gemini-api/docs/files). The maximum
allowed file size for an input file is 2GB.

The following is an example of a JSONL file. You can save it in a file named
`my-batch-requests.json`:

 {"key": "request-1", "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}], "generation_config": {"temperature": 0.7}}}
 {"key": "request-2", "request": {"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}}

Similarly to inline requests, you can specify other parameters like system
instructions, tools or other configurations in each request JSON.

You can upload this file using the [File API](https://ai.google.dev/gemini-api/docs/files) as
shown in the following example. If
you are working with multimodal input, you can reference other uploaded files
within your JSONL file.

### Python

 import json
 from google import genai
 from google.genai import types

 client = genai.Client()

 # Create a sample JSONL file
 with open("my-batch-requests.jsonl", "w") as f:
 requests = [
 {"key": "request-1", "request": {"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}]}},
 {"key": "request-2", "request": {"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}}
 ]
 for req in requests:
 f.write(json.dumps(req) + "\n")

 # Upload the file to the File API
 uploaded_file = client.files.upload(
 file='my-batch-requests.jsonl',
 config=types.UploadFileConfig(display_name='my-batch-requests', mime_type='jsonl')
 )

 print(f"Uploaded file: {uploaded_file.name}")

### JavaScript

 import {GoogleGenAI} from '@google/genai';
 import * as fs from "fs";
 import * as path from "path";
 import { fileURLToPath } from 'url';

 const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
 const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});
 const fileName = "my-batch-requests.jsonl";

 // Define the requests
 const requests = [
 { "key": "request-1", "request": { "contents": [{ "parts": [{ "text": "Describe the process of photosynthesis." }] }] } },
 { "key": "request-2", "request": { "contents": [{ "parts": [{ "text": "What are the main ingredients in a Margherita pizza?" }] }] } }
 ];

 // Construct the full path to file
 const __filename = fileURLToPath(import.meta.url);
 const __dirname = path.dirname(__filename);
 const filePath = path.join(__dirname, fileName); // __dirname is the directory of the current script

 async function writeBatchRequestsToFile(requests, filePath) {
 try {
 // Use a writable stream for efficiency, especially with larger files.
 const writeStream = fs.createWriteStream(filePath, { flags: 'w' });

 writeStream.on('error', (err) => {
 console.error(`Error writing to file ${filePath}:`, err);
 });

 for (const req of requests) {
 writeStream.write(JSON.stringify(req) + '\n');
 }

 writeStream.end();

 console.log(`Successfully wrote batch requests to ${filePath}`);

 } catch (error) {
 // This catch block is for errors that might occur before stream setup,
 // stream errors are handled by the 'error' event.
 console.error(`An unexpected error occurred:`, error);
 }
 }

 // Write to a file.
 writeBatchRequestsToFile(requests, filePath);

 // Upload the file to the File API.
 const uploadedFile = await ai.files.upload({file: 'my-batch-requests.jsonl', config: {
 mimeType: 'jsonl',
 }});
 console.log(uploadedFile.name);

### REST

 tmp_batch_input_file=batch_input.tmp
 echo -e '{"contents": [{"parts": [{"text": "Describe the process of photosynthesis."}]}], "generationConfig": {"temperature": 0.7}}\n{"contents": [{"parts": [{"text": "What are the main ingredients in a Margherita pizza?"}]}]}' > batch_input.tmp
 MIME_TYPE=$(file -b --mime-type "${tmp_batch_input_file}")
 NUM_BYTES=$(wc -c < "${tmp_batch_input_file}")
 DISPLAY_NAME=BatchInput

 tmp_header_file=upload-header.tmp

 # Initial resumable request defining metadata.
 # The upload url is in the response headers dump them to a file.
 curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
 -D "${tmp_header_file}" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "X-Goog-Upload-Protocol: resumable" \
 -H "X-Goog-Upload-Command: start" \
 -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
 -H "Content-Type: application/jsonl" \
 -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

 upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
 rm "${tmp_header_file}"

 # Upload the actual bytes.
 curl "${upload_url}" \
 -H "Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Offset: 0" \
 -H "X-Goog-Upload-Command: upload, finalize" \
 --data-binary "@${tmp_batch_input_file}" 2> /dev/null > file_info.json

 file_uri=$(jq ".file.uri" file_info.json)

The following example calls the
[`BatchGenerateContent`](https://ai.google.dev/api/batch-mode#google.ai.generativelanguage.v1beta.BatchService.BatchGenerateContent)
method with the input file uploaded using File API:

### Python

 from google import genai

 # Assumes `uploaded_file` is the file object from the previous step
 client = genai.Client()
 file_batch_job = client.batches.create(
 model="gemini-2.5-flash",
 src=uploaded_file.name,
 config={
 'display_name': "file-upload-job-1",
 },
 )

 print(f"Created batch job: {file_batch_job.name}")

### JavaScript

 // Assumes `uploadedFile` is the file object from the previous step
 const fileBatchJob = await ai.batches.create({
 model: 'gemini-2.5-flash',
 src: uploadedFile.name,
 config: {
 displayName: 'file-upload-job-1',
 }
 });

 console.log(fileBatchJob);

### REST

 # Set the File ID taken from the upload response.
 BATCH_INPUT_FILE='files/123456'
 curl https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:batchGenerateContent \
 -X POST \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type:application/json" \
 -d "{
 'batch': {
 'display_name': 'my-batch-requests',
 'input_config': {
 'file_name': '${BATCH_INPUT_FILE}'
 }
 }
 }"

When you create a batch job, you will get a job name returned. Use this name
for [monitoring](https://ai.google.dev/gemini-api/docs/batch-api#batch-job-status) the job status as well as
[retrieving the results](https://ai.google.dev/gemini-api/docs/batch-api#retrieve-batch-results) once the job completes.

The following is an example output that contains a job name:

 Created batch job from file: batches/123456789

### Batch embedding support

You can use the Batch API to interact with the
[Embeddings model](https://ai.google.dev/gemini-api/docs/embeddings) for higher throughput.
To create an embeddings batch job with either [inline requests](https://ai.google.dev/gemini-api/docs/batch-api#inline-requests)
or [input files](https://ai.google.dev/gemini-api/docs/batch-api#input-file), use the `batches.create_embeddings` API and
specify the embeddings model.

### Python

 from google import genai

 client = genai.Client()

 # Creating an embeddings batch job with an input file request:
 file_job = client.batches.create_embeddings(
 model="gemini-embedding-001",
 src={'file_name': uploaded_batch_requests.name},
 config={'display_name': "Input embeddings batch"},
 )

 # Creating an embeddings batch job with an inline request:
 batch_job = client.batches.create_embeddings(
 model="gemini-embedding-001",
 # For a predefined list of requests `inlined_requests`
 src={'inlined_requests': inlined_requests},
 config={'display_name': "Inlined embeddings batch"},
 )

### JavaScript

 // Creating an embeddings batch job with an input file request:
 let fileJob;
 fileJob = await client.batches.createEmbeddings({
 model: 'gemini-embedding-001',
 src: {fileName: uploadedBatchRequests.name},
 config: {displayName: 'Input embeddings batch'},
 });
 console.log(`Created batch job: ${fileJob.name}`);

 // Creating an embeddings batch job with an inline request:
 let batchJob;
 batchJob = await client.batches.createEmbeddings({
 model: 'gemini-embedding-001',
 // For a predefined a list of requests `inlinedRequests`
 src: {inlinedRequests: inlinedRequests},
 config: {displayName: 'Inlined embeddings batch'},
 });
 console.log(`Created batch job: ${batchJob.name}`);

Read the Embeddings section in the [Batch API cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Batch_mode.ipynb)
for more examples.

### Request configuration

You can include any request configurations you would use in a standard non-batch
request. For example, you could specify the temperature, system instructions or
even pass in other modalities. The following example shows an example inline
request that contains a system instruction for one of the requests:

### Python

 inline_requests_list = [
 {'contents': [{'parts': [{'text': 'Write a short poem about a cloud.'}]}]},
 {'contents': [{'parts': [{'text': 'Write a short poem about a cat.'}]}], 'system_instructions': {'parts': [{'text': 'You are a cat. Your name is Neko.'}]}}
 ]

### JavaScript

 inlineRequestsList = [
 {contents: [{parts: [{text: 'Write a short poem about a cloud.'}]}]},
 {contents: [{parts: [{text: 'Write a short poem about a cat.'}]}], systemInstructions: {parts: [{text: 'You are a cat. Your name is Neko.'}]}}
 ]

Similarly can specify tools to use for a request. The following example
shows a request that enables the [Google Search tool](https://ai.google.dev/gemini-api/docs/google-search):

### Python

 inline_requests_list = [
 {'contents': [{'parts': [{'text': 'Who won the euro 1998?'}]}]},
 {'contents': [{'parts': [{'text': 'Who won the euro 2025?'}]}], 'tools': [{'google_search ': {}}]}
 ]

### JavaScript

 inlineRequestsList = [
 {contents: [{parts: [{text: 'Who won the euro 1998?'}]}]},
 {contents: [{parts: [{text: 'Who won the euro 2025?'}]}], tools: [{googleSearch: {}}]}
 ]

You can specify [structured output](https://ai.google.dev/gemini-api/docs/structured-output) as well.
The following example shows how to specify for your batch requests.

### Python

 import time
 from google import genai
 from pydantic import BaseModel, TypeAdapter

 class Recipe(BaseModel):
 recipe_name: str
 ingredients: list[str]

 client = genai.Client()

 # A list of dictionaries, where each is a GenerateContentRequest
 inline_requests = [
 {
 'contents': [{
 'parts': [{'text': 'List a few popular cookie recipes, and include the amounts of ingredients.'}],
 'role': 'user'
 }],
 'config': {
 'response_mime_type': 'application/json',
 'response_schema': list[Recipe]
 }
 },
 {
 'contents': [{
 'parts': [{'text': 'List a few popular gluten free cookie recipes, and include the amounts of ingredients.'}],
 'role': 'user'
 }],
 'config': {
 'response_mime_type': 'application/json',
 'response_schema': list[Recipe]
 }
 }
 ]

 inline_batch_job = client.batches.create(
 model="models/gemini-2.5-flash",
 src=inline_requests,
 config={
 'display_name': "structured-output-job-1"
 },
 )

 # wait for the job to finish
 job_name = inline_batch_job.name
 print(f"Polling status for job: {job_name}")

 while True:
 batch_job_inline = client.batches.get(name=job_name)
 if batch_job_inline.state.name in ('JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED', 'JOB_STATE_CANCELLED', 'JOB_STATE_EXPIRED'):
 break
 print(f"Job not finished. Current state: {batch_job_inline.state.name}. Waiting 30 seconds...")
 time.sleep(30)

 print(f"Job finished with state: {batch_job_inline.state.name}")

 # print the response
 for i, inline_response in enumerate(batch_job_inline.dest.inlined_responses, start=1):
 print(f"\n--- Response {i} ---")

 # Check for a successful response
 if inline_response.response:
 # The .text property is a shortcut to the generated text.
 print(inline_response.response.text)

### JavaScript

 import {GoogleGenAI, Type} from '@google/genai';
 const GEMINI_API_KEY = process.env.GEMINI_API_KEY;

 const ai = new GoogleGenAI({apiKey: GEMINI_API_KEY});

 const inlinedRequests = [
 {
 contents: [{
 parts: [{text: 'List a few popular cookie recipes, and include the amounts of ingredients.'}],
 role: 'user'
 }],
 config: {
 responseMimeType: 'application/json',
 responseSchema: {
 type: Type.ARRAY,
 items: {
 type: Type.OBJECT,
 properties: {
 'recipeName': {
 type: Type.STRING,
 description: 'Name of the recipe',
 nullable: false,
 },
 'ingredients': {
 type: Type.ARRAY,
 items: {
 type: Type.STRING,
 description: 'Ingredients of the recipe',
 nullable: false,
 },
 },
 },
 required: ['recipeName'],
 },
 },
 }
 },
 {
 contents: [{
 parts: [{text: 'List a few popular gluten free cookie recipes, and include the amounts of ingredients.'}],
 role: 'user'
 }],
 config: {
 responseMimeType: 'application/json',
 responseSchema: {
 type: Type.ARRAY,
 items: {
 type: Type.OBJECT,
 properties: {
 'recipeName': {
 type: Type.STRING,
 description: 'Name of the recipe',
 nullable: false,
 },
 'ingredients': {
 type: Type.ARRAY,
 items: {
 type: Type.STRING,
 description: 'Ingredients of the recipe',
 nullable: false,
 },
 },
 },
 required: ['recipeName'],
 },
 },
 }
 }
 ]

 const inlinedBatchJob = await ai.batches.create({
 model: 'gemini-2.5-flash',
 src: inlinedRequests,
 config: {
 displayName: 'inlined-requests-job-1',
 }
 });

## Monitoring job status

Use the operation name obtained when creating the batch job to poll its status.
The state field of the batch job will indicate its current status. A batch job
can be in one of the following states:

- `JOB_STATE_PENDING`: The job has been created and is waiting to be processed by the service.
- `JOB_STATE_RUNNING`: The job is in progress.
- `JOB_STATE_SUCCEEDED`: The job completed successfully. You can now retrieve the results.
- `JOB_STATE_FAILED`: The job failed. Check the error details for more information.
- `JOB_STATE_CANCELLED`: The job was cancelled by the user.
- `JOB_STATE_EXPIRED`: The job has expired because it was running or pending for more than 48 hours. The job will not have any results to retrieve. You can try submitting the job again or splitting up the requests into smaller batches.

You can poll the job status periodically to check for completion.

### Python

 import time
 from google import genai

 client = genai.Client()

 # Use the name of the job you want to check
 # e.g., inline_batch_job.name from the previous step
 job_name = "YOUR_BATCH_JOB_NAME" # (e.g. 'batches/your-batch-id')
 batch_job = client.batches.get(name=job_name)

 completed_states = set([
 'JOB_STATE_SUCCEEDED',
 'JOB_STATE_FAILED',
 'JOB_STATE_CANCELLED',
 'JOB_STATE_EXPIRED',
 ])

 print(f"Polling status for job: {job_name}")
 batch_job = client.batches.get(name=job_name) # Initial get
 while batch_job.state.name not in completed_states:
 print(f"Current state: {batch_job.state.name}")
 time.sleep(30) # Wait for 30 seconds before polling again
 batch_job = client.batches.get(name=job_name)

 print(f"Job finished with state: {batch_job.state.name}")
 if batch_job.state.name == 'JOB_STATE_FAILED':
 print(f"Error: {batch_job.error}")

### JavaScript

 // Use the name of the job you want to check
 // e.g., inlinedBatchJob.name from the previous step
 let batchJob;
 const completedStates = new Set([
 'JOB_STATE_SUCCEEDED',
 'JOB_STATE_FAILED',
 'JOB_STATE_CANCELLED',
 'JOB_STATE_EXPIRED',
 ]);

 try {
 batchJob = await ai.batches.get({name: inlinedBatchJob.name});
 while (!completedStates.has(batchJob.state)) {
 console.log(`Current state: ${batchJob.state}`);
 // Wait for 30 seconds before polling again
 await new Promise(resolve => setTimeout(resolve, 30000));
 batchJob = await client.batches.get({ name: batchJob.name });
 }
 console.log(`Job finished with state: ${batchJob.state}`);
 if (batchJob.state === 'JOB_STATE_FAILED') {
 // The exact structure of `error` might vary depending on the SDK
 // This assumes `error` is an object with a `message` property.
 console.error(`Error: ${batchJob.state}`);
 }
 } catch (error) {
 console.error(`An error occurred while polling job ${batchJob.name}:`, error);
 }

## Retrieving results

Once the job status indicates your batch job has succeeded, the results are
available in the `response` field.

### Python

 import json
 from google import genai

 client = genai.Client()

 # Use the name of the job you want to check
 # e.g., inline_batch_job.name from the previous step
 job_name = "YOUR_BATCH_JOB_NAME"
 batch_job = client.batches.get(name=job_name)

 if batch_job.state.name == 'JOB_STATE_SUCCEEDED':

 # If batch job was created with a file
 if batch_job.dest and batch_job.dest.file_name:
 # Results are in a file
 result_file_name = batch_job.dest.file_name
 print(f"Results are in file: {result_file_name}")

 print("Downloading result file content...")
 file_content = client.files.download(file=result_file_name)
 # Process file_content (bytes) as needed
 print(file_content.decode('utf-8'))

 # If batch job was created with inline request
 # (for embeddings, use batch_job.dest.inlined_embed_content_responses)
 elif batch_job.dest and batch_job.dest.inlined_responses:
 # Results are inline
 print("Results are inline:")
 for i, inline_response in enumerate(batch_job.dest.inlined_responses):
 print(f"Response {i+1}:")
 if inline_response.response:
 # Accessing response, structure may vary.
 try:
 print(inline_response.response.text)
 except AttributeError:
 print(inline_response.response) # Fallback
 elif inline_response.error:
 print(f"Error: {inline_response.error}")
 else:
 print("No results found (neither file nor inline).")
 else:
 print(f"Job did not succeed. Final state: {batch_job.state.name}")
 if batch_job.error:
 print(f"Error: {batch_job.error}")

### JavaScript

 // Use the name of the job you want to check
 // e.g., inlinedBatchJob.name from the previous step
 const jobName = "YOUR_BATCH_JOB_NAME";

 try {
 const batchJob = await ai.batches.get({ name: jobName });

 if (batchJob.state === 'JOB_STATE_SUCCEEDED') {
 console.log('Found completed batch:', batchJob.displayName);
 console.log(batchJob);

 // If batch job was created with a file destination
 if (batchJob.dest?.fileName) {
 const resultFileName = batchJob.dest.fileName;
 console.log(`Results are in file: ${resultFileName}`);

 console.log("Downloading result file content...");
 const fileContentBuffer = await ai.files.download({ file: resultFileName });

 // Process fileContentBuffer (Buffer) as needed
 console.log(fileContentBuffer.toString('utf-8'));
 }

 // If batch job was created with inline responses
 else if (batchJob.dest?.inlinedResponses) {
 console.log("Results are inline:");
 for (let i = 0; i < batchJob.dest.inlinedResponses.length; i++) {
 const inlineResponse = batchJob.dest.inlinedResponses[i];
 console.log(`Response ${i + 1}:`);
 if (inlineResponse.response) {
 // Accessing response, structure may vary.
 if (inlineResponse.response.text !== undefined) {
 console.log(inlineResponse.response.text);
 } else {
 console.log(inlineResponse.response); // Fallback
 }
 } else if (inlineResponse.error) {
 console.error(`Error: ${inlineResponse.error}`);
 }
 }
 }

 // If batch job was an embedding batch with inline responses
 else if (batchJob.dest?.inlinedEmbedContentResponses) {
 console.log("Embedding results found inline:");
 for (let i = 0; i < batchJob.dest.inlinedEmbedContentResponses.length; i++) {
 const inlineResponse = batchJob.dest.inlinedEmbedContentResponses[i];
 console.log(`Response ${i + 1}:`);
 if (inlineResponse.response) {
 console.log(inlineResponse.response);
 } else if (inlineResponse.error) {
 console.error(`Error: ${inlineResponse.error}`);
 }
 }
 } else {
 console.log("No results found (neither file nor inline).");
 }
 } else {
 console.log(`Job did not succeed. Final state: ${batchJob.state}`);
 if (batchJob.error) {
 console.error(`Error: ${typeof batchJob.error === 'string' ? batchJob.error : batchJob.error.message || JSON.stringify(batchJob.error)}`);
 }
 }
 } catch (error) {
 console.error(`An error occurred while processing job ${jobName}:`, error);
 }

### REST

 BATCH_NAME="batches/123456" # Your batch job name

 curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type:application/json" 2> /dev/null > batch_status.json

 if jq -r '.done' batch_status.json | grep -q "false"; then
 echo "Batch has not finished processing"
 fi

 batch_state=$(jq -r '.metadata.state' batch_status.json)
 if [[ $batch_state = "JOB_STATE_SUCCEEDED" ]]; then
 if [[ $(jq '.response | has("inlinedResponses")' batch_status.json) = "true" ]]; then
 jq -r '.response.inlinedResponses' batch_status.json
 exit
 fi
 responses_file_name=$(jq -r '.response.responsesFile' batch_status.json)
 curl https://generativelanguage.googleapis.com/download/v1beta/$responses_file_name:download?alt=media \
 -H "x-goog-api-key: $GEMINI_API_KEY" 2> /dev/null
 elif [[ $batch_state = "JOB_STATE_FAILED" ]]; then
 jq '.error' batch_status.json
 elif [[ $batch_state == "JOB_STATE_CANCELLED" ]]; then
 echo "Batch was cancelled by the user"
 elif [[ $batch_state == "JOB_STATE_EXPIRED" ]]; then
 echo "Batch expired after 48 hours"
 fi

## Cancelling a batch job

You can cancel an ongoing batch job using its name. When a job is
canceled, it stops processing new requests.

### Python

 from google import genai

 client = genai.Client()

 # Cancel a batch job
 client.batches.cancel(name=batch_job_to_cancel.name)

### JavaScript

 await ai.batches.cancel({name: batchJobToCancel.name});

### REST

 BATCH_NAME="batches/123456" # Your batch job name

 # Cancel the batch
 curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME:cancel \
 -H "x-goog-api-key: $GEMINI_API_KEY" \

 # Confirm that the status of the batch after cancellation is JOB_STATE_CANCELLED
 curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type:application/json" 2> /dev/null | jq -r '.metadata.state'

## Deleting a batch job

You can delete an existing batch job using its name. When a job is
deleted, it stops processing new requests and is removed from the list of
batch jobs.

### Python

 from google import genai

 client = genai.Client()

 # Delete a batch job
 client.batches.delete(name=batch_job_to_delete.name)

### JavaScript

 await ai.batches.delete({name: batchJobToDelete.name});

### REST

 BATCH_NAME="batches/123456" # Your batch job name

 # Delete the batch job
 curl https://generativelanguage.googleapis.com/v1beta/$BATCH_NAME:delete \
 -H "x-goog-api-key: $GEMINI_API_KEY"

## Technical details

- **Supported models:** Batch API supports a range of Gemini models. Refer to the [Models page](https://ai.google.dev/gemini-api/docs/models) for each model's support of Batch API. The supported modalities for Batch API are the same as what's supported on the interactive (or non-batch) API.
- **Pricing:** Batch API usage is priced at 50% of the standard interactive API cost for the equivalent model. See the [pricing page](https://ai.google.dev/gemini-api/docs/pricing) for details. Refer to the [rate limits page](https://ai.google.dev/gemini-api/docs/rate-limits#batch-mode) for details on rate limits for this feature.
- **Service Level Objective (SLO):** Batch jobs are designed to complete within a 24-hour turnaround time. Many jobs may complete much faster depending on their size and current system load.
- **Caching:** [Context caching](https://ai.google.dev/gemini-api/docs/caching) is enabled for batch requests. If a request in your batch results in a cache hit, the cached tokens are priced the same as for non-batch API traffic.

## Best practices

- **Use input files for large requests:** For a large number of requests, always use the file input method for better manageability and to avoid hitting request size limits for the [`BatchGenerateContent`](https://ai.google.dev/api/batch-mode#google.ai.generativelanguage.v1beta.BatchService.BatchGenerateContent) call itself. Note that there's a the 2GB file size limit per input file.
- **Error handling:** Check the `batchStats` for `failedRequestCount` after a job completes. If using file output, parse each line to check if it's a `GenerateContentResponse` or a status object indicating an error for that specific request. See the [troubleshooting
 guide](https://ai.google.dev/gemini-api/docs/troubleshooting#error-codes) for a complete set of error codes.
- **Submit jobs once:** The creation of a batch job is not idempotent. If you send the same creation request twice, two separate batch jobs will be created.
- **Break up very large batches:** While the target turnaround time is 24 hours, actual processing time can vary based on system load and job size. For large jobs, consider breaking them into smaller batches if intermediate results are needed sooner.

## What's next

- Check out the [Batch API notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Batch_mode.ipynb) for more examples.
- The OpenAI compatibility layer supports Batch API. Read the examples on the [OpenAI Compatibility](https://ai.google.dev/gemini-api/docs/openai#batch) page.

[END OF DOCUMENT: GEMINI8TLX3AA1L]
---

[START OF DOCUMENT: GEMINICED9JOS8H | Title: Billing.Md]

This guide provides an overview of different Gemini API billing options,
explains how to enable billing and monitor usage, and provides answers to
frequently asked questions (FAQs) about billing.
[Upgrade to the Gemini API paid tier](https://aistudio.google.com/api-keys)

## About billing

Billing for the Gemini API is based on two pricing tiers: *free of charge*
(or *free* ) and *pay-as-you-go* (or *paid* ). Pricing and rate limits differ
between these tiers and also vary by model. You can check out the [rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)
and [pricing](https://ai.google.dev/gemini-api/docs/pricing) pages for more into. For a model-by-model
breakdown of capabilities, see the [Gemini models page](https://ai.google.dev/gemini-api/docs/models/gemini).

#### How to request an upgrade

To transition from the free tier to the pay-as-you-go plan, you need to
enable billing for your Google Cloud project. The button you see in
Google AI Studio depends on your project's current plan.

- If you're on the free tier, you'll see a **Set up Billing** button for your project.
- If you're already on the paid tier and meet the criteria for a plan change, you might see an **Upgrade** button.

To start the process, follow these steps:

1. Go to the [AI Studio API keys page](https://aistudio.google.com/api-keys).
2. Find the project you want to move to the paid plan and click either **Set up Billing** or **Upgrade**, depending on the button displayed.
3. The next step depends on the button you clicked:
 - **If you clicked Set up Billing:** You'll be redirected to the Google Cloud console to link a billing account to your project. Follow the on-screen instructions to complete the process.
 - **If you clicked Upgrade:** The system will automatically verify your project's eligibility. If your project meets all the requirements, it will be instantly upgraded to the next tier.

### Why use the paid tier?

When you enable billing and use the paid tier, you benefit from [higher rate limits](https://ai.google.dev/gemini-api/docs/rate-limits),
and your prompts and responses aren't used to improve Google products.
For more information on data use for paid services, see the
[terms of service](https://ai.google.dev/gemini-api/terms#data-use-paid).

### Cloud Billing

The Gemini API uses
[Cloud Billing](https://cloud.google.com/billing/docs/concepts)
for billing services. To use the paid tier, you must set up Cloud Billing on
your cloud project. After you've enabled Cloud Billing, you can use Cloud
Billing tools to track spending, understand costs, make payments, and access
Cloud Billing support.

## Enable billing

You can enable Cloud Billing starting from Google AI Studio:

1. Open [Google AI Studio](https://aistudio.google.com/).

2. In the bottom of the left sidebar, select **Settings** \>
 **Plan information**.

3. Click **Set up Billing** for your chosen project to enable Cloud Billing.

## Monitor usage

After you enable Cloud Billing, you can monitor your usage of the Gemini API in
[Google AI Studio](https://aistudio.google.com/usage).

## Frequently asked questions

This section provides answers to frequently asked questions.

### What am I billed for?

Gemini API pricing is based on the following:

- Input token count
- Output token count
- Cached token count
- Cached token storage duration

For pricing information, see the [pricing page](https://ai.google.dev/pricing).

### Where can I view my quota?

You can view your quota and system limits in the
[Google Cloud console](https://console.cloud.google.com/apis/api/generativelanguage.googleapis.com/quotas).

### How do I request more quota?

To request more quota, follow the instructions at
[How to request an upgrade](https://ai.google.dev/gemini-api/docs/billing#request-an-upgrade).

### Can I use the Gemini API for free in EEA (including EU), the UK, and CH?

Yes, we make the free tier and paid tier available in
[many regions](https://ai.google.dev/gemini-api/docs/available-regions).

### If I set up billing with the Gemini API, will I be charged for my Google AI Studio usage?

No, Google AI Studio usage remains free of charge regardless of if you set up
billing across all supported regions.

### Can I use 1M tokens in the free tier?

The free tier for Gemini API differs based on the model selected. For now, you
can try the 1M token context window in the following ways:

- In Google AI Studio
- With pay-as-you-go plans
- With free-of-charge plans for select models

See the latest free-of-charge rate limits per model on [rate limits page](https://ai.google.dev/gemini-api/docs/rate-limits).

### How can I calculate the number of tokens I'm using?

Use the
[`GenerativeModel.count_tokens`](https://ai.google.dev/api/python/google/generativeai/GenerativeModel#count_tokens)
method to count the number of tokens. Refer to the
[Tokens guide](https://ai.google.dev/gemini-api/docs/tokens) to learn more about tokens.

### Can I use my Google Cloud credits with the Gemini API?

Yes, Google Cloud credits can be used towards Gemini API usage.

### How is billing handled?

Billing for the Gemini API is handled by the
[Cloud Billing](https://cloud.google.com/billing/docs/concepts) system.

### Am I charged for failed requests?

If your request fails with a 400 or 500 error, you won't be charged for the
tokens used. However, the request will still count against your quota.

### Is there a charge for fine-tuning the models?

[Model tuning](https://ai.google.dev/gemini-api/docs/model-tuning) is free, but inference on tuned
models is charged at the same rate as the base models.

### Is GetTokens billed?

Requests to the GetTokens API are not billed, and they don't count against
inference quota.

### How is my Google AI Studio data handled if I have a paid API account?

Refer to the [terms](https://ai.google.dev/gemini-api/terms#paid-services) for details on how data is
handled when Cloud billing is enabled (see "How Google Uses Your Data" under
"Paid Services"). Note that your Google AI Studio prompts are treated under the
same "Paid Services" terms so long as at least 1 API project has billing enabled,
which you can validate on the [Gemini API Key page](https://aistudio.google.com/api-keys)
if you see any projects marked as "Paid" under "Plan".

### Where can I get help with billing?

To get help with billing, see
[Get Cloud Billing support](https://cloud.google.com/support/billing).

[END OF DOCUMENT: GEMINICED9JOS8H]
---

[START OF DOCUMENT: GEMINI1YCWMSOV0T | Title: Caching.Md]

Python JavaScript Go REST

In a typical AI workflow, you might pass the same input tokens over and over to
a model. The Gemini API offers two different caching mechanisms:

- Implicit caching (automatically enabled on Gemini 2.5 models, no cost saving guarantee)
- Explicit caching (can be manually enabled on most models, cost saving guarantee)

Explicit caching is useful in cases where you want to guarantee cost savings,
but with some added developer work.

## Implicit caching

Implicit caching is enabled by default for all Gemini 2.5 models. We automatically
pass on cost savings if your request hits caches. There is nothing you need to do
in order to enable this. It is effective as of May 8th, 2025. The minimum input
token count for context caching is 1,024 for 2.5 Flash and 4,096 for 2.5 Pro.

To increase the chance of an implicit cache hit:

- Try putting large and common contents at the beginning of your prompt
- Try to send requests with similar prefix in a short amount of time

You can see the number of tokens which were cache hits in the response object's
`usage_metadata` field.

## Explicit caching

Using the Gemini API explicit caching feature, you can pass some content
to the model once, cache the input tokens, and then refer to the cached tokens
for subsequent requests. At certain volumes, using cached tokens is lower cost
than passing in the same corpus of tokens repeatedly.

When you cache a set of tokens, you can choose how long you want the cache to
exist before the tokens are automatically deleted. This caching duration is
called the *time to live* (TTL). If not set, the TTL defaults to 1 hour. The
cost for caching depends on the input token size and how long you want the
tokens to persist.

This section assumes that you've installed a Gemini SDK (or have curl installed)
and that you've configured an API key, as shown in the
[quickstart](https://ai.google.dev/gemini-api/docs/quickstart).

### Generate content using a cache

The following example shows how to generate content using a cached system
instruction and video file.

### Videos

 import os
 import pathlib
 import requests
 import time

 from google import genai
 from google.genai import types

 client = genai.Client()

 # Download video file
 url = 'https://storage.googleapis.com/generativeai-downloads/data/SherlockJr._10min.mp4'
 path_to_video_file = pathlib.Path('SherlockJr._10min.mp4')
 if not path_to_video_file.exists():
 with path_to_video_file.open('wb') as wf:
 response = requests.get(url, stream=True)
 for chunk in response.iter_content(chunk_size=32768):
 wf.write(chunk)

 # Upload the video using the Files API
 video_file = client.files.upload(file=path_to_video_file)

 # Wait for the file to finish processing
 while video_file.state.name == 'PROCESSING':
 print('Waiting for video to be processed.')
 time.sleep(2)
 video_file = client.files.get(name=video_file.name)

 print(f'Video processing complete: {video_file.uri}')

 # You must use an explicit version suffix: "-flash-001", not just "-flash".
 model='models/gemini-2.0-flash-001'

 # Create a cache with a 5 minute TTL
 cache = client.caches.create(
 model=model,
 config=types.CreateCachedContentConfig(
 display_name='sherlock jr movie', # used to identify the cache
 system_instruction=(
 'You are an expert video analyzer, and your job is to answer '
 'the user\'s query based on the video file you have access to.'
 ),
 contents=[video_file],
 ttl="300s",
 )
 )

 # Construct a GenerativeModel which uses the created cache.
 response = client.models.generate_content(
 model = model,
 contents= (
 'Introduce different characters in the movie by describing '
 'their personality, looks, and names. Also list the timestamps '
 'they were introduced for the first time.'),
 config=types.GenerateContentConfig(cached_content=cache.name)
 )

 print(response.usage_metadata)

 # The output should look something like this:
 #
 # prompt_token_count: 696219
 # cached_content_token_count: 696190
 # candidates_token_count: 214
 # total_token_count: 696433

 print(response.text)

### PDFs

 from google import genai
 from google.genai import types
 import io
 import httpx

 client = genai.Client()

 long_context_pdf_path = "https://www.nasa.gov/wp-content/uploads/static/history/alsj/a17/A17_FlightPlan.pdf"

 # Retrieve and upload the PDF using the File API
 doc_io = io.BytesIO(httpx.get(long_context_pdf_path).content)

 document = client.files.upload(
 file=doc_io,
 config=dict(mime_type='application/pdf')
 )

 model_name = "gemini-2.0-flash-001"
 system_instruction = "You are an expert analyzing transcripts."

 # Create a cached content object
 cache = client.caches.create(
 model=model_name,
 config=types.CreateCachedContentConfig(
 system_instruction=system_instruction,
 contents=[document],
 )
 )

 # Display the cache details
 print(f'{cache=}')

 # Generate content using the cached prompt and document
 response = client.models.generate_content(
 model=model_name,
 contents="Please summarize this transcript",
 config=types.GenerateContentConfig(
 cached_content=cache.name
 ))

 # (Optional) Print usage metadata for insights into the API call
 print(f'{response.usage_metadata=}')

 # Print the generated text
 print('\n\n', response.text)

### List caches

It's not possible to retrieve or view cached content, but you can retrieve
cache metadata (`name`, `model`, `display_name`, `usage_metadata`,
`create_time`, `update_time`, and `expire_time`).

To list metadata for all uploaded caches, use `CachedContent.list()`:

 for cache in client.caches.list():
 print(cache)

To fetch the metadata for one cache object, if you know its name, use `get`:

 client.caches.get(name=name)

### Update a cache

You can set a new `ttl` or `expire_time` for a cache. Changing anything else
about the cache isn't supported.

The following example shows how to update the `ttl` of a cache using
`client.caches.update()`.

 from google import genai
 from google.genai import types

 client.caches.update(
 name = cache.name,
 config = types.UpdateCachedContentConfig(
 ttl='300s'
 )
 )

To set the expiry time, it will accepts either a `datetime` object
or an ISO-formatted datetime string (`dt.isoformat()`, like
`2025-01-27T16:02:36.473528+00:00`). Your time must include a time zone
(`datetime.utcnow()` doesn't attach a time zone,
`datetime.now(datetime.timezone.utc)` does attach a time zone).

 from google import genai
 from google.genai import types
 import datetime

 # You must use a time zone-aware time.
 in10min = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=10)

 client.caches.update(
 name = cache.name,
 config = types.UpdateCachedContentConfig(
 expire_time=in10min
 )
 )

### Delete a cache

The caching service provides a delete operation for manually removing content
from the cache. The following example shows how to delete a cache:

 client.caches.delete(cache.name)

### Explicit caching using the OpenAI library

If you're using an [OpenAI library](https://ai.google.dev/gemini-api/docs/openai), you can enable
explicit caching using the `cached_content` property on
[`extra_body`](https://ai.google.dev/gemini-api/docs/openai#extra-body).

## When to use explicit caching

Context caching is particularly well suited to scenarios where a substantial
initial context is referenced repeatedly by shorter requests. Consider using
context caching for use cases such as:

- Chatbots with extensive [system instructions](https://ai.google.dev/gemini-api/docs/system-instructions)
- Repetitive analysis of lengthy video files
- Recurring queries against large document sets
- Frequent code repository analysis or bug fixing

### How explicit caching reduces costs

Context caching is a paid feature designed to reduce overall operational costs.
Billing is based on the following factors:

1. **Cache token count:** The number of input tokens cached, billed at a reduced rate when included in subsequent prompts.
2. **Storage duration:** The amount of time cached tokens are stored (TTL), billed based on the TTL duration of cached token count. There are no minimum or maximum bounds on the TTL.
3. **Other factors:** Other charges apply, such as for non-cached input tokens and output tokens.

For up-to-date pricing details, refer to the Gemini API [pricing
page](https://ai.google.dev/pricing). To learn how to count tokens, see the [Token
guide](https://ai.google.dev/gemini-api/docs/tokens).

### Additional considerations

Keep the following considerations in mind when using context caching:

- The *minimum* input token count for context caching is 1,024 for 2.5 Flash and 2,048 for 2.5 Pro. The *maximum* is the same as the maximum for the given model. (For more on counting tokens, see the [Token guide](https://ai.google.dev/gemini-api/docs/tokens)).
- The model doesn't make any distinction between cached tokens and regular input tokens. Cached content is a prefix to the prompt.
- There are no special rate or usage limits on context caching; the standard rate limits for `GenerateContent` apply, and token limits include cached tokens.
- The number of cached tokens is returned in the `usage_metadata` from the create, get, and list operations of the cache service, and also in `GenerateContent` when using the cache.

[END OF DOCUMENT: GEMINI1YCWMSOV0T]
---

