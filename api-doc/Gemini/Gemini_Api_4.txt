
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: GEMINI1955I6NLHY (sha256-73d3db09aa96b2efb61e0d917a1273970b1b239ea8cf35e4f58fd8420609df71) | Title: Google-Search.Md]
[DocID: GEMINI2PJFPMRLHB (sha256-fa4430b32f7fd5762720a659bba4ed3b89ff356b792b292049017fde5bca9a9c) | Title: Image-Generation.Md]
[DocID: GEMINI1E51EEWV1J (sha256-80a5fabb7b278b29c11d371443cfaaac05bd5becf66f137895693dbb5933c269) | Title: Image-Understanding.Md]
[DocID: GEMINI2MKGH0X99T (sha256-f2a44c889131c9a936d89cea96b258b020a5704b3fba42538aca7ef460402b73) | Title: Imagen.Md]
[DocID: GEMINIDXXRQ7EXM (sha256-23c626833d5ab79711bcb76e7ebe9a0a52c1f4886d19b86073f7ccc46e96d0d5) | Title: Langgraph-Example.Md]
--- END OF TOC ---

[START OF DOCUMENT: GEMINI1955I6NLHY | Title: Google-Search.Md]

<br />

Grounding with Google Search connects the Gemini model to real-time web content and works with all available languages. This allows Gemini to provide more accurate answers and cite verifiable sources beyond its knowledge cutoff.

Grounding helps you build applications that can:

- **Increase factual accuracy:**Reduce model hallucinations by basing responses on real-world information.
- **Access real-time information:**Answer questions about recent events and topics.
- **Provide citations:**Build user trust by showing the sources for the model's claims.

### Python

 from google import genai
 from google.genai import types

 client = genai.Client()

 grounding_tool = types.Tool(
 google_search=types.GoogleSearch()
 )

 config = types.GenerateContentConfig(
 tools=[grounding_tool]
 )

 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="Who won the euro 2024?",
 config=config,
 )

 print(response.text)

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const ai = new GoogleGenAI({});

 const groundingTool = {
 googleSearch: {},
 };

 const config = {
 tools: [groundingTool],
 };

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: "Who won the euro 2024?",
 config,
 });

 console.log(response.text);

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -X POST \
 -d '{
 "contents": [
 {
 "parts": [
 {"text": "Who won the euro 2024?"}
 ]
 }
 ],
 "tools": [
 {
 "google_search": {}
 }
 ]
 }'

You can learn more by trying the[Search tool notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb).

## How grounding with Google Search works

When you enable the`google_search`tool, the model handles the entire workflow of searching, processing, and citing information automatically.

![grounding-overview](https://ai.google.dev/static/gemini-api/docs/images/google-search-tool-overview.png)

1. **User Prompt:** Your application sends a user's prompt to the Gemini API with the`google_search`tool enabled.
2. **Prompt Analysis:**The model analyzes the prompt and determines if a Google Search can improve the answer.
3. **Google Search:**If needed, the model automatically generates one or multiple search queries and executes them.
4. **Search Results Processing:**The model processes the search results, synthesizes the information, and formulates a response.
5. **Grounded Response:** The API returns a final, user-friendly response that is grounded in the search results. This response includes the model's text answer and`groundingMetadata`with the search queries, web results, and citations.

## Understanding the grounding response

When a response is successfully grounded, the response includes a`groundingMetadata`field. This structured data is essential for verifying claims and building a rich citation experience in your application.

 {
 "candidates": [
 {
 "content": {
 "parts": [
 {
 "text": "Spain won Euro 2024, defeating England 2-1 in the final. This victory marks Spain's record fourth European Championship title."
 }
 ],
 "role": "model"
 },
 "groundingMetadata": {
 "webSearchQueries": [
 "UEFA Euro 2024 winner",
 "who won euro 2024"
 ],
 "searchEntryPoint": {
 "renderedContent": "<!-- HTML and CSS for the search widget -->"
 },
 "groundingChunks": [
 {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "aljazeera.com"}},
 {"web": {"uri": "https://vertexaisearch.cloud.google.com.....", "title": "uefa.com"}}
 ],
 "groundingSupports": [
 {
 "segment": {"startIndex": 0, "endIndex": 85, "text": "Spain won Euro 2024, defeatin..."},
 "groundingChunkIndices": [0]
 },
 {
 "segment": {"startIndex": 86, "endIndex": 210, "text": "This victory marks Spain's..."},
 "groundingChunkIndices": [0, 1]
 }
 ]
 }
 }
 ]
 }

The Gemini API returns the following information with the`groundingMetadata`:

- `webSearchQueries`: Array of the search queries used. This is useful for debugging and understanding the model's reasoning process.
- `searchEntryPoint`: Contains the HTML and CSS to render the required Search Suggestions. Full usage requirements are detailed in the[Terms of Service](https://ai.google.dev/gemini-api/terms#grounding-with-google-search).
- `groundingChunks`: Array of objects containing the web sources (`uri`and`title`).
- `groundingSupports`: Array of chunks to connect model response`text`to the sources in`groundingChunks`. Each chunk links a text`segment`(defined by`startIndex`and`endIndex`) to one or more`groundingChunkIndices`. This is the key to building inline citations.

Grounding with Google Search can also be used in combination with the[URL context tool](https://ai.google.dev/gemini-api/docs/url-context)to ground responses in both public web data and the specific URLs you provide.

## Attributing sources with inline citations

The API returns structured citation data, giving you complete control over how you display sources in your user interface. You can use the`groundingSupports`and`groundingChunks`fields to link the model's statements directly to their sources. Here is a common pattern for processing the metadata to create a response with inline, clickable citations.

### Python

 def add_citations(response):
 text = response.text
 supports = response.candidates[0].grounding_metadata.grounding_supports
 chunks = response.candidates[0].grounding_metadata.grounding_chunks

 # Sort supports by end_index in descending order to avoid shifting issues when inserting.
 sorted_supports = sorted(supports, key=lambda s: s.segment.end_index, reverse=True)

 for support in sorted_supports:
 end_index = support.segment.end_index
 if support.grounding_chunk_indices:
 # Create citation string like [1](link1)[2](link2)
 citation_links = []
 for i in support.grounding_chunk_indices:
 if i < len(chunks):
 uri = chunks[i].web.uri
 citation_links.append(f"[{i + 1}]({uri})")

 citation_string = ", ".join(citation_links)
 text = text[:end_index] + citation_string + text[end_index:]

 return text

 # Assuming response with grounding metadata
 text_with_citations = add_citations(response)
 print(text_with_citations)

### JavaScript

 function addCitations(response) {
 let text = response.text;
 const supports = response.candidates[0]?.groundingMetadata?.groundingSupports;
 const chunks = response.candidates[0]?.groundingMetadata?.groundingChunks;

 // Sort supports by end_index in descending order to avoid shifting issues when inserting.
 const sortedSupports = [...supports].sort(
 (a, b) => (b.segment?.endIndex ?? 0) - (a.segment?.endIndex ?? 0),
 );

 for (const support of sortedSupports) {
 const endIndex = support.segment?.endIndex;
 if (endIndex === undefined || !support.groundingChunkIndices?.length) {
 continue;
 }

 const citationLinks = support.groundingChunkIndices
 .map(i => {
 const uri = chunks[i]?.web?.uri;
 if (uri) {
 return `[${i + 1}](${uri})`;
 }
 return null;
 })
 .filter(Boolean);

 if (citationLinks.length > 0) {
 const citationString = citationLinks.join(", ");
 text = text.slice(0, endIndex) + citationString + text.slice(endIndex);
 }
 }

 return text;
 }

 const textWithCitations = addCitations(response);
 console.log(textWithCitations);

The new response with inline citations will look like this:

 Spain won Euro 2024, defeating England 2-1 in the final.[1](https:/...), [2](https:/...), [4](https:/...), [5](https:/...) This victory marks Spain's record-breaking fourth European Championship title.[5]((https:/...), [2](https:/...), [3](https:/...), [4](https:/...)

## Pricing

When you use Grounding with Google Search, your project is billed per API request that includes the`google_search`tool. If the model decides to execute multiple search queries to answer a single prompt (for example, searching for`"UEFA Euro 2024 winner"`and`"Spain vs England Euro 2024 final score"`within the same API call), this counts as a single billable use of the tool for that request.

For detailed pricing information, see the[Gemini API pricing page](https://ai.google.dev/gemini-api/docs/pricing).

## Supported models

Experimental and Preview models are not included. You can find their capabilities on the[model overview](https://ai.google.dev/gemini-api/docs/models)page.

| Model | Grounding with Google Search |
|-----------------------|------------------------------|
| Gemini 2.5 Pro | ✔️ |
| Gemini 2.5 Flash | ✔️ |
| Gemini 2.5 Flash-Lite | ✔️ |
| Gemini 2.0 Flash | ✔️ |
| Gemini 1.5 Pro | ✔️ |
| Gemini 1.5 Flash | ✔️ |

| **Note:** Older models use a`google_search_retrieval`tool. For all current models, use the`google_search`tool as shown in the examples.

## Supported tools combinations

You can use Grounding with Google Search with other tools like[code execution](https://ai.google.dev/gemini-api/docs/code-execution)and[URL context](https://ai.google.dev/gemini-api/docs/url-context)to power more complex use cases.

## Grounding with Gemini 1.5 Models (Legacy)

While the`google_search`tool is recommended for Gemini 2.0 and later, Gemini 1.5 supports a legacy tool named`google_search_retrieval`. This tool provides a`dynamic`mode that allows the model to decide whether to perform a search based on its confidence that the prompt requires fresh information. If the model's confidence is above a`dynamic_threshold`you set (a value between 0.0 and 1.0), it will perform a search.

### Python

 # Note: This is a legacy approach for Gemini 1.5 models.
 # The 'google_search' tool is recommended for all new development.
 import os
 from google import genai
 from google.genai import types

 client = genai.Client()

 retrieval_tool = types.Tool(
 google_search_retrieval=types.GoogleSearchRetrieval(
 dynamic_retrieval_config=types.DynamicRetrievalConfig(
 mode=types.DynamicRetrievalConfigMode.MODE_DYNAMIC,
 dynamic_threshold=0.7 # Only search if confidence > 70%
 )
 )
 )

 config = types.GenerateContentConfig(
 tools=[retrieval_tool]
 )

 response = client.models.generate_content(
 model='gemini-1.5-flash',
 contents="Who won the euro 2024?",
 config=config,
 )
 print(response.text)
 if not response.candidates[0].grounding_metadata:
 print("\nModel answered from its own knowledge.")

### JavaScript

 // Note: This is a legacy approach for Gemini 1.5 models.
 // The 'googleSearch' tool is recommended for all new development.
 import { GoogleGenAI, DynamicRetrievalConfigMode } from "@google/genai";

 const ai = new GoogleGenAI({});

 const retrievalTool = {
 googleSearchRetrieval: {
 dynamicRetrievalConfig: {
 mode: DynamicRetrievalConfigMode.MODE_DYNAMIC,
 dynamicThreshold: 0.7, // Only search if confidence > 70%
 },
 },
 };

 const config = {
 tools: [retrievalTool],
 };

 const response = await ai.models.generateContent({
 model: "gemini-1.5-flash",
 contents: "Who won the euro 2024?",
 config,
 });

 console.log(response.text);
 if (!response.candidates?.[0]?.groundingMetadata) {
 console.log("\nModel answered from its own knowledge.");
 }

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \

 -H "Content-Type: application/json" \
 -X POST \
 -d '{
 "contents": [
 {"parts": [{"text": "Who won the euro 2024?"}]}
 ],
 "tools": [{
 "google_search_retrieval": {
 "dynamic_retrieval_config": {
 "mode": "MODE_DYNAMIC",
 "dynamic_threshold": 0.7
 }
 }
 }]
 }'

## What's next

- Try the[Grounding with Google Search in the Gemini API Cookbook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Search_Grounding.ipynb).
- Learn about other available tools, like[Function Calling](https://ai.google.dev/gemini-api/docs/function-calling).
- Learn how to augment prompts with specific URLs using the[URL context tool](https://ai.google.dev/gemini-api/docs/url-context).

[END OF DOCUMENT: GEMINI1955I6NLHY]
---

[START OF DOCUMENT: GEMINI2PJFPMRLHB | Title: Image-Generation.Md]

<br />

<br />

Gemini can generate and process images conversationally. You can prompt Gemini with text, images, or a combination of both allowing you to create, edit, and iterate on visuals with unprecedented control:

- **Text-to-Image:**Generate high-quality images from simple or complex text descriptions.
- **Image + Text-to-Image (Editing):**Provide an image and use text prompts to add, remove, or modify elements, change the style, or adjust the color grading.
- **Multi-Image to Image (Composition \& Style Transfer):**Use multiple input images to compose a new scene or transfer the style from one image to another.
- **Iterative Refinement:**Engage in a conversation to progressively refine your image over multiple turns, making small adjustments until it's perfect.
- **High-Fidelity Text Rendering:**Accurately generate images that contain legible and well-placed text, ideal for logos, diagrams, and posters.

All generated images include a[SynthID watermark](https://ai.google.dev/responsible/docs/safeguards/synthid).

## Image generation (text-to-image)

The following code demonstrates how to generate an image based on a descriptive prompt.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 prompt = (
 "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"
 )

 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[prompt],
 )

 for part in response.parts:
 if part.text is not None:
 print(part.text)
 elif part.inline_data is not None:
 image = part.as_image()
 image.save("generated_image.png")

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("gemini-native-image.png", buffer);
 console.log("Image saved as gemini-native-image.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("Create a picture of a nano banana dish in a " +
 " fancy restaurant with a Gemini theme"),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "gemini_generated_image.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > gemini-native-image.png

![AI-generated image of a nano banana dish](https://ai.google.dev/static/gemini-api/docs/images/nano-banana.png)AI-generated image of a nano banana dish in a Gemini-themed restaurant

## Image editing (text-and-image-to-image)

**Reminder** : Make sure you have the necessary rights to any images you upload. Don't generate content that infringe on others' rights, including videos or images that deceive, harass, or harm. Your use of this generative AI service is subject to our[Prohibited Use Policy](https://policies.google.com/terms/generative-ai/use-policy).

The following example demonstrates uploading base64 encoded images. For multiple images, larger payloads, and supported MIME types, check the[Image understanding](https://ai.google.dev/gemini-api/docs/image-understanding)page.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 prompt = (
 "Create a picture of my cat eating a nano-banana in a "
 "fancy restaurant under the Gemini constellation",
 )

 image = Image.open("/path/to/cat_image.png")

 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[prompt, image],
 )

 for part in response.parts:
 if part.text is not None:
 print(part.text)
 elif part.inline_data is not None:
 image = part.as_image()
 image.save("generated_image.png")

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const imagePath = "path/to/cat_image.png";
 const imageData = fs.readFileSync(imagePath);
 const base64Image = imageData.toString("base64");

 const prompt = [
 { text: "Create a picture of my cat eating a nano-banana in a" +
 "fancy restaurant under the Gemini constellation" },
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image,
 },
 },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("gemini-native-image.png", buffer);
 console.log("Image saved as gemini-native-image.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 imagePath := "/path/to/cat_image.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
 genai.NewPartFromText("Create a picture of my cat eating a nano-banana in a fancy restaurant under the Gemini constellation"),
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData,
 },
 },
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "gemini_generated_image.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 IMG_PATH=/path/to/cat_image.jpeg

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d "{
 \"contents\": [{
 \"parts\":[
 {\"text\": \"'Create a picture of my cat eating a nano-banana in a fancy restaurant under the Gemini constellation\"},
 {
 \"inline_data\": {
 \"mime_type\":\"image/jpeg\",
 \"data\": \"$IMG_BASE64\"
 }
 }
 ]
 }]
 }" \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > gemini-edited-image.png

![AI-generated image of a cat eating anano banana](https://ai.google.dev/static/gemini-api/docs/images/cat-banana.png)AI-generated image of a cat eating a nano banana

## Other image generation modes

Gemini supports other image interaction modes based on prompt structure and context, including:

- **Text to image(s) and text (interleaved):** Outputs images with related text.
 - Example prompt: "Generate an illustrated recipe for a paella."
- **Image(s) and text to image(s) and text (interleaved)** : Uses input images and text to create new related images and text.
 - Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? can you update the image?"
- **Multi-turn image editing (chat):** Keep generating and editing images conversationally.
 - Example prompts: \[upload an image of a blue car.\] , "Turn this car into a convertible.", "Now change the color to yellow."

## Prompting guide and strategies

Mastering Gemini 2.5 Flash Image Generation starts with one fundamental principle:
> **Describe the scene, don't just list keywords.**The model's core strength is its deep language understanding. A narrative, descriptive paragraph will almost always produce a better, more coherent image than a list of disconnected words.

### Prompts for generating images

The following strategies will help you create effective prompts to generate exactly the images you're looking for.

#### 1. Photorealistic scenes

For realistic images, use photography terms. Mention camera angles, lens types, lighting, and fine details to guide the model toward a photorealistic result.

### Template

 A photorealistic [shot type] of [subject], [action or expression], set in
 [environment]. The scene is illuminated by [lighting description], creating
 a [mood] atmosphere. Captured with a [camera/lens details], emphasizing
 [key textures and details]. The image should be in a [aspect ratio] format.

### Prompt

 A photorealistic close-up portrait of an elderly Japanese ceramicist with
 deep, sun-etched wrinkles and a warm, knowing smile. He is carefully
 inspecting a freshly glazed tea bowl. The setting is his rustic,
 sun-drenched workshop. The scene is illuminated by soft, golden hour light
 streaming through a window, highlighting the fine texture of the clay.
 Captured with an 85mm portrait lens, resulting in a soft, blurred background
 (bokeh). The overall mood is serene and masterful. Vertical portrait
 orientation.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents="A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful.",
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('photorealistic_example.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful.";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("photorealistic_example.png", buffer);
 console.log("Image saved as photorealistic_example.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful."),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "photorealistic_example.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "A photorealistic close-up portrait of an elderly Japanese ceramicist with deep, sun-etched wrinkles and a warm, knowing smile. He is carefully inspecting a freshly glazed tea bowl. The setting is his rustic, sun-drenched workshop with pottery wheels and shelves of clay pots in the background. The scene is illuminated by soft, golden hour light streaming through a window, highlighting the fine texture of the clay and the fabric of his apron. Captured with an 85mm portrait lens, resulting in a soft, blurred background (bokeh). The overall mood is serene and masterful."}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > photorealistic_example.png

![A photorealistic close-up portrait of an elderly Japanese ceramicist...](https://ai.google.dev/static/gemini-api/docs/images/photorealistic_example.png)A photorealistic close-up portrait of an elderly Japanese ceramicist...

#### 2. Stylized illustrations \& stickers

To create stickers, icons, or assets, be explicit about the style and request a transparent background.

### Template

 A [style] sticker of a [subject], featuring [key characteristics] and a
 [color palette]. The design should have [line style] and [shading style].
 The background must be transparent.

### Prompt

 A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's
 munching on a green bamboo leaf. The design features bold, clean outlines,
 simple cel-shading, and a vibrant color palette. The background must be white.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents="A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.",
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('red_panda_sticker.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white.";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("red_panda_sticker.png", buffer);
 console.log("Image saved as red_panda_sticker.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It's munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "red_panda_sticker.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "A kawaii-style sticker of a happy red panda wearing a tiny bamboo hat. It'"'"'s munching on a green bamboo leaf. The design features bold, clean outlines, simple cel-shading, and a vibrant color palette. The background must be white."}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > red_panda_sticker.png

![A kawaii-style sticker of a happy red...](https://ai.google.dev/static/gemini-api/docs/images/red_panda_sticker.png)A kawaii-style sticker of a happy red panda...

#### 3. Accurate text in images

Gemini excels at rendering text. Be clear about the text, the font style (descriptively), and the overall design.

### Template

 Create a [image type] for [brand/concept] with the text "[text to render]"
 in a [font style]. The design should be [style description], with a
 [color scheme].

### Prompt

 Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'.
 The text should be in a clean, bold, sans-serif font. The design should
 feature a simple, stylized icon of a a coffee bean seamlessly integrated
 with the text. The color scheme is black and white.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents="Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white.",
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('logo_example.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white.";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("logo_example.png", buffer);
 console.log("Image saved as logo_example.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white."),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "logo_example.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "Create a modern, minimalist logo for a coffee shop called '"'"'The Daily Grind'"'"'. The text should be in a clean, bold, sans-serif font. The design should feature a simple, stylized icon of a a coffee bean seamlessly integrated with the text. The color scheme is black and white."}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > logo_example.png

![Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'...](https://ai.google.dev/static/gemini-api/docs/images/logo_example.png)Create a modern, minimalist logo for a coffee shop called 'The Daily Grind'...

#### 4. Product mockups \& commercial photography

Perfect for creating clean, professional product shots for e-commerce, advertising, or branding.

### Template

 A high-resolution, studio-lit product photograph of a [product description]
 on a [background surface/description]. The lighting is a [lighting setup,
 e.g., three-point softbox setup] to [lighting purpose]. The camera angle is
 a [angle type] to showcase [specific feature]. Ultra-realistic, with sharp
 focus on [key detail]. [Aspect ratio].

### Prompt

 A high-resolution, studio-lit product photograph of a minimalist ceramic
 coffee mug in matte black, presented on a polished concrete surface. The
 lighting is a three-point softbox setup designed to create soft, diffused
 highlights and eliminate harsh shadows. The camera angle is a slightly
 elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with
 sharp focus on the steam rising from the coffee. Square image.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents="A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image.",
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('product_mockup.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image.";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("product_mockup.png", buffer);
 console.log("Image saved as product_mockup.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image."),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "product_mockup.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug in matte black, presented on a polished concrete surface. The lighting is a three-point softbox setup designed to create soft, diffused highlights and eliminate harsh shadows. The camera angle is a slightly elevated 45-degree shot to showcase its clean lines. Ultra-realistic, with sharp focus on the steam rising from the coffee. Square image."}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > product_mockup.png

![A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug...](https://ai.google.dev/static/gemini-api/docs/images/product_mockup.png)A high-resolution, studio-lit product photograph of a minimalist ceramic coffee mug...

#### 5. Minimalist \& negative space design

Excellent for creating backgrounds for websites, presentations, or marketing materials where text will be overlaid.

### Template

 A minimalist composition featuring a single [subject] positioned in the
 [bottom-right/top-left/etc.] of the frame. The background is a vast, empty
 [color] canvas, creating significant negative space. Soft, subtle lighting.
 [Aspect ratio].

### Prompt

 A minimalist composition featuring a single, delicate red maple leaf
 positioned in the bottom-right of the frame. The background is a vast, empty
 off-white canvas, creating significant negative space for text. Soft,
 diffused lighting from the top left. Square image.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents="A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image.",
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('minimalist_design.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image.";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("minimalist_design.png", buffer);
 console.log("Image saved as minimalist_design.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image."),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "minimalist_design.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "A minimalist composition featuring a single, delicate red maple leaf positioned in the bottom-right of the frame. The background is a vast, empty off-white canvas, creating significant negative space for text. Soft, diffused lighting from the top left. Square image."}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > minimalist_design.png

![A minimalist composition featuring a single, delicate red maple leaf...](https://ai.google.dev/static/gemini-api/docs/images/minimalist_design.png)A minimalist composition featuring a single, delicate red maple leaf...

#### 6. Sequential art (Comic panel / Storyboard)

Builds on character consistency and scene description to create panels for visual storytelling.

### Template

 A single comic book panel in a [art style] style. In the foreground,
 [character description and action]. In the background, [setting details].
 The panel has a [dialogue/caption box] with the text "[Text]". The lighting
 creates a [mood] mood. [Aspect ratio].

### Prompt

 A single comic book panel in a gritty, noir art style with high-contrast
 black and white inks. In the foreground, a detective in a trench coat stands
 under a flickering streetlamp, rain soaking his shoulders. In the
 background, the neon sign of a desolate bar reflects in a puddle. A caption
 box at the top reads "The city was a tough place to keep secrets." The
 lighting is harsh, creating a dramatic, somber mood. Landscape.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents="A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape.",
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('comic_panel.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const prompt =
 "A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape.";

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("comic_panel.png", buffer);
 console.log("Image saved as comic_panel.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape."),
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "comic_panel.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 curl -s -X POST
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "contents": [{
 "parts": [
 {"text": "A single comic book panel in a gritty, noir art style with high-contrast black and white inks. In the foreground, a detective in a trench coat stands under a flickering streetlamp, rain soaking his shoulders. In the background, the neon sign of a desolate bar reflects in a puddle. A caption box at the top reads \"The city was a tough place to keep secrets.\" The lighting is harsh, creating a dramatic, somber mood. Landscape."}
 ]
 }]
 }' \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > comic_panel.png

![A single comic book panel in a gritty, noir art style...](https://ai.google.dev/static/gemini-api/docs/images/comic_panel.png)A single comic book panel in a gritty, noir art style...

### Prompts for editing images

These examples show how to provide images alongside your text prompts for editing, composition, and style transfer.

#### 1. Adding and removing elements

Provide an image and describe your change. The model will match the original image's style, lighting, and perspective.

### Template

 Using the provided image of [subject], please [add/remove/modify] [element]
 to/from the scene. Ensure the change is [description of how the change should
 integrate].

### Prompt

 "Using the provided image of my cat, please add a small, knitted wizard hat
 on its head. Make it look like it's sitting comfortably and matches the soft
 lighting of the photo."

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Base image prompt: "A photorealistic picture of a fluffy ginger cat sitting on a wooden floor, looking directly at the camera. Soft, natural light from a window."
 image_input = Image.open('/path/to/your/cat_photo.png')
 text_input = """Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off."""

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[text_input, image_input],
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('cat_with_hat.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const imagePath = "/path/to/your/cat_photo.png";
 const imageData = fs.readFileSync(imagePath);
 const base64Image = imageData.toString("base64");

 const prompt = [
 { text: "Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off." },
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image,
 },
 },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("cat_with_hat.png", buffer);
 console.log("Image saved as cat_with_hat.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 imagePath := "/path/to/your/cat_photo.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
 genai.NewPartFromText("Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off."),
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData,
 },
 },
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "cat_with_hat.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 IMG_PATH=/path/to/your/cat_photo.png

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d "{
 \"contents\": [{
 \"parts\":[
 {\"text\": \"Using the provided image of my cat, please add a small, knitted wizard hat on its head. Make it look like it's sitting comfortably and not falling off.\"},
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG_BASE64\"
 }
 }
 ]
 }]
 }" \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > cat_with_hat.png

|---------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Input | Output |
| :cat:A photorealistic picture of a fluffy ginger cat... | ![Using the provided image of my cat, please add a small, knitted wizard hat...](https://ai.google.dev/static/gemini-api/docs/images/cat_with_hat.png)Using the provided image of my cat, please add a small, knitted wizard hat... |

#### 2. Inpainting (Semantic masking)

Conversationally define a "mask" to edit a specific part of an image while leaving the rest untouched.

### Template

 Using the provided image, change only the [specific element] to [new
 element/description]. Keep everything else in the image exactly the same,
 preserving the original style, lighting, and composition.

### Prompt

 "Using the provided image of a living room, change only the blue sofa to be
 a vintage, brown leather chesterfield sofa. Keep the rest of the room,
 including the pillows on the sofa and the lighting, unchanged."

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Base image prompt: "A wide shot of a modern, well-lit living room with a prominent blue sofa in the center. A coffee table is in front of it and a large window is in the background."
 living_room_image = Image.open('/path/to/your/living_room.png')
 text_input = """Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."""

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[living_room_image, text_input],
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('living_room_edited.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const imagePath = "/path/to/your/living_room.png";
 const imageData = fs.readFileSync(imagePath);
 const base64Image = imageData.toString("base64");

 const prompt = [
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image,
 },
 },
 { text: "Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged." },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("living_room_edited.png", buffer);
 console.log("Image saved as living_room_edited.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 imagePath := "/path/to/your/living_room.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData,
 },
 },
 genai.NewPartFromText("Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged."),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "living_room_edited.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 IMG_PATH=/path/to/your/living_room.png

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d "{
 \"contents\": [{
 \"parts\":[
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG_BASE64\"
 }
 },
 {\"text\": \"Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa. Keep the rest of the room, including the pillows on the sofa and the lighting, unchanged.\"}
 ]
 }]
 }" \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > living_room_edited.png

|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Input | Output |
| ![A wide shot of a modern, well-lit living room...](https://ai.google.dev/static/gemini-api/docs/images/living_room.png)A wide shot of a modern, well-lit living room... | ![Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa...](https://ai.google.dev/static/gemini-api/docs/images/living_room_edited.png)Using the provided image of a living room, change only the blue sofa to be a vintage, brown leather chesterfield sofa... |

#### 3. Style transfer

Provide an image and ask the model to recreate its content in a different artistic style.

### Template

 Transform the provided photograph of [subject] into the artistic style of [artist/art style]. Preserve the original composition but render it with [description of stylistic elements].

### Prompt

 "Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Base image prompt: "A photorealistic, high-resolution photograph of a busy city street in New York at night, with bright neon signs, yellow taxis, and tall skyscrapers."
 city_image = Image.open('/path/to/your/city.png')
 text_input = """Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."""

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[city_image, text_input],
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('city_style_transfer.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const imagePath = "/path/to/your/city.png";
 const imageData = fs.readFileSync(imagePath);
 const base64Image = imageData.toString("base64");

 const prompt = [
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image,
 },
 },
 { text: "Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows." },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("city_style_transfer.png", buffer);
 console.log("Image saved as city_style_transfer.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 imagePath := "/path/to/your/city.png"
 imgData, _ := os.ReadFile(imagePath)

 parts := []*genai.Part{
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData,
 },
 },
 genai.NewPartFromText("Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows."),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "city_style_transfer.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 IMG_PATH=/path/to/your/city.png

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 IMG_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH" 2>&1)

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d "{
 \"contents\": [{
 \"parts\":[
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG_BASE64\"
 }
 },
 {\"text\": \"Transform the provided photograph of a modern city street at night into the artistic style of Vincent van Gogh's 'Starry Night'. Preserve the original composition of buildings and cars, but render all elements with swirling, impasto brushstrokes and a dramatic palette of deep blues and bright yellows.\"}
 ]
 }]
 }" \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > city_style_transfer.png

|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Input | Output |
| ![A photorealistic, high-resolution photograph of a busy city street...](https://ai.google.dev/static/gemini-api/docs/images/city.png)A photorealistic, high-resolution photograph of a busy city street... | ![Transform the provided photograph of a modern city street at night...](https://ai.google.dev/static/gemini-api/docs/images/city_style_transfer.png)Transform the provided photograph of a modern city street at night... |

#### 4. Advanced composition: Combining multiple images

Provide multiple images as context to create a new, composite scene. This is perfect for product mockups or creative collages.

### Template

 Create a new image by combining the elements from the provided images. Take
 the [element from image 1] and place it with/on the [element from image 2].
 The final image should be a [description of the final scene].

### Prompt

 "Create a professional e-commerce fashion photo. Take the blue floral dress
 from the first image and let the woman from the second image wear it.
 Generate a realistic, full-body shot of the woman wearing the dress, with
 the lighting and shadows adjusted to match the outdoor environment."

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Base image prompts:
 # 1. Dress: "A professionally shot photo of a blue floral summer dress on a plain white background, ghost mannequin style."
 # 2. Model: "Full-body shot of a woman with her hair in a bun, smiling, standing against a neutral grey studio background."
 dress_image = Image.open('/path/to/your/dress.png')
 model_image = Image.open('/path/to/your/model.png')

 text_input = """Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."""

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[dress_image, model_image, text_input],
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('fashion_ecommerce_shot.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const imagePath1 = "/path/to/your/dress.png";
 const imageData1 = fs.readFileSync(imagePath1);
 const base64Image1 = imageData1.toString("base64");
 const imagePath2 = "/path/to/your/model.png";
 const imageData2 = fs.readFileSync(imagePath2);
 const base64Image2 = imageData2.toString("base64");

 const prompt = [
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image1,
 },
 },
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image2,
 },
 },
 { text: "Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment." },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("fashion_ecommerce_shot.png", buffer);
 console.log("Image saved as fashion_ecommerce_shot.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 imgData1, _ := os.ReadFile("/path/to/your/dress.png")
 imgData2, _ := os.ReadFile("/path/to/your/model.png")

 parts := []*genai.Part{
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData1,
 },
 },
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData2,
 },
 },
 genai.NewPartFromText("Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment."),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "fashion_ecommerce_shot.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 IMG_PATH1=/path/to/your/dress.png
 IMG_PATH2=/path/to/your/model.png

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 IMG1_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH1" 2>&1)
 IMG2_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH2" 2>&1)

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d "{
 \"contents\": [{
 \"parts\":[
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG1_BASE64\"
 }
 },
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG2_BASE64\"
 }
 },
 {\"text\": \"Create a professional e-commerce fashion photo. Take the blue floral dress from the first image and let the woman from the second image wear it. Generate a realistic, full-body shot of the woman wearing the dress, with the lighting and shadows adjusted to match the outdoor environment.\"}
 ]
 }]
 }" \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > fashion_ecommerce_shot.png

|---------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Input 1 | Input 2 | Output |
| :dress:A professionally shot photo of a blue floral summer dress... | ![Full-body shot of a woman with her hair in a bun...](https://ai.google.dev/static/gemini-api/docs/images/model.png)Full-body shot of a woman with her hair in a bun... | ![Create a professional e-commerce fashion photo...](https://ai.google.dev/static/gemini-api/docs/images/fashion_ecommerce_shot.png)Create a professional e-commerce fashion photo... |

#### 5. High-fidelity detail preservation

To ensure critical details (like a face or logo) are preserved during an edit, describe them in great detail along with your edit request.

### Template

 Using the provided images, place [element from image 2] onto [element from
 image 1]. Ensure that the features of [element from image 1] remain
 completely unchanged. The added element should [description of how the
 element should integrate].

### Prompt

 "Take the first image of the woman with brown hair, blue eyes, and a neutral
 expression. Add the logo from the second image onto her black t-shirt.
 Ensure the woman's face and features remain completely unchanged. The logo
 should look like it's naturally printed on the fabric, following the folds
 of the shirt."

### Python

 from google import genai
 from google.genai import types
 from PIL import Image

 client = genai.Client()

 # Base image prompts:
 # 1. Woman: "A professional headshot of a woman with brown hair and blue eyes, wearing a plain black t-shirt, against a neutral studio background."
 # 2. Logo: "A simple, modern logo with the letters 'G' and 'A' in a white circle."
 woman_image = Image.open('/path/to/your/woman.png')
 logo_image = Image.open('/path/to/your/logo.png')
 text_input = """Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt."""

 # Generate an image from a text prompt
 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[woman_image, logo_image, text_input],
 )

 image_parts = [
 part.inline_data.data
 for part in response.parts
 if part.inline_data
 ]

 if image_parts:
 image = part.as_image()
 image.save('woman_with_logo.png')
 image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const imagePath1 = "/path/to/your/woman.png";
 const imageData1 = fs.readFileSync(imagePath1);
 const base64Image1 = imageData1.toString("base64");
 const imagePath2 = "/path/to/your/logo.png";
 const imageData2 = fs.readFileSync(imagePath2);
 const base64Image2 = imageData2.toString("base64");

 const prompt = [
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image1,
 },
 },
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image2,
 },
 },
 { text: "Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt." },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 });
 for (const part of response.parts) {
 if (part.text) {
 console.log(part.text);
 } else if (part.inlineData) {
 const imageData = part.inlineData.data;
 const buffer = Buffer.from(imageData, "base64");
 fs.writeFileSync("woman_with_logo.png", buffer);
 console.log("Image saved as woman_with_logo.png");
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 imgData1, _ := os.ReadFile("/path/to/your/woman.png")
 imgData2, _ := os.ReadFile("/path/to/your/logo.png")

 parts := []*genai.Part{
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData1,
 },
 },
 &genai.Part{
 InlineData: &genai.Blob{
 MIMEType: "image/png",
 Data: imgData2,
 },
 },
 genai.NewPartFromText("Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt."),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 contents,
 )

 for _, part := range result.Candidates[0].Content.Parts {
 if part.Text != "" {
 fmt.Println(part.Text)
 } else if part.InlineData != nil {
 imageBytes := part.InlineData.Data
 outputFilename := "woman_with_logo.png"
 _ = os.WriteFile(outputFilename, imageBytes, 0644)
 }
 }
 }

### REST

 IMG_PATH1=/path/to/your/woman.png
 IMG_PATH2=/path/to/your/logo.png

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 IMG1_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH1" 2>&1)
 IMG2_BASE64=$(base64 "$B64FLAGS" "$IMG_PATH2" 2>&1)

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d "{
 \"contents\": [{
 \"parts\":[
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG1_BASE64\"
 }
 },
 {
 \"inline_data\": {
 \"mime_type\":\"image/png\",
 \"data\": \"$IMG2_BASE64\"
 }
 },
 {\"text\": \"Take the first image of the woman with brown hair, blue eyes, and a neutral expression. Add the logo from the second image onto her black t-shirt. Ensure the woman's face and features remain completely unchanged. The logo should look like it's naturally printed on the fabric, following the folds of the shirt.\"}
 ]
 }]
 }" \
 | grep -o '"data": "[^"]*"' \
 | cut -d'"' -f4 \
 | base64 --decode > woman_with_logo.png

|----------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Input 1 | Input 2 | Output |
| :woman:A professional headshot of a woman with brown hair and blue eyes... | ![A simple, modern logo with the letters 'G' and 'A'...](https://ai.google.dev/static/gemini-api/docs/images/logo.png)A simple, modern logo with the letters 'G' and 'A'... | ![Take the first image of the woman with brown hair, blue eyes, and a neutral expression...](https://ai.google.dev/static/gemini-api/docs/images/woman_with_logo.png)Take the first image of the woman with brown hair, blue eyes, and a neutral expression... |

### Best Practices

To elevate your results from good to great, incorporate these professional strategies into your workflow.

- **Be Hyper-Specific:**The more detail you provide, the more control you have. Instead of "fantasy armor," describe it: "ornate elven plate armor, etched with silver leaf patterns, with a high collar and pauldrons shaped like falcon wings."
- **Provide Context and Intent:** Explain the*purpose*of the image. The model's understanding of context will influence the final output. For example, "Create a logo for a high-end, minimalist skincare brand" will yield better results than just "Create a logo."
- **Iterate and Refine:**Don't expect a perfect image on the first try. Use the conversational nature of the model to make small changes. Follow up with prompts like, "That's great, but can you make the lighting a bit warmer?" or "Keep everything the same, but change the character's expression to be more serious."
- **Use Step-by-Step Instructions:**For complex scenes with many elements, break your prompt into steps. "First, create a background of a serene, misty forest at dawn. Then, in the foreground, add a moss-covered ancient stone altar. Finally, place a single, glowing sword on top of the altar."
- **Use "Semantic Negative Prompts":**Instead of saying "no cars," describe the desired scene positively: "an empty, deserted street with no signs of traffic."
- **Control the Camera:** Use photographic and cinematic language to control the composition. Terms like`wide-angle shot`,`macro shot`,`low-angle
 perspective`.

## Limitations

- For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.
- Image generation does not support audio or video inputs.
- The model won't always follow the exact number of image outputs that the user explicitly asks for.
- The model works best with up to 3 images as an input.
- When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.
- Uploading images of children is not currently supported in EEA, CH, and UK.
- All generated images include a[SynthID watermark](https://ai.google.dev/responsible/docs/safeguards/synthid).

## Optional configurations

You can optionally configure the response modalities and aspect ratio of the model's output in the`config`field of`generate_content`calls.

### Output types

The model defaults to returning text and image responses (i.e.`response_modalities=['Text', 'Image']`). You can configure the response to return only images without text using`response_modalities=['Image']`.

### Python

 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[prompt],
 config=types.GenerateContentConfig(
 response_modalities=['Image']
 )
 )

### JavaScript

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 config: {
 responseModalities: ['Image']
 }
 });

### Go

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("Create a picture of a nano banana dish in a " +
 " fancy restaurant with a Gemini theme"),
 &genai.GenerateContentConfig{
 ResponseModalities: "Image",
 },
 )

### REST

 -d '{
 "contents": [{
 "parts": [
 {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
 ]
 }],
 "generationConfig": {
 "responseModalities": ["Image"]
 }
 }' \

### Aspect ratios

The model defaults to matching the output image size to that of your input image, or otherwise generates 1:1 squares. You can control the aspect ratio of the output image using the`aspect_ratio`field under`image_config`in the response request, shown here:

### Python

 response = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=[prompt],
 config=types.GenerateContentConfig(
 image_config=types.ImageConfig(
 aspect_ratio="16:9",
 )
 )
 )

### JavaScript

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 contents: prompt,
 config: {
 imageConfig: {
 aspectRatio: "16:9",
 },
 }
 });

### Go

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 genai.Text("Create a picture of a nano banana dish in a " +
 " fancy restaurant with a Gemini theme"),
 &genai.GenerateContentConfig{
 ImageConfig: &genai.ImageConfig{
 AspectRatio: "16:9",
 },
 }
 )

### REST

 -d '{
 "contents": [{
 "parts": [
 {"text": "Create a picture of a nano banana dish in a fancy restaurant with a Gemini theme"}
 ]
 }],
 "generationConfig": {
 "imageConfig": {
 "aspectRatio": "16:9"
 }
 }
 }' \

The different ratios available and the size of the image generated are listed in this table:

| Aspect ratio | Resolution | Tokens |
|--------------|------------|--------|
| 1:1 | 1024x1024 | 1290 |
| 2:3 | 832x1248 | 1290 |
| 3:2 | 1248x832 | 1290 |
| 3:4 | 864x1184 | 1290 |
| 4:3 | 1184x864 | 1290 |
| 4:5 | 896x1152 | 1290 |
| 5:4 | 1152x896 | 1290 |
| 9:16 | 768x1344 | 1290 |
| 16:9 | 1344x768 | 1290 |
| 21:9 | 1536x672 | 1290 |

## When to use Imagen

In addition to using Gemini's built-in image generation capabilities, you can also access[Imagen](https://ai.google.dev/gemini-api/docs/imagen), our specialized image generation model, through the Gemini API.

| Attribute | Imagen | Gemini Native Image |
|-------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Strengths | Most capable image generation model to date. Recommended for photorealistic images, sharper clarity, improved spelling and typography. | **Default recommendation.** Unparalleled flexibility, contextual understanding, and simple, mask-free editing. Uniquely capable of multi-turn conversational editing. |
| Availability | Generally available | Preview (Production usage allowed) |
| Latency | **Low**. Optimized for near-real-time performance. | Higher. More computation is required for its advanced capabilities. |
| Cost | Cost-effective for specialized tasks. $0.02/image to $0.12/image | Token-based pricing. $30 per 1 million tokens for image output (image output tokenized at 1290 tokens per image flat, up to 1024x1024px) |
| Recommended tasks | - Image quality, photorealism, artistic detail, or specific styles (e.g., impressionism, anime) are top priorities. - Infusing branding, style, or generating logos and product designs. - Generating advanced spelling or typography. | - Interleaved text and image generation to seamlessly blend text and images. - Combine creative elements from multiple images with a single prompt. - Make highly specific edits to images, modify individual elements with simple language commands, and iteratively work on an image. - Apply a specific design or texture from one image to another while preserving the original subject's form and details. |

Imagen 4 should be your go-to model starting to generate images with Imagen. Choose Imagen 4 Ultra for advanced use-cases or when you need the best image quality (note that can only generate one image at a time).

## What's next

- Find more examples and code samples in the[cookbook guide](https://colab.sandbox.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Image_out.ipynb).
- Check out the[Veo guide](https://ai.google.dev/gemini-api/docs/video)to learn how to generate videos with the Gemini API.
- To learn more about Gemini models, see[Gemini models](https://ai.google.dev/gemini-api/docs/models/gemini).

[END OF DOCUMENT: GEMINI2PJFPMRLHB]
---

[START OF DOCUMENT: GEMINI1E51EEWV1J | Title: Image-Understanding.Md]

Gemini models are built to be multimodal from the ground up, unlocking a wide range of image processing and computer vision tasks including but not limited to image captioning, classification, and visual question answering without having to train specialized ML models.
| **Tip:** In addition to their general multimodal capabilities, Gemini models (2.0 and newer) offer **improved accuracy** for specific use cases like [object detection](https://ai.google.dev/gemini-api/docs/image-understanding#object-detection) and [segmentation](https://ai.google.dev/gemini-api/docs/image-understanding#segmentation), through additional training. See the [Capabilities](https://ai.google.dev/gemini-api/docs/image-understanding#capabilities) section for more details.

## Passing images to Gemini

You can provide images as input to Gemini using two methods:

- [Passing inline image data](https://ai.google.dev/gemini-api/docs/image-understanding#inline-image): Ideal for smaller files (total request size less than 20MB, including prompts).
- [Uploading images using the File API](https://ai.google.dev/gemini-api/docs/image-understanding#upload-image): Recommended for larger files or for reusing images across multiple requests.

### Passing inline image data

You can pass inline image data in the
request to `generateContent`. You can provide image data as Base64 encoded
strings or by reading local files directly (depending on the language).

The following example shows how to read an image from a local file and pass
it to `generateContent` API for processing.

### Python

 from google import genai
 from google.genai import types

 with open('path/to/small-sample.jpg', 'rb') as f:
 image_bytes = f.read()

 client = genai.Client()
 response = client.models.generate_content(
 model='gemini-2.5-flash',
 contents=[
 types.Part.from_bytes(
 data=image_bytes,
 mime_type='image/jpeg',
 ),
 'Caption this image.'
 ]
 )

 print(response.text)

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 const ai = new GoogleGenAI({});
 const base64ImageFile = fs.readFileSync("path/to/small-sample.jpg", {
 encoding: "base64",
 });

 const contents = [
 {
 inlineData: {
 mimeType: "image/jpeg",
 data: base64ImageFile,
 },
 },
 { text: "Caption this image." },
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: contents,
 });
 console.log(response.text);

### Go

 bytes, _ := os.ReadFile("path/to/small-sample.jpg")

 parts := []*genai.Part{
 genai.NewPartFromBytes(bytes, "image/jpeg"),
 genai.NewPartFromText("Caption this image."),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())

### REST

 IMG_PATH="/path/to/your/image1.jpg"

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {
 "inline_data": {
 "mime_type":"image/jpeg",
 "data": "'"$(base64 $B64FLAGS $IMG_PATH)"'"
 }
 },
 {"text": "Caption this image."},
 ]
 }]
 }' 2> /dev/null

You can also fetch an image from a URL, convert it to bytes, and pass it to
`generateContent` as shown in the following examples.

### Python

 from google import genai
 from google.genai import types

 import requests

 image_path = "https://goo.gle/instrument-img"
 image_bytes = requests.get(image_path).content
 image = types.Part.from_bytes(
 data=image_bytes, mime_type="image/jpeg"
 )

 client = genai.Client()

 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents=["What is this image?", image],
 )

 print(response.text)

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 async function main() {
 const ai = new GoogleGenAI({});

 const imageUrl = "https://goo.gle/instrument-img";

 const response = await fetch(imageUrl);
 const imageArrayBuffer = await response.arrayBuffer();
 const base64ImageData = Buffer.from(imageArrayBuffer).toString('base64');

 const result = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: [
 {
 inlineData: {
 mimeType: 'image/jpeg',
 data: base64ImageData,
 },
 },
 { text: "Caption this image." }
 ],
 });
 console.log(result.text);
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "io"
 "net/http"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 // Download the image.
 imageResp, _ := http.Get("https://goo.gle/instrument-img")

 imageBytes, _ := io.ReadAll(imageResp.Body)

 parts := []*genai.Part{
 genai.NewPartFromBytes(imageBytes, "image/jpeg"),
 genai.NewPartFromText("Caption this image."),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

### REST

 IMG_URL="https://goo.gle/instrument-img"

 MIME_TYPE=$(curl -sIL "$IMG_URL" | grep -i '^content-type:' | awk -F ': ' '{print $2}' | sed 's/\r$//' | head -n 1)
 if [[ -z "$MIME_TYPE" || ! "$MIME_TYPE" == image/* ]]; then
 MIME_TYPE="image/jpeg"
 fi

 # Check for macOS
 if [[ "$(uname)" == "Darwin" ]]; then
 IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -b 0)
 elif [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 IMAGE_B64=$(curl -sL "$IMG_URL" | base64)
 else
 IMAGE_B64=$(curl -sL "$IMG_URL" | base64 -w0)
 fi

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {
 "inline_data": {
 "mime_type":"'"$MIME_TYPE"'",
 "data": "'"$IMAGE_B64"'"
 }
 },
 {"text": "Caption this image."}
 ]
 }]
 }' 2> /dev/null

| **Note:** Inline image data limits your total request size (text prompts, system instructions, and inline bytes) to 20MB. For larger requests, [upload image files](https://ai.google.dev/gemini-api/docs/image-understanding#upload-image) using the File API. Files API is also more efficient for scenarios that use the same image repeatedly.

### Uploading images using the File API

For large files or to be able to use the same image file repeatedly, use the
Files API. The following code uploads an image file and then uses the file in a
call to `generateContent`. See the [Files API guide](https://ai.google.dev/gemini-api/docs/files) for
more information and examples.

### Python

 from google import genai

 client = genai.Client()

 my_file = client.files.upload(file="path/to/sample.jpg")

 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents=[my_file, "Caption this image."],
 )

 print(response.text)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";

 const ai = new GoogleGenAI({});

 async function main() {
 const myfile = await ai.files.upload({
 file: "path/to/sample.jpg",
 config: { mimeType: "image/jpeg" },
 });

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: createUserContent([
 createPartFromUri(myfile.uri, myfile.mimeType),
 "Caption this image.",
 ]),
 });
 console.log(response.text);
 }

 await main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.jpg", nil)

 parts := []*genai.Part{
 genai.NewPartFromText("Caption this image."),
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

### REST

 IMAGE_PATH="path/to/sample.jpg"
 MIME_TYPE=$(file -b --mime-type "${IMAGE_PATH}")
 NUM_BYTES=$(wc -c < "${IMAGE_PATH}")
 DISPLAY_NAME=IMAGE

 tmp_header_file=upload-header.tmp

 # Initial resumable request defining metadata.
 # The upload url is in the response headers dump them to a file.
 curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -D upload-header.tmp \
 -H "X-Goog-Upload-Protocol: resumable" \
 -H "X-Goog-Upload-Command: start" \
 -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
 -H "Content-Type: application/json" \
 -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

 upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
 rm "${tmp_header_file}"

 # Upload the actual bytes.
 curl "${upload_url}" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Offset: 0" \
 -H "X-Goog-Upload-Command: upload, finalize" \
 --data-binary "@${IMAGE_PATH}" 2> /dev/null > file_info.json

 file_uri=$(jq -r ".file.uri" file_info.json)
 echo file_uri=$file_uri

 # Now generate content using that file
 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
 {"text": "Caption this image."}]
 }]
 }' 2> /dev/null > response.json

 cat response.json
 echo

 jq ".candidates[].content.parts[].text" response.json

## Prompting with multiple images

You can provide multiple images in a single prompt by including multiple image
`Part` objects in the `contents` array. These can be a mix of inline data
(local files or URLs) and File API references.

### Python

 from google import genai
 from google.genai import types

 client = genai.Client()

 # Upload the first image
 image1_path = "path/to/image1.jpg"
 uploaded_file = client.files.upload(file=image1_path)

 # Prepare the second image as inline data
 image2_path = "path/to/image2.png"
 with open(image2_path, 'rb') as f:
 img2_bytes = f.read()

 # Create the prompt with text and multiple images
 response = client.models.generate_content(

 model="gemini-2.5-flash",
 contents=[
 "What is different between these two images?",
 uploaded_file, # Use the uploaded file reference
 types.Part.from_bytes(
 data=img2_bytes,
 mime_type='image/png'
 )
 ]
 )

 print(response.text)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";
 import * as fs from "node:fs";

 const ai = new GoogleGenAI({});

 async function main() {
 // Upload the first image
 const image1_path = "path/to/image1.jpg";
 const uploadedFile = await ai.files.upload({
 file: image1_path,
 config: { mimeType: "image/jpeg" },
 });

 // Prepare the second image as inline data
 const image2_path = "path/to/image2.png";
 const base64Image2File = fs.readFileSync(image2_path, {
 encoding: "base64",
 });

 // Create the prompt with text and multiple images

 const response = await ai.models.generateContent({

 model: "gemini-2.5-flash",
 contents: createUserContent([
 "What is different between these two images?",
 createPartFromUri(uploadedFile.uri, uploadedFile.mimeType),
 {
 inlineData: {
 mimeType: "image/png",
 data: base64Image2File,
 },
 },
 ]),
 });
 console.log(response.text);
 }

 await main();

### Go

 // Upload the first image
 image1Path := "path/to/image1.jpg"
 uploadedFile, _ := client.Files.UploadFromPath(ctx, image1Path, nil)

 // Prepare the second image as inline data
 image2Path := "path/to/image2.jpeg"
 imgBytes, _ := os.ReadFile(image2Path)

 parts := []*genai.Part{
 genai.NewPartFromText("What is different between these two images?"),
 genai.NewPartFromBytes(imgBytes, "image/jpeg"),
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())

### REST

 # Upload the first image
 IMAGE1_PATH="path/to/image1.jpg"
 MIME1_TYPE=$(file -b --mime-type "${IMAGE1_PATH}")
 NUM1_BYTES=$(wc -c < "${IMAGE1_PATH}")
 DISPLAY_NAME1=IMAGE1

 tmp_header_file1=upload-header1.tmp

 curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -D upload-header1.tmp \
 -H "X-Goog-Upload-Protocol: resumable" \
 -H "X-Goog-Upload-Command: start" \
 -H "X-Goog-Upload-Header-Content-Length: ${NUM1_BYTES}" \
 -H "X-Goog-Upload-Header-Content-Type: ${MIME1_TYPE}" \
 -H "Content-Type: application/json" \
 -d "{'file': {'display_name': '${DISPLAY_NAME1}'}}" 2> /dev/null

 upload_url1=$(grep -i "x-goog-upload-url: " "${tmp_header_file1}" | cut -d" " -f2 | tr -d "\r")
 rm "${tmp_header_file1}"

 curl "${upload_url1}" \
 -H "Content-Length: ${NUM1_BYTES}" \
 -H "X-Goog-Upload-Offset: 0" \
 -H "X-Goog-Upload-Command: upload, finalize" \
 --data-binary "@${IMAGE1_PATH}" 2> /dev/null > file_info1.json

 file1_uri=$(jq ".file.uri" file_info1.json)
 echo file1_uri=$file1_uri

 # Prepare the second image (inline)
 IMAGE2_PATH="path/to/image2.png"
 MIME2_TYPE=$(file -b --mime-type "${IMAGE2_PATH}")

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi
 IMAGE2_BASE64=$(base64 $B64FLAGS $IMAGE2_PATH)

 # Now generate content using both images
 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {"text": "What is different between these two images?"},
 {"file_data":{"mime_type": "'"${MIME1_TYPE}"'", "file_uri": '$file1_uri'}},
 {
 "inline_data": {
 "mime_type":"'"${MIME2_TYPE}"'",
 "data": "'"$IMAGE2_BASE64"'"
 }
 }
 ]
 }]
 }' 2> /dev/null > response.json

 cat response.json
 echo

 jq ".candidates[].content.parts[].text" response.json

## Object detection

From Gemini 2.0 onwards, models are further trained to detect objects in an
image and get their bounding box coordinates. The coordinates, relative to image
dimensions, scale to \[0, 1000\]. You need to descale these coordinates based on
your original image size.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image
 import json

 client = genai.Client()
 prompt = "Detect the all of the prominent items in the image. The box_2d should be [ymin, xmin, ymax, xmax] normalized to 0-1000."

 image = Image.open("/path/to/image.png")

 config = types.GenerateContentConfig(
 response_mime_type="application/json"
 )

 response = client.models.generate_content(model="gemini-2.5-flash",
 contents=[image, prompt],
 config=config
 )

 width, height = image.size
 bounding_boxes = json.loads(response.text)

 converted_bounding_boxes = []
 for bounding_box in bounding_boxes:
 abs_y1 = int(bounding_box["box_2d"][0]/1000 * height)
 abs_x1 = int(bounding_box["box_2d"][1]/1000 * width)
 abs_y2 = int(bounding_box["box_2d"][2]/1000 * height)
 abs_x2 = int(bounding_box["box_2d"][3]/1000 * width)
 converted_bounding_boxes.append([abs_x1, abs_y1, abs_x2, abs_y2])

 print("Image size: ", width, height)
 print("Bounding boxes:", converted_bounding_boxes)

| **Note:** The model also supports generating bounding boxes based on custom instructions, such as: "Show bounding boxes of all green objects in this image". It also support custom labels like "label the items with the allergens they can contain".

For more examples, check following notebooks in the [Gemini Cookbook](https://github.com/google-gemini/cookbook):

- [2D spatial understanding notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb)
- [Experimental 3D pointing notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/examples/Spatial_understanding_3d.ipynb)

## Segmentation

Starting with Gemini 2.5, models not only detect items but also segment them
and provide their contour masks.

The model predicts a JSON list, where each item represents a segmentation mask.
Each item has a bounding box ("`box_2d`") in the format `[y0, x0, y1, x1]` with
normalized coordinates between 0 and 1000, a label ("`label`") that identifies
the object, and finally the segmentation mask inside the bounding box, as base64
encoded png that is a probability map with values between 0 and 255.
The mask needs to be resized to match the bounding box dimensions, then
binarized at your confidence threshold (127 for the midpoint).
**Note:** For better results, disable [thinking](https://ai.google.dev/gemini-api/docs/thinking) by setting the thinking budget to 0. See code sample below for an example.

### Python

 from google import genai
 from google.genai import types
 from PIL import Image, ImageDraw
 import io
 import base64
 import json
 import numpy as np
 import os

 client = genai.Client()

 def parse_json(json_output: str):
 # Parsing out the markdown fencing
 lines = json_output.splitlines()
 for i, line in enumerate(lines):
 if line == "```json":
          json_output = "\n".join(lines[i+1:])  # Remove everything before "```json"
 output = json_output.split("```")[0]  # Remove everything after the closing "```"
 break # Exit the loop once "```json" is found
 return json_output

 def extract_segmentation_masks(image_path: str, output_dir: str = "segmentation_outputs"):
 # Load and resize image
 im = Image.open(image_path)
 im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)

 prompt = """
 Give the segmentation masks for the wooden and glass items.
 Output a JSON list of segmentation masks where each entry contains the 2D
 bounding box in the key "box_2d", the segmentation mask in key "mask", and
 the text label in the key "label". Use descriptive labels.
 """

 config = types.GenerateContentConfig(
 thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection
 )

 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents=[prompt, im], # Pillow images can be directly passed as inputs (which will be converted by the SDK)
 config=config
 )

 # Parse JSON response
 items = json.loads(parse_json(response.text))

 # Create output directory
 os.makedirs(output_dir, exist_ok=True)

 # Process each mask
 for i, item in enumerate(items):
 # Get bounding box coordinates
 box = item["box_2d"]
 y0 = int(box[0] / 1000 * im.size[1])
 x0 = int(box[1] / 1000 * im.size[0])
 y1 = int(box[2] / 1000 * im.size[1])
 x1 = int(box[3] / 1000 * im.size[0])

 # Skip invalid boxes
 if y0 >= y1 or x0 >= x1:
 continue

 # Process mask
 png_str = item["mask"]
 if not png_str.startswith("data:image/png;base64,"):
 continue

 # Remove prefix
 png_str = png_str.removeprefix("data:image/png;base64,")
 mask_data = base64.b64decode(png_str)
 mask = Image.open(io.BytesIO(mask_data))

 # Resize mask to match bounding box
 mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)

 # Convert mask to numpy array for processing
 mask_array = np.array(mask)

 # Create overlay for this mask
 overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))
 overlay_draw = ImageDraw.Draw(overlay)

 # Create overlay for the mask
 color = (255, 255, 255, 200)
 for y in range(y0, y1):
 for x in range(x0, x1):
 if mask_array[y - y0, x - x0] > 128: # Threshold for mask
 overlay_draw.point((x, y), fill=color)

 # Save individual mask and its overlay
 mask_filename = f"{item['label']}_{i}_mask.png"
 overlay_filename = f"{item['label']}_{i}_overlay.png"

 mask.save(os.path.join(output_dir, mask_filename))

 # Create and save overlay
 composite = Image.alpha_composite(im.convert('RGBA'), overlay)
 composite.save(os.path.join(output_dir, overlay_filename))
 print(f"Saved mask and overlay for {item['label']} to {output_dir}")

 # Example usage
 if __name__ == "__main__":
 extract_segmentation_masks("path/to/image.png")

Check the
[segmentation example](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Spatial_understanding.ipynb#scrollTo=WQJTJ8wdGOKx)
in the cookbook guide for a more detailed example.
![A table with cupcakes, with the wooden and glass objects highlighted](https://ai.google.dev/static/gemini-api/docs/images/segmentation.jpg) An example segmentation output with objects and segmentation masks

## Supported image formats

Gemini supports the following image format MIME types:

- PNG - `image/png`
- JPEG - `image/jpeg`
- WEBP - `image/webp`
- HEIC - `image/heic`
- HEIF - `image/heif`

## Capabilities

All Gemini model versions are multimodal and can be utilized in a wide range of
image processing and computer vision tasks including but not limited to image captioning,
visual question and answering, image classification, object detection and segmentation.

Gemini can reduce the need to use specialized ML models depending on your quality and performance requirements.

Some later model versions are specifically trained improve accuracy of specialized tasks in addition to generic capabilities:

- **Gemini 2.0 models** are further trained to support enhanced [object detection](https://ai.google.dev/gemini-api/docs/image-understanding#object-detection).

- **Gemini 2.5 models** are further trained to support enhanced [segmentation](https://ai.google.dev/gemini-api/docs/image-understanding#segmentation) in addition to [object detection](https://ai.google.dev/gemini-api/docs/image-understanding#object-detection).

## Limitations and key technical information

### File limit

Gemini 2.5 Pro/Flash, 2.0 Flash, 1.5 Pro, and 1.5 Flash support a
maximum of 3,600 image files per request.

### Token calculation

- **Gemini 1.5 Flash and Gemini 1.5 Pro**: 258 tokens if both dimensions \<= 384 pixels. Larger images are tiled (min tile 256px, max 768px, resized to 768x768), with each tile costing 258 tokens.
- **Gemini 2.0 Flash and Gemini 2.5 Flash/Pro**: 258 tokens if both dimensions \<= 384 pixels. Larger images are tiled into 768x768 pixel tiles, each costing 258 tokens.

A rough formula for calculating the number of tiles is as follows:

- Calculate the crop unit size which is roughly: floor(min(width, height) / 1.5).
- Divide each dimension by the crop unit size and multiply together to get the number of tiles.

For example, for an image of dimensions 960x540 would have a crop unit size
of 360. Divide each dimension by 360 and the number of tile is 3 \* 2 = 6.

## Tips and best practices

- Verify that images are correctly rotated.
- Use clear, non-blurry images.
- When using a single image with text, place the text prompt *after* the image part in the `contents` array.

## What's next

This guide shows you how to upload image files and generate text outputs from image
inputs. To learn more, see the following resources:

- [Files API](https://ai.google.dev/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
- [System instructions](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
- [File prompting strategies](https://ai.google.dev/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
- [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

[END OF DOCUMENT: GEMINI1E51EEWV1J]
---

[START OF DOCUMENT: GEMINI2MKGH0X99T | Title: Imagen.Md]

<br />

Imagen is Google's high-fidelity image generation model, capable of generating
realistic and high quality images from text prompts. All generated images
include a SynthID watermark. To learn more about the available Imagen model
variants, see the [Model versions](https://ai.google.dev/gemini-api/docs/imagen#model-versions) section.
| **Note:** You can also generate images with Gemini's built-in multimodal capabilities. See the [Image generation
| guide](https://ai.google.dev/gemini-api/docs/image-generation) for details.

## Generate images using the Imagen models

This example demonstrates generating images with an [Imagen model](https://deepmind.google/technologies/imagen-3/):

### Python

 from google import genai
 from google.genai import types
 from PIL import Image
 from io import BytesIO

 client = genai.Client()

 response = client.models.generate_images(
 model='imagen-4.0-generate-001',
 prompt='Robot holding a red skateboard',
 config=types.GenerateImagesConfig(
 number_of_images= 4,
 )
 )
 for generated_image in response.generated_images:
 generated_image.image.show()

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 async function main() {

 const ai = new GoogleGenAI({});

 const response = await ai.models.generateImages({
 model: 'imagen-4.0-generate-001',
 prompt: 'Robot holding a red skateboard',
 config: {
 numberOfImages: 4,
 },
 });

 let idx = 1;
 for (const generatedImage of response.generatedImages) {
 let imgBytes = generatedImage.image.imageBytes;
 const buffer = Buffer.from(imgBytes, "base64");
 fs.writeFileSync(`imagen-${idx}.png`, buffer);
 idx++;
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {

 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 config := &genai.GenerateImagesConfig{
 NumberOfImages: 4,
 }

 response, _ := client.Models.GenerateImages(
 ctx,
 "imagen-4.0-generate-001",
 "Robot holding a red skateboard",
 config,
 )

 for n, image := range response.GeneratedImages {
 fname := fmt.Sprintf("imagen-%d.png", n)
 _ = os.WriteFile(fname, image.Image.ImageBytes, 0644)
 }
 }

### REST

 curl -X POST \
 "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-generate-001:predict" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -d '{
 "instances": [
 {
 "prompt": "Robot holding a red skateboard"
 }
 ],
 "parameters": {
 "sampleCount": 4
 }
 }'

![AI-generated image of a robot holding a red skateboard](https://ai.google.dev/static/gemini-api/docs/images/robot-skateboard.png) AI-generated image of a robot holding a red skateboard

### Imagen configuration

Imagen supports English only prompts at this time and the following parameters:
| **Note:** Naming conventions of parameters vary by programming language.

- `numberOfImages`: The number of images to generate, from 1 to 4 (inclusive). The default is 4.
- `imageSize`: The size of the generated image. This is only supported for the Standard and Ultra models. The supported values are `1K` and `2K`. Default is `1K`.
- `aspectRatio`: Changes the aspect ratio of the generated image. Supported values are `"1:1"`, `"3:4"`, `"4:3"`, `"9:16"`, and `"16:9"`. The default is `"1:1"`.
- `personGeneration`: Allow the model to generate images of people. The
 following values are supported:

 - `"dont_allow"`: Block generation of images of people.
 - `"allow_adult"`: Generate images of adults, but not children. This is the default.
 - `"allow_all"`: Generate images that include adults and children.

 | **Note:** The `"allow_all"` parameter value is not allowed in EU, UK, CH, MENA locations.

## Imagen prompt guide

This section of the Imagen guide shows you how modifying a text-to-image prompt
can produce different results, along with examples of images you can create.

### Prompt writing basics

| **Note:** Maximum prompt length is 480 tokens.

A good prompt is descriptive and clear, and makes use of meaningful keywords and
modifiers. Start by thinking of your **subject** , **context** , and **style**.
![Prompt with subject, context, and style emphasized](https://ai.google.dev/static/gemini-api/docs/images/imagen/style-subject-context.png) Image text: A *sketch* (**style** ) of a *modern apartment building* (**subject** ) surrounded by *skyscrapers* (**context and background**).

1. **Subject** : The first thing to think about with any prompt is the
 *subject*: the object, person, animal, or scenery you want an image of.

2. **Context and background:** Just as important is the *background or context*
 in which the subject will be placed. Try placing your subject in a variety
 of backgrounds. For example, a studio with a white background, outdoors, or
 indoor environments.

3. **Style:** Finally, add the style of image you want. *Styles* can be general
 (painting, photograph, sketches) or very specific (pastel painting, charcoal
 drawing, isometric 3D). You can also combine styles.

After you write a first version of your prompt, refine your prompt by adding
more details until you get to the image that you want. Iteration is important.
Start by establishing your core idea, and then refine and expand upon that core
idea until the generated image is close to your vision.

|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![photorealistic sample image 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_short.png) Prompt: A park in the spring next to a lake | ![photorealistic sample image 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_medium.png) Prompt: A park in the spring next to a lake, **the sun sets across the lake, golden hour** | ![photorealistic sample image 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/0_prompt-writing-basics_park_long.png) Prompt: A park in the spring next to a lake, ***the sun sets across the lake, golden hour, red wildflowers*** |

Imagen models can transform your ideas into detailed images, whether
your prompts are short or long and detailed. Refine your vision
through iterative prompting, adding details until you achieve the perfect
result.

|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Short prompts let you generate an image quickly. ![Imagen 3 short prompt example](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_short-prompt.png) Prompt: close-up photo of a woman in her 20s, street photography, movie still, muted orange warm tones | Longer prompts let you add specific details and build your image. ![Imagen 3 long prompt example](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_long-prompt.png) Prompt: captivating photo of a woman in her 20s utilizing a street photography style. The image should look like a movie still with muted orange warm tones. |

Additional advice for Imagen prompt writing:

- **Use descriptive language**: Employ detailed adjectives and adverbs to paint a clear picture for Imagen.
- **Provide context**: If necessary, include background information to aid the AI's understanding.
- **Reference specific artists or styles**: If you have a particular aesthetic in mind, referencing specific artists or art movements can be helpful.
- **Use prompt engineering tools**: Consider exploring prompt engineering tools or resources to help you refine your prompts and achieve optimal results.
- **Enhancing the facial details in your personal and group images**: Specify facial details as a focus of the photo (for example, use the word "portrait" in the prompt).

### Generate text in images

Imagen models can add text into images, opening up more creative image generation
possibilities. Use the following guidance to get the most out of this feature:

- **Iterate with confidence**: You might have to regenerate images until you achieve the look you want. Imagen's text integration is still evolving, and sometimes multiple attempts yield the best results.
- **Keep it short**: Limit text to 25 characters or less for optimal generation.
- **Multiple phrases**: Experiment with two or three distinct phrases to
 provide additional information. Avoid exceeding three phrases for cleaner
 compositions.

 ![Imagen 3 generate text example](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_generate-text.png) Prompt: A poster with the text "Summerland" in bold font as a title, underneath this text is the slogan "Summer never felt so good"
- **Guide Placement**: While Imagen can attempt to position text
 as directed, expect occasional variations. This feature is continually
 improving.

- **Inspire font style**: Specify a general font style to subtly influence
 Imagen's choices. Don't rely on precise font replication, but expect
 creative interpretations.

- **Font size** : Specify a font size or a general indication of size (for
 example, *small* , *medium* , *large*) to influence the font size generation.

### Prompt parameterization

To better control output results, you might find it helpful to parameterize the
inputs into Imagen. For example, suppose you
want your customers to be able to generate logos for their business, and you
want to make sure logos are always generated on a solid color background. You
also want to limit the options that the client can select from a menu.

In this example, you can create a parameterized prompt similar to the
following:```
A {logo_style} logo for a {company_area} company on a solid color background. Include the text {company_name}.
```In your custom user interface, the customer can input the parameters using
a menu, and their chosen value populates the prompt Imagen receives.

For example:

1. Prompt: `A `<var translate="no">minimalist</var>` logo for a `<var translate="no">health care</var>` company on a solid color background. Include the text `<var translate="no">Journey</var>`.`

 ![Imagen 3 prompt parameterization example 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_healthcare.png)
2. Prompt: `A `<var translate="no">modern</var>` logo for a `<var translate="no">software</var>` company on a solid color background. Include the text `<var translate="no">Silo</var>`.`

 ![Imagen 3 prompt parameterization example 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_software.png)
3. Prompt: `A `<var translate="no">traditional</var>` logo for a `<var translate="no">baking</var>` company on a solid color background. Include the text `<var translate="no">Seed</var>`.`

 ![Imagen 3 prompt parameterization example 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/imagen3_prompt-param_baking.png)

### Advanced prompt writing techniques

Use the following examples to create more specific prompts based on attributes
like photography descriptors, shapes and materials, historical art
movements, and image quality modifiers.

#### Photography

- Prompt includes: *"A photo of..."*

To use this style, start with using keywords that clearly tell
Imagen that you're looking for a photograph. Start your prompts with
*"A photo of. . ."*. For example:

|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![photorealistic sample image 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_coffee-beans.png) Prompt: **A photo of** coffee beans in a kitchen on a wooden surface | ![photorealistic sample image 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_chocolate-bar.png) Prompt: **A photo of** a chocolate bar on a kitchen counter | ![photorealistic sample image 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/1_style-photography_modern-building.png) Prompt: **A photo of** a modern building with water in the background |

^Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.^

##### Photography modifiers

In the following examples, you can see several photography-specific modifiers
and parameters. You can combine multiple modifiers for more precise control.

1. **Camera Proximity** - *Close up, taken from far away*

 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![close up camera sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_close-up.png) Prompt: A **close-up** photo of coffee beans | ![zoomed out camera sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/3_camera-proximity_zoomed-out.png) Prompt: A **zoomed out** photo of a small bag of coffee beans in a messy kitchen |

 <br />

2. **Camera Position** - *aerial, from below*

 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![aerial photo sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_aerial-photo.png) Prompt: **aerial photo** of urban city with skyscrapers | ![a view from underneath sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/4_camera-position_from-below.png) Prompt: A photo of a forest canopy with blue skies **from below** |

3. **Lighting** - *natural, dramatic, warm, cold*

 |-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![natural lighting sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_natural-lighting.png) Prompt: studio photo of a modern arm chair, **natural lighting** | ![dramatic lighting sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/5_lighting_dramatic-lighting.png) Prompt: studio photo of a modern arm chair, **dramatic lighting** |

4. **Camera Settings** *- motion blur, soft focus, bokeh, portrait*

 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![motion blur sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_motion-blur.png) Prompt: photo of a city with skyscrapers from the inside of a car with **motion blur** | ![soft focus sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/6_camera-settings_soft-focus.png) Prompt: **soft focus** photograph of a bridge in an urban city at night |

5. **Lens types** - *35mm, 50mm, fisheye, wide angle, macro*

 |------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![macro lens sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_macro-lens.png) Prompt: photo of a leaf, **macro lens** | ![fisheye lens sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/7_lens-types_fisheye-lens.png) Prompt: street photography, new york city, **fisheye lens** |

6. **Film types** - *black and white, polaroid*

 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![polaroid photo sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_polaroid-portrait.png) Prompt: a **polaroid portrait** of a dog wearing sunglasses | ![black and white photo sample image](https://ai.google.dev/static/gemini-api/docs/images/imagen/8_film-types_bw-photo.png) Prompt: **black and white photo** of a dog wearing sunglasses |

^Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.^

### Illustration and art

- Prompt includes: *"A <var translate="no">painting</var> of..."* , *"A <var translate="no">sketch</var> of..."*

Art styles vary from monochrome styles like pencil sketches, to hyper-realistic
digital art. For example, the following images use the same prompt with
different styles:

*"An <var translate="no">[art style or creation technique]</var> of an angular
sporty electric sedan with skyscrapers in the background"*

|---------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![art sample images](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1A.png) Prompt: A **technical pencil drawing** of an angular... | ![art sample images](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1B.png) Prompt: A **charcoal drawing** of an angular... | ![art sample images](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration1C.png) Prompt: A **color pencil drawing** of an angular... |

|------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![art sample images](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2E.png) Prompt: A **pastel painting** of an angular... | ![art sample images](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2F.png) Prompt: A **digital art** of an angular... | ![art sample images](https://ai.google.dev/static/gemini-api/docs/images/imagen/2_style-illustration2G.png) Prompt: An **art deco (poster)** of an angular... |

^Image source: Each image was generated using its corresponding text prompt with the Imagen 2 model.^

##### Shapes and materials

- Prompt includes: *"...made of..."* , *"...in the shape of..."*

One of the strengths of this technology is that you can create imagery that
is otherwise difficult or impossible. For example, you can recreate
your company logo in different materials and textures.

|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![shapes and materials example image 1](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_duffel.png) Prompt: a duffle bag **made of** cheese | ![shapes and materials example image 2](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_bird.png) Prompt: neon tubes **in the shape** of a bird | ![shapes and materials example image 3](https://ai.google.dev/static/gemini-api/docs/images/imagen/9_shapes-materials_paper.png) Prompt: an armchair **made of paper**, studio photo, origami style |

^Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.^

#### Historical art references

- Prompt includes: *"...in the style of..."*

Certain styles have become iconic over the years. The following are some ideas
of historical painting or art styles that you can try.

*"generate an image in the style of <var translate="no">[art period or movement] </var>: a wind farm"*

|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![impressionism example image](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_impressionism.png) Prompt: generate an image **in the style of *an impressionist painting***: a wind farm | ![renaissance example image](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_renaissance.png) Prompt: generate an image **in the style of *a renaissance painting***: a wind farm | ![pop art example image](https://ai.google.dev/static/gemini-api/docs/images/imagen/10_historical-ref1_pop-art.png) Prompt: generate an image **in the style of *pop art***: a wind farm |

^Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.^

#### Image quality modifiers

Certain keywords can let the model know that you're looking for a high-quality
asset. Examples of quality modifiers include the following:

- **General Modifiers** - *high-quality, beautiful, stylized*
- **Photos** - *4K, HDR, Studio Photo*
- **Art, Illustration** - *by a professional, detailed*

The following are a few examples of prompts without quality modifiers and
the same prompt with quality modifiers.

|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![corn example image without modifiers](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_no-mods.png) Prompt (no quality modifiers): a photo of a corn stalk | ![corn example image with modifiers](https://ai.google.dev/static/gemini-api/docs/images/imagen/11_quality-modifier2_4k-hdr.png) Prompt (with quality modifiers): **4k HDR beautiful** photo of a corn stalk **taken by a professional photographer** |

^Image source: Each image was generated using its corresponding text prompt with the Imagen 3 model.^

#### Aspect ratios

Imagen image generation lets you set five distinct image aspect
ratios.

1. **Square** (1:1, default) - A standard square photo. Common uses for this aspect ratio include social media posts.
2. **Fullscreen** (4:3) - This aspect ratio is commonly used in media or film.
 It is also the dimensions of most old (non-widescreen) TVs and medium format
 cameras. It captures more of the scene horizontally (compared to 1:1),
 making it a preferred aspect ratio for photography.

 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![aspect ratio example](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_piano.png) Prompt: close up of a musician's fingers playing the piano, black and white film, vintage (4:3 aspect ratio) | ![aspect ratio example](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_4-3_fries.png) Prompt: A professional studio photo of french fries for a high end restaurant, in the style of a food magazine (4:3 aspect ratio) |

3. **Portrait full screen** (3:4) - This is the fullscreen aspect ratio rotated
 90 degrees. This lets to capture more of the scene vertically compared to
 the 1:1 aspect ratio.

 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | ![aspect ratio example](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_hiking.png) Prompt: a woman hiking, close of her boots reflected in a puddle, large mountains in the background, in the style of an advertisement, dramatic angles (3:4 aspect ratio) | ![aspect ratio example](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_3-4_valley.png) Prompt: aerial shot of a river flowing up a mystical valley (3:4 aspect ratio) |

4. **Widescreen** (16:9) - This ratio has replaced 4:3 and is now the most
 common aspect ratio for TVs, monitors, and mobile phone screens (landscape).
 Use this aspect ratio when you want to capture more of the background (for
 example, scenic landscapes).

 ![aspect ratio example](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_16-9_man.png) Prompt: a man wearing all white clothing sitting on the beach, close up, golden hour lighting (16:9 aspect ratio)
5. **Portrait** (9:16) - This ratio is widescreen but rotated. This a
 relatively new aspect ratio that has been popularized by short form video
 apps (for example, YouTube shorts). Use this for tall objects with strong
 vertical orientations such as buildings, trees, waterfalls, or other similar
 objects.

 ![aspect ratio example](https://ai.google.dev/static/gemini-api/docs/images/imagen/aspect-ratios_9-16_skyscraper.png) Prompt: a digital render of a massive skyscraper, modern, grand, epic with a beautiful sunset in the background (9:16 aspect ratio)

#### Photorealistic images

Different versions of the image generation
model might offer a mix of artistic and photorealistic output. Use the following
wording in prompts to generate more photorealistic output, based on the subject
you want to generate.
| **Note:** Take these keywords as general guidance when you try to create photorealistic images. They aren't required to achieve your goal.

| Use case | Lens type | Focal lengths | Additional details |
|---------------------------------------------|----------------|---------------|-------------------------------------------------------------------------------|
| People (portraits) | Prime, zoom | 24-35mm | black and white film, Film noir, Depth of field, duotone (mention two colors) |
| Food, insects, plants (objects, still life) | Macro | 60-105mm | High detail, precise focusing, controlled lighting |
| Sports, wildlife (motion) | Telephoto zoom | 100-400mm | Fast shutter speed, Action or movement tracking |
| Astronomical, landscape (wide-angle) | Wide-angle | 10-24mm | Long exposure times, sharp focus, long exposure, smooth water or clouds |

##### Portraits

| Use case | Lens type | Focal lengths | Additional details |
|--------------------|-------------|---------------|-------------------------------------------------------------------------------|
| People (portraits) | Prime, zoom | 24-35mm | black and white film, Film noir, Depth of field, duotone (mention two colors) |

Using several keywords from the table, Imagen can generate the following
portraits:

|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray1.png) | ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray2.png) | ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray3.png) | ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_blue-gray4.png) |

Prompt: *A woman, 35mm portrait, blue and grey duotones*

Model: `imagen-3.0-generate-002`

|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir1.png) | ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir2.png) | ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir3.png) | ![portrait photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/portrait_film-noir4.png) |

Prompt: *A woman, 35mm portrait, film noir*

Model: `imagen-3.0-generate-002`

##### Objects

| Use case | Lens type | Focal lengths | Additional details |
|---------------------------------------------|-----------|---------------|----------------------------------------------------|
| Food, insects, plants (objects, still life) | Macro | 60-105mm | High detail, precise focusing, controlled lighting |

Using several keywords from the table, Imagen can
generate the following object images:

|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf1.png) | ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf2.png) | ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf3.png) | ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_leaf4.png) |

Prompt: *leaf of a prayer plant, macro lens, 60mm*

Model: `imagen-3.0-generate-002`

|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta1.png) | ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta2.png) | ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta3.png) | ![object photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/object_pasta4.png) |

Prompt: *a plate of pasta, 100mm Macro lens*

Model: `imagen-3.0-generate-002`

##### Motion

| Use case | Lens type | Focal lengths | Additional details |
|---------------------------|----------------|---------------|-------------------------------------------------|
| Sports, wildlife (motion) | Telephoto zoom | 100-400mm | Fast shutter speed, Action or movement tracking |

Using several keywords from the table, Imagen can
generate the following motion images:

|----------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football1.png) | ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football2.png) | ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football3.png) | ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_football4.png) |

Prompt: *a winning touchdown, fast shutter speed, movement tracking*

Model: `imagen-3.0-generate-002`

|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------|
| ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer1.png) | ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer2.png) | ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer3.png) | ![motion photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/motion_deer4.png) |

Prompt: *A deer running in the forest, fast shutter speed, movement tracking*

Model: `imagen-3.0-generate-002`

##### Wide-angle

| Use case | Lens type | Focal lengths | Additional details |
|--------------------------------------|------------|---------------|-------------------------------------------------------------------------|
| Astronomical, landscape (wide-angle) | Wide-angle | 10-24mm | Long exposure times, sharp focus, long exposure, smooth water or clouds |

Using several keywords from the table, Imagen can
generate the following wide-angle images:

|------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|
| ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain1.png) | ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain2.png) | ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain3.png) | ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_mountain4.png) |

Prompt: *an expansive mountain range, landscape wide angle 10mm*

Model: `imagen-3.0-generate-002`

|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------|
| ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro1.png) | ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro2.png) | ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro3.png) | ![wide-angle photography example](https://ai.google.dev/static/gemini-api/docs/images/imagen/wide-angle_astro4.png) |

Prompt: *a photo of the moon, astro photography, wide angle 10mm*

Model: `imagen-3.0-generate-002`

## Model versions

### Imagen 4

| Property | Description |
|-------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|
| id_cardModel code | **Gemini API** `imagen-4.0-generate-001` `imagen-4.0-ultra-generate-001` `imagen-4.0-fast-generate-001` |
| saveSupported data types | **Input** Text **Output** Images |
| token_autoToken limits^[\[\*\]](https://ai.google.dev/gemini-api/docs/imagen#token-size)^ | **Input token limit** 480 tokens (text) **Output images** 1 to 4 (Ultra/Standard/Fast) |
| calendar_monthLatest update | June 2025 |

### Imagen 3

| Property | Description |
|-------------------------------------------------------------------------------------------|-----------------------------------------------------|
| id_cardModel code | **Gemini API** `imagen-3.0-generate-002` |
| saveSupported data types | **Input** Text **Output** Images |
| token_autoToken limits^[\[\*\]](https://ai.google.dev/gemini-api/docs/imagen#token-size)^ | **Input token limit** N/A **Output images** Up to 4 |
| calendar_monthLatest update | February 2025 |

[END OF DOCUMENT: GEMINI2MKGH0X99T]
---

[START OF DOCUMENT: GEMINIDXXRQ7EXM | Title: Langgraph-Example.Md]

LangGraph is a framework for building stateful LLM applications, making it a good choice for constructing ReAct (Reasoning and Acting) Agents.

ReAct agents combine LLM reasoning with action execution. They iteratively think, use tools, and act on observations to achieve user goals, dynamically adapting their approach. Introduced in ["ReAct: Synergizing Reasoning and Acting in Language Models"](https://arxiv.org/abs/2210.03629) (2023), this pattern tries to mirror human-like, flexible problem-solving over rigid workflows.

While LangGraph offers a prebuilt ReAct agent ([`create_react_agent`](https://langchain-ai.github.io/langgraph/reference/prebuilt/#langgraph.prebuilt.chat_agent_executor.create_react_agent)), it shines when you need more control and customization for your ReAct implementations.

LangGraph models agents as graphs using three key components:

- `State`: Shared data structure (typically `TypedDict` or `Pydantic BaseModel`) representing the application's current snapshot.
- `Nodes`: Encodes logic of your agents. They receive the current State as input, perform some computation or side-effect, and return an updated State, such as LLM calls or tool calls.
- `Edges`: Define the next `Node` to execute based on the current `State`, allowing for conditional logic and fixed transitions.

If you don't have an API Key yet, you can get one for free at the [Google AI Studio](https://aistudio.google.com/app/apikey).

 pip install langgraph langchain-google-genai geopy requests

Set your API key in the environment variable `GEMINI_API_KEY`.

 import os

 # Read your API key from the environment variable or set it manually
 api_key = os.getenv("GEMINI_API_KEY")

To better understand how to implement a ReAct agent using LangGraph, let's walk through a practical example. You will create a simple agent whose goal is to use a tool to find the current weather for a specified location.

For this weather agent, its `State` will need to maintain the ongoing conversation history (as a list of messages) and a counter for the number of steps taken to further illustrate state management.

LangGraph provides a convenient helper, `add_messages`, for updating message lists in the state. It functions as a [reducer](https://langchain-ai.github.io/langgraph/concepts/low_level/#reducers), meaning it takes the current list and new messages, then returns a combined list. It smartly handles updates by message ID and defaults to an "append-only" behavior for new, unique messages.
**Note:** Since having a list of messages in the state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages.

 from typing import Annotated,Sequence, TypedDict

 from langchain_core.messages import BaseMessage
 from langgraph.graph.message import add_messages # helper function to add messages to the state

 class AgentState(TypedDict):
 """The state of the agent."""
 messages: Annotated[Sequence[BaseMessage], add_messages]
 number_of_steps: int

Next, you define your weather tool.

 from langchain_core.tools import tool
 from geopy.geocoders import Nominatim
 from pydantic import BaseModel, Field
 import requests

 geolocator = Nominatim(user_agent="weather-app")

 class SearchInput(BaseModel):
 location:str = Field(description="The city and state, e.g., San Francisco")
 date:str = Field(description="the forecasting date for when to get the weather format (yyyy-mm-dd)")

 @tool("get_weather_forecast", args_schema=SearchInput, return_direct=True)
 def get_weather_forecast(location: str, date: str):
 """Retrieves the weather using Open-Meteo API for a given location (city) and a date (yyyy-mm-dd). Returns a list dictionary with the time and temperature for each hour."""
 location = geolocator.geocode(location)
 if location:
 try:
 response = requests.get(f"https://api.open-meteo.com/v1/forecast?latitude={location.latitude}&longitude={location.longitude}&hourly=temperature_2m&start_date={date}&end_date={date}")
 data = response.json()
 return {time: temp for time, temp in zip(data["hourly"]["time"], data["hourly"]["temperature_2m"])}
 except Exception as e:
 return {"error": str(e)}
 else:
 return {"error": "Location not found"}

 tools = [get_weather_forecast]

Next, you initialize your model and bind the tools to the model.

 from datetime import datetime
 from langchain_google_genai import ChatGoogleGenerativeAI

 # Create LLM class
 llm = ChatGoogleGenerativeAI(
 model= "gemini-2.5-pro",
 temperature=1.0,
 max_retries=2,
 google_api_key=api_key,
 )

 # Bind tools to the model
 model = llm.bind_tools([get_weather_forecast])

 # Test the model with tools
 res=model.invoke(f"What is the weather in Berlin on {datetime.today()}?")

 print(res)

The last step before you can run your agent is to define your nodes and edges. In this example, you have two nodes and one edge.
- `call_tool` node that executes your tool method. LangGraph has a prebuilt node for this called [ToolNode](https://langchain-ai.github.io/langgraph/how-tos/tool-calling/).
- `call_model` node that uses the `model_with_tools` to call the model.
- `should_continue` edge that decides whether to call the tool or the model.

The number of nodes and edges is not fixed. You can add as many nodes and edges as you want to your graph. For example, you could add a node for adding structured output or a self-verification/reflection node to check the model output before calling the tool or the model.

 from langchain_core.messages import ToolMessage
 from langchain_core.runnables import RunnableConfig

 tools_by_name = {tool.name: tool for tool in tools}

 # Define our tool node
 def call_tool(state: AgentState):
 outputs = []
 # Iterate over the tool calls in the last message
 for tool_call in state["messages"][-1].tool_calls:
 # Get the tool by name
 tool_result = tools_by_name[tool_call["name"]].invoke(tool_call["args"])
 outputs.append(
 ToolMessage(
 content=tool_result,
 name=tool_call["name"],
 tool_call_id=tool_call["id"],
 )
 )
 return {"messages": outputs}

 def call_model(
 state: AgentState,
 config: RunnableConfig,
 ):
 # Invoke the model with the system prompt and the messages
 response = model.invoke(state["messages"], config)
 # We return a list, because this will get added to the existing messages state using the add_messages reducer
 return {"messages": [response]}

 # Define the conditional edge that determines whether to continue or not
 def should_continue(state: AgentState):
 messages = state["messages"]
 # If the last message is not a tool call, then we finish
 if not messages[-1].tool_calls:
 return "end"
 # default to continue
 return "continue"

Now you have all the components to build your agent. Let's put them together.

 from langgraph.graph import StateGraph, END

 # Define a new graph with our state
 workflow = StateGraph(AgentState)

 # 1. Add our nodes
 workflow.add_node("llm", call_model)
 workflow.add_node("tools", call_tool)
 # 2. Set the entrypoint as `agent`, this is the first node called
 workflow.set_entry_point("llm")
 # 3. Add a conditional edge after the `llm` node is called.
 workflow.add_conditional_edges(
 # Edge is used after the `llm` node is called.
 "llm",
 # The function that will determine which node is called next.
 should_continue,
 # Mapping for where to go next, keys are strings from the function return, and the values are other nodes.
 # END is a special node marking that the graph is finish.
 {
 # If `tools`, then we call the tool node.
 "continue": "tools",
 # Otherwise we finish.
 "end": END,
 },
 )
 # 4. Add a normal edge after `tools` is called, `llm` node is called next.
 workflow.add_edge("tools", "llm")

 # Now we can compile and visualize our graph
 graph = workflow.compile()

You can visualize your graph using the `draw_mermaid_png` method.

 from IPython.display import Image, display

 display(Image(graph.get_graph().draw_mermaid_png()))

![png](https://ai.google.dev/static/gemini-api/docs/images/langgraph-react-agent_16_0.png)

Now let's run the agent.

 from datetime import datetime
 # Create our initial message dictionary
 inputs = {"messages": [("user", f"What is the weather in Berlin on {datetime.today()}?")]}

 # call our graph with streaming to see the steps
 for state in graph.stream(inputs, stream_mode="values"):
 last_message = state["messages"][-1]
 last_message.pretty_print()

You can now continue with your conversation and for example ask for the weather in another city or let it compare it.

 state["messages"].append(("user", "Would it be in Munich warmer?"))

 for state in graph.stream(state, stream_mode="values"):
 last_message = state["messages"][-1]
 last_message.pretty_print()

[END OF DOCUMENT: GEMINIDXXRQ7EXM]
---

