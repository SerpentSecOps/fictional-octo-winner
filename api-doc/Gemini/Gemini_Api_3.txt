
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: GEMINI5UPTD6OMJ (sha256-0f04a7cf932bfc22af2dd118d5275f1a02a157e06db1709969afaf0f2f4fb1a7) | Title: Embeddings.Md]
[DocID: GEMINI15TSE07OK0 (sha256-6b51fbf670101df56bc2b8444ffe4631885e43050da3d456aaa67a47c9f96254) | Title: Ephemeral-Tokens.Md]
[DocID: GEMINI1BNX91CDID (sha256-7a4c051a0465581084f0cee41cf6e497c1e00f344441364e37ce3b6f4874233e) | Title: File-Search.Md]
[DocID: GEMINI1O1Y8T19V9 (sha256-9a160d37a5352adedfd8b99b87b8e0c6c2a74fb8dbd1db8b0eb0520aa2111309) | Title: Files.Md]
[DocID: GEMINI5WBTXZ7SI (sha256-0f220f2e17326e141da8fec20499e195c4c2350929309ec25317c3b104258bd1) | Title: Function-Calling.Md]
--- END OF TOC ---

[START OF DOCUMENT: GEMINI5UPTD6OMJ | Title: Embeddings.Md]

<br />

The Gemini API offers text embedding models to generate embeddings for words,
phrases, sentences, and code. These foundational embeddings power advanced NLP
tasks such as semantic search, classification, and clustering, providing more
accurate, context-aware results than keyword-based approaches.

Building Retrieval Augmented Generation (RAG) systems is a common use case for
embeddings. Embeddings play a key role in significantly enhancing model outputs
with improved factual accuracy, coherence, and contextual richness. They
efficiently retrieve relevant information from knowledge bases, represented by
embeddings, which are then passed as additional context in the input prompt to
language models, guiding it to generate more informed and accurate responses.

To learn more about the available embedding model variants, see the [Model
versions](https://ai.google.dev/gemini-api/docs/embeddings#model-versions) section. For higher throughput serving at half the
price, try [Batch API Embedding](https://ai.google.dev/gemini-api/docs/embeddings#batch-embedding).

## Generating embeddings

Use the `embedContent` method to generate text embeddings:

### Python

 from google import genai

 client = genai.Client()

 result = client.models.embed_content(
 model="gemini-embedding-001",
 contents="What is the meaning of life?")

 print(result.embeddings)

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 async function main() {

 const ai = new GoogleGenAI({});

 const response = await ai.models.embedContent({
 model: 'gemini-embedding-001',
 contents: 'What is the meaning of life?',
 });

 console.log(response.embeddings);
 }

 main();

### Go

 package main

 import (
 "context"
 "encoding/json"
 "fmt"
 "log"

 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 contents := []*genai.Content{
 genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
 }
 result, err := client.Models.EmbedContent(ctx,
 "gemini-embedding-001",
 contents,
 nil,
 )
 if err != nil {
 log.Fatal(err)
 }

 embeddings, err := json.MarshalIndent(result.Embeddings, "", " ")
 if err != nil {
 log.Fatal(err)
 }
 fmt.Println(string(embeddings))
 }

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d '{"model": "models/gemini-embedding-001",
 "content": {"parts":[{"text": "What is the meaning of life?"}]}
 }'

You can also generate embeddings for multiple chunks at once by passing them in
as a list of strings.

### Python

 from google import genai

 client = genai.Client()

 result = client.models.embed_content(
 model="gemini-embedding-001",
 contents= [
 "What is the meaning of life?",
 "What is the purpose of existence?",
 "How do I bake a cake?"
 ])

 for embedding in result.embeddings:
 print(embedding)

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 async function main() {

 const ai = new GoogleGenAI({});

 const response = await ai.models.embedContent({
 model: 'gemini-embedding-001',
 contents: [
 'What is the meaning of life?',
 'What is the purpose of existence?',
 'How do I bake a cake?'
 ],
 });

 console.log(response.embeddings);
 }

 main();

### Go

 package main

 import (
 "context"
 "encoding/json"
 "fmt"
 "log"

 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 contents := []*genai.Content{
 genai.NewContentFromText("What is the meaning of life?"),
 genai.NewContentFromText("How does photosynthesis work?"),
 genai.NewContentFromText("Tell me about the history of the internet."),
 }
 result, err := client.Models.EmbedContent(ctx,
 "gemini-embedding-001",
 contents,
 nil,
 )
 if err != nil {
 log.Fatal(err)
 }

 embeddings, err := json.MarshalIndent(result.Embeddings, "", " ")
 if err != nil {
 log.Fatal(err)
 }
 fmt.Println(string(embeddings))
 }

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:batchEmbedContents" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d '{"requests": [{
 "model": "models/gemini-embedding-001",
 "content": {
 "parts":[{
 "text": "What is the meaning of life?"}]}, },
 {
 "model": "models/gemini-embedding-001",
 "content": {
 "parts":[{
 "text": "How much wood would a woodchuck chuck?"}]}, },
 {
 "model": "models/gemini-embedding-001",
 "content": {
 "parts":[{
 "text": "How does the brain work?"}]}, }, ]}' 2> /dev/null | grep -C 5 values
 ```

## Specify task type to improve performance

You can use embeddings for a wide range of tasks from classification to document
search. Specifying the right task type helps optimize the embeddings for the
intended relationships, maximizing accuracy and efficiency. For a complete list
of supported task types, see the [Supported task types](https://ai.google.dev/gemini-api/docs/embeddings#supported-task-types)
table.

The following example shows how you can use
`SEMANTIC_SIMILARITY` to check how similar in meaning strings of texts are.
**Note:** Cosine similarity is a good distance metric because it focuses on direction rather than magnitude, which more accurately reflects conceptual closeness. Values range from -1 (opposite) to 1 (greatest similarity).

### Python

 from google import genai
 from google.genai import types
 import numpy as np
 from sklearn.metrics.pairwise import cosine_similarity

 client = genai.Client()

 texts = [
 "What is the meaning of life?",
 "What is the purpose of existence?",
 "How do I bake a cake?"]

 result = [
 np.array(e.values) for e in client.models.embed_content(
 model="gemini-embedding-001",
 contents=texts,
 config=types.EmbedContentConfig(task_type="SEMANTIC_SIMILARITY")).embeddings
 ]

 # Calculate cosine similarity. Higher scores = greater semantic similarity.

 embeddings_matrix = np.array(result)
 similarity_matrix = cosine_similarity(embeddings_matrix)

 for i, text1 in enumerate(texts):
 for j in range(i + 1, len(texts)):
 text2 = texts[j]
 similarity = similarity_matrix[i, j]
 print(f"Similarity between '{text1}' and '{text2}': {similarity:.4f}")

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as cosineSimilarity from "compute-cosine-similarity";

 async function main() {
 const ai = new GoogleGenAI({});

 const texts = [
 "What is the meaning of life?",
 "What is the purpose of existence?",
 "How do I bake a cake?",
 ];

 const response = await ai.models.embedContent({
 model: 'gemini-embedding-001',
 contents: texts,
 taskType: 'SEMANTIC_SIMILARITY'
 });

 const embeddings = response.embeddings.map(e => e.values);

 for (let i = 0; i < texts.length; i++) {
 for (let j = i + 1; j < texts.length; j++) {
 const text1 = texts[i];
 const text2 = texts[j];
 const similarity = cosineSimilarity(embeddings[i], embeddings[j]);
 console.log(`Similarity between '${text1}' and '${text2}': ${similarity.toFixed(4)}`);
 }
 }
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "log"
 "math"

 "google.golang.org/genai"
 )

 // cosineSimilarity calculates the similarity between two vectors.
 func cosineSimilarity(a, b []float32) (float64, error) {
 if len(a) != len(b) {
 return 0, fmt.Errorf("vectors must have the same length")
 }

 var dotProduct, aMagnitude, bMagnitude float64
 for i := 0; i < len(a); i++ {
 dotProduct += float64(a[i] * b[i])
 aMagnitude += float64(a[i] * a[i])
 bMagnitude += float64(b[i] * b[i])
 }

 if aMagnitude == 0 || bMagnitude == 0 {
 return 0, nil
 }

 return dotProduct / (math.Sqrt(aMagnitude) * math.Sqrt(bMagnitude)), nil
 }

 func main() {
 ctx := context.Background()
 client, _ := genai.NewClient(ctx, nil)
 defer client.Close()

 texts := []string{
 "What is the meaning of life?",
 "What is the purpose of existence?",
 "How do I bake a cake?",
 }

 var contents []*genai.Content
 for _, text := range texts {
 contents = append(contents, genai.NewContentFromText(text, genai.RoleUser))
 }

 result, _ := client.Models.EmbedContent(ctx,
 "gemini-embedding-001",
 contents,
 &genai.EmbedContentRequest{TaskType: genai.TaskTypeSemanticSimilarity},
 )

 embeddings := result.Embeddings

 for i := 0; i < len(texts); i++ {
 for j := i + 1; j < len(texts); j++ {
 similarity, _ := cosineSimilarity(embeddings[i].Values, embeddings[j].Values)
 fmt.Printf("Similarity between '%s' and '%s': %.4f\n", texts[i], texts[j], similarity)
 }
 }
 }

### REST

 curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d '{
 "contents": [
 {"parts": [{"text": "What is the meaning of life?"}]},
 {"parts": [{"text": "What is the purpose of existence?"}]},
 {"parts": [{"text": "How do I bake a cake?"}]}
 ],
 "embedding_config": {
 "task_type": "SEMANTIC_SIMILARITY"
 }
 }'

The following shows an example output from this code snippet:

 Similarity between 'What is the meaning of life?' and 'What is the purpose of existence?': 0.9481

 Similarity between 'What is the meaning of life?' and 'How do I bake a cake?': 0.7471

 Similarity between 'What is the purpose of existence?' and 'How do I bake a cake?': 0.7371

### Supported task types

| Task type | Description | Examples |
|--------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------|
| **SEMANTIC_SIMILARITY** | Embeddings optimized to assess text similarity. | Recommendation systems, duplicate detection |
| **CLASSIFICATION** | Embeddings optimized to classify texts according to preset labels. | Sentiment analysis, spam detection |
| **CLUSTERING** | Embeddings optimized to cluster texts based on their similarities. | Document organization, market research, anomaly detection |
| **RETRIEVAL_DOCUMENT** | Embeddings optimized for document search. | Indexing articles, books, or web pages for search. |
| **RETRIEVAL_QUERY** | Embeddings optimized for general search queries. Use `RETRIEVAL_QUERY` for queries; `RETRIEVAL_DOCUMENT` for documents to be retrieved. | Custom search |
| **CODE_RETRIEVAL_QUERY** | Embeddings optimized for retrieval of code blocks based on natural language queries. Use `CODE_RETRIEVAL_QUERY` for queries; `RETRIEVAL_DOCUMENT` for code blocks to be retrieved. | Code suggestions and search |
| **QUESTION_ANSWERING** | Embeddings for questions in a question-answering system, optimized for finding documents that answer the question. Use `QUESTION_ANSWERING` for questions; `RETRIEVAL_DOCUMENT` for documents to be retrieved. | Chatbox |
| **FACT_VERIFICATION** | Embeddings for statements that need to be verified, optimized for retrieving documents that contain evidence supporting or refuting the statement. Use `FACT_VERIFICATION` for the target text; `RETRIEVAL_DOCUMENT` for documents to be retrieved | Automated fact-checking systems |

## Controlling embedding size

The Gemini embedding model, `gemini-embedding-001`, is trained using the
Matryoshka Representation Learning (MRL) technique which teaches a model to
learn high-dimensional embeddings that have initial segments (or prefixes) which
are also useful, simpler versions of the same data.

Use the `output_dimensionality` parameter to control the size of
the output embedding vector. Selecting a smaller output dimensionality can save
storage space and increase computational efficiency for downstream applications,
while sacrificing little in terms of quality. By default, it outputs a
3072-dimensional embedding, but you can truncate it to a smaller size without
losing quality to save storage space. We recommend using 768, 1536, or 3072
output dimensions.

### Python

 from google import genai
 from google.genai import types

 client = genai.Client()

 result = client.models.embed_content(
 model="gemini-embedding-001",
 contents="What is the meaning of life?",
 config=types.EmbedContentConfig(output_dimensionality=768)
 )

 [embedding_obj] = result.embeddings
 embedding_length = len(embedding_obj.values)

 print(f"Length of embedding: {embedding_length}")

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 async function main() {
 const ai = new GoogleGenAI({});

 const response = await ai.models.embedContent({
 model: 'gemini-embedding-001',
 content: 'What is the meaning of life?',
 outputDimensionality: 768,
 });

 const embeddingLength = response.embedding.values.length;
 console.log(`Length of embedding: ${embeddingLength}`);
 }

 main();

### Go

 package main

 import (
 "context"
 "fmt"
 "log"

 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 // The client uses Application Default Credentials.
 // Authenticate with 'gcloud auth application-default login'.
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }
 defer client.Close()

 contents := []*genai.Content{
 genai.NewContentFromText("What is the meaning of life?", genai.RoleUser),
 }

 result, err := client.Models.EmbedContent(ctx,
 "gemini-embedding-001",
 contents,
 &genai.EmbedContentRequest{OutputDimensionality: 768},
 )
 if err != nil {
 log.Fatal(err)
 }

 embedding := result.Embeddings[0]
 embeddingLength := len(embedding.Values)
 fmt.Printf("Length of embedding: %d\n", embeddingLength)
 }

### REST

 curl -X POST "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent" \
 -H "x-goog-api-key: YOUR_GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -d '{
 "contents": [
 {"parts": [{"text": "What is the meaning of life?"}]}
 ],
 "embedding_config": {
 "output_dimensionality": 768
 }
 }'

Example output from the code snippet:

 Length of embedding: 768

## Ensuring quality for smaller dimensions

The 3072 dimension embedding is normalized. Normalized embeddings produce more
accurate semantic similarity by comparing vector direction, not magnitude. For
other dimensions, including 768 and 1536, you need to normalize the embeddings
as follows:

### Python

 import numpy as np
 from numpy.linalg import norm

 embedding_values_np = np.array(embedding_obj.values)
 normed_embedding = embedding_values_np / np.linalg.norm(embedding_values_np)

 print(f"Normed embedding length: {len(normed_embedding)}")
 print(f"Norm of normed embedding: {np.linalg.norm(normed_embedding):.6f}") # Should be very close to 1

Example output from this code snippet:

 Normed embedding length: 768
 Norm of normed embedding: 1.000000

The following table shows the MTEB scores, a commonly used benchmark for
embeddings, for different dimensions. Notably, the result shows that performance
is not strictly tied to the size of the embedding dimension, with lower
dimensions achieving scores comparable to their higher dimension counterparts.

| MRL Dimension | MTEB Score |
|---------------|------------|
| 2048 | 68.16 |
| 1536 | 68.17 |
| 768 | 67.99 |
| 512 | 67.55 |
| 256 | 66.19 |
| 128 | 63.31 |

## Use cases

Text embeddings are crucial for a variety of common AI use cases, such as:

- **Retrieval-Augmented Generation (RAG):** Embeddings enhance the quality of generated text by retrieving and incorporating relevant information into the context of a model.
- **Information retrieval:** Search for the most semantically similar text or
 documents given a piece of input text.

 [Document search tutorialtask](https://github.com/google-gemini/cookbook/blob/main/examples/Talk_to_documents_with_embeddings.ipynb)
- **Search reranking**: Prioritize the most relevant items by semantically
 scoring initial results against the query.

 [Search reranking tutorialtask](https://github.com/google-gemini/cookbook/blob/main/examples/Search_reranking_using_embeddings.ipynb)
- **Anomaly detection:** Comparing groups of embeddings can help identify
 hidden trends or outliers.

 [Anomaly detection tutorialbubble_chart](https://github.com/google-gemini/cookbook/blob/main/examples/Anomaly_detection_with_embeddings.ipynb)
- **Classification:** Automatically categorize text based on its content, such
 as sentiment analysis or spam detection

 [Classification tutorialtoken](https://github.com/google-gemini/cookbook/blob/main/examples/Classify_text_with_embeddings.ipynb)
- **Clustering:** Effectively grasp complex relationships by creating clusters
 and visualizations of your embeddings.

 [Clustering visualization tutorialbubble_chart](https://github.com/google-gemini/cookbook/blob/main/examples/clustering_with_embeddings.ipynb)

## Storing embeddings

As you take embeddings to production, it is common to
use **vector databases** to efficiently store, index, and retrieve
high-dimensional embeddings. Google Cloud offers managed data services that
can be used for this purpose including
[BigQuery](https://cloud.google.com/bigquery/docs/introduction),
[AlloyDB](https://cloud.google.com/alloydb/docs/overview), and
[Cloud SQL](https://cloud.google.com/sql/docs/postgres/introduction).

The following tutorials show how to use other third party vector databases
with Gemini Embedding.

- [ChromaDB tutorialsbolt](https://github.com/google-gemini/cookbook/tree/main/examples/chromadb)
- [QDrant tutorialsbolt](https://github.com/google-gemini/cookbook/tree/main/examples/qdrant)
- [Weaviate tutorialsbolt](https://github.com/google-gemini/cookbook/tree/main/examples/weaviate)
- [Pinecone tutorialsbolt](https://github.com/google-gemini/cookbook/blob/main/examples/langchain/Gemini_LangChain_QA_Pinecone_WebLoad.ipynb)

## Model versions

| Property | Description |
|-----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| id_cardModel code | **Gemini API** `gemini-embedding-001` |
| saveSupported data types | **Input** Text **Output** Text embeddings |
| token_autoToken limits^[\[\*\]](https://ai.google.dev/gemini-api/docs/embeddings#token-size)^ | **Input token limit** 2,048 **Output dimension size** Flexible, supports: 128 - 3072, Recommended: 768, 1536, 3072 |
| 123Versions | Read the [model version patterns](https://ai.google.dev/gemini-api/docs/models/gemini#model-versions) for more details. - Stable: `gemini-embedding-001` - Experimental: `gemini-embedding-exp-03-07` (deprecating in Oct of 2025) |
| calendar_monthLatest update | June 2025 |

## Batch embeddings

If latency is not a concern, try using the Gemini Embeddings model with
[Batch API](https://ai.google.dev/gemini-api/docs/batch-api#batch-embedding). This
allows for much higher throughput at 50% of interactive Embedding pricing.
Find examples on how to get started in the [Batch API cookbook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Batch_mode.ipynb).

## Responsible use notice

Unlike generative AI models that create new content, the Gemini Embedding model
is only intended to transform the format of your input data into a numerical
representation. While Google is responsible for providing an embedding model
that transforms the format of your input data to the numerical-format requested,
users retain full responsibility for the data they input and the resulting
embeddings. By using the Gemini Embedding model you confirm that you have the
necessary rights to any content that you upload. Do not generate content that
infringes on others' intellectual property or privacy rights. Your use of this
service is subject to our [Prohibited Use
Policy](https://policies.google.com/terms/generative-ai/use-policy) and
[Google's Terms of Service](https://ai.google.dev/gemini-api/terms).

## Start building with embeddings

Check out the [embeddings quickstart
notebook](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Embeddings.ipynb)
to explore the model capabilities and learn how to customize and visualize your
embeddings.

## Deprecation notice for legacy models

The following models will be deprecated in October, 2025:
- `embedding-001`
- `embedding-gecko-001`
- `gemini-embedding-exp-03-07` (`gemini-embedding-exp`)

[END OF DOCUMENT: GEMINI5UPTD6OMJ]
---

[START OF DOCUMENT: GEMINI15TSE07OK0 | Title: Ephemeral-Tokens.Md]

Ephemeral tokens are short-lived authentication tokens for accessing the Gemini
API through [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API). They are designed to enhance security when
you are connecting directly from a user's device to the API (a
[client-to-server](https://ai.google.dev/gemini-api/docs/live#implementation-approach)
implementation). Like standard API keys, ephemeral tokens can be extracted from
client-side applications such as web browsers or mobile apps. But because
ephemeral tokens expire quickly and can be restricted, they significantly reduce
the security risks in a production environment.
| **Note:** Ephemeral tokens are only compatible with [Live API](https://ai.google.dev/gemini-api/docs/live) at this time. You should use them when accessing the Live API directly from client-side applications to enhance API key security.

## How ephemeral tokens work

Here's how ephemeral tokens work at a high level:

1. Your client (e.g. web app) authenticates with your backend.
2. Your backend requests an ephemeral token from Gemini API's provisioning service.
3. Gemini API issues a short-lived token.
4. Your backend sends the token to the client for WebSocket connections to Live API. You can do this by swapping your API key with an ephemeral token.
5. The client then uses the token as if it were an API key.

![Ephemeral tokens overview](https://ai.google.dev/static/gemini-api/docs/images/Live_API_01.png)

This enhances security because even if extracted, the token is short-lived,
unlike a long-lived API key deployed client-side. Since the client sends data
directly to Gemini, this also improves latency and avoids your backends needing
to proxy the real time data.

## Create an ephemeral token

Here is a simplified example of how to get an ephemeral token from Gemini.
By default, you'll have 1 minute to start new Live API sessions using the token
from this request (`newSessionExpireTime`), and 30 minutes to send messages over
that connection (`expireTime`).

### Python

 import datetime

 now = datetime.datetime.now(tz=datetime.timezone.utc)

 client = genai.Client(
 http_options={'api_version': 'v1alpha',}
 )

 token = client.auth_tokens.create(
 config = {
 'uses': 1, # The ephemeral token can only be used to start a single session
 'expire_time': now + datetime.timedelta(minutes=30), # Default is 30 minutes in the future
 # 'expire_time': '2025-05-17T00:00:00Z', # Accepts isoformat.
 'new_session_expire_time': now + datetime.timedelta(minutes=1), # Default 1 minute in the future
 'http_options': {'api_version': 'v1alpha'},
 }
 )

 # You'll need to pass the value under token.name back to your client to use it

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const client = new GoogleGenAI({});
 const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

 const token: AuthToken = await client.authTokens.create({
 config: {
 uses: 1, // The default
 expireTime: expireTime // Default is 30 mins
 newSessionExpireTime: new Date(Date.now() + (1 * 60 * 1000)), // Default 1 minute in the future
 httpOptions: {apiVersion: 'v1alpha'},
 },
 });

For `expireTime` value constraints, defaults, and other field specs, see the
[API reference](https://ai.google.dev/api/live#ephemeral-auth-tokens).
Within the `expireTime` timeframe, you'll need
[`sessionResumption`](https://ai.google.dev/gemini-api/docs/live-session#session-resumption) to
reconnect the call every 10 minutes (this can be done with the same token even
if `uses: 1`).

It's also possible to lock an ephemeral token to a set of configurations. This
might be useful to further improve security of your application and keep your
system instructions on the server side.

### Python

 client = genai.Client(
 http_options={'api_version': 'v1alpha',}
 )

 token = client.auth_tokens.create(
 config = {
 'uses': 1,
 'live_connect_constraints': {
 'model': 'gemini-2.0-flash-live-001',
 'config': {
 'session_resumption':{},
 'temperature':0.7,
 'response_modalities':['TEXT']
 }
 },
 'http_options': {'api_version': 'v1alpha'},
 }
 )

 # You'll need to pass the value under token.name back to your client to use it

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const client = new GoogleGenAI({});
 const expireTime = new Date(Date.now() + 30 * 60 * 1000).toISOString();

 const token = await client.authTokens.create({
 config: {
 uses: 1, // The default
 expireTime: expireTime,
 liveConnectConstraints: {
 model: 'gemini-2.0-flash-live-001',
 config: {
 sessionResumption: {},
 temperature: 0.7,
 responseModalities: ['TEXT']
 }
 },
 httpOptions: {
 apiVersion: 'v1alpha'
 }
 }
 });

 // You'll need to pass the value under token.name back to your client to use it

You can also lock a subset of fields, see the [SDK documentation](https://googleapis.github.io/python-genai/genai.html#genai.types.CreateAuthTokenConfig.lock_additional_fields)
for more info.

## Connect to Live API with an ephemeral token

Once you have an ephemeral token, you use it as if it were an API key (but
remember, it only works for the live API, and only with the `v1alpha` version of
the API).

Note that use of ephemeral tokens only adds value when deploying applications
that follow [client-to-server implementation](https://ai.google.dev/gemini-api/docs/live#implementation-approach) approach.

### JavaScript

 import { GoogleGenAI, Modality } from '@google/genai';

 // Use the token generated in the "Create an ephemeral token" section here
 const ai = new GoogleGenAI({
 apiKey: token.name
 });
 const model = 'gemini-2.0-flash-live-001';
 const config = { responseModalities: [Modality.TEXT] };

 async function main() {

 const session = await ai.live.connect({
 model: model,
 config: config,
 callbacks: { ... },
 });

 // Send content...

 session.close();
 }

 main();

| **Note:** If not using the SDK, note that ephemeral tokens must either be passed in an `access_token` query parameter, or in an HTTP `Authorization` prefixed by the [auth-scheme](https://datatracker.ietf.org/doc/html/rfc7235#section-2.1) `Token`.

See [Get started with Live API](https://ai.google.dev/gemini-api/docs/live) for more examples.

## Best practices

- Set a short expiration duration using the `expire_time` parameter.
- Tokens expire, requiring re-initiation of the provisioning process.
- Verify secure authentication for your own backend. Ephemeral tokens will only be as secure as your backend authentication method.
- Generally, avoid using ephemeral tokens for backend-to-Gemini connections, as this path is typically considered secure.

## Limitations

Ephemeral tokens are only compatible with [Live API](https://ai.google.dev/gemini-api/docs/live) at this time.

## What's next

- Read the Live API [reference](https://ai.google.dev/api/live#ephemeral-auth-tokens) on ephemeral tokens for more information.

[END OF DOCUMENT: GEMINI15TSE07OK0]
---

[START OF DOCUMENT: GEMINI1BNX91CDID | Title: File-Search.Md]

<br />

The Gemini API enables Retrieval Augmented Generation ("RAG") through the File Search tool. File Search imports, chunks, and indexes your data to enable fast retrieval of relevant information based on a user's prompt. This information is then provided as context to the model, allowing the model to provide more accurate and relevant answers.

You can use the[`uploadToFileSearchStore`](https://ai.google.dev/api/file-search/file-search-stores#method:-media.uploadtofilesearchstore)API to directly upload an existing file to your file search store, or separately upload and then[`importFile`](https://ai.google.dev/api/file-search/file-search-stores#method:-filesearchstores.importfile)if you want to create the file at the same time.

## Directly upload to file search store

This examples shows how to directly upload a file to a file store:

### Python

 from google import genai
 from google.genai import types
 import time

 client = genai.Client()

 # Create the file search store with an optional display name
 file_search_store = client.file_search_stores.create(config={'display_name': 'your-fileSearchStore-name'})

 # Upload and import a file into the file search store, supply a file name which will be visible in citations
 operation = client.file_search_stores.upload_to_file_search_store(
 file='sample.txt',
 file_search_store_name=file_search_store.name,
 config={
 'display_name' : 'display-file-name',
 }
 )

 # Wait until import is complete
 while not operation.done:
 time.sleep(5)
 operation = client.operations.get(operation)

 # Ask a question about the file
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="""Can you tell me about Robert Graves""",
 config=types.GenerateContentConfig(
 tools=[
 file_search=(
 file_search_store_names=[file_search_store.name]
 )
 ]
 )
 )

 print(response.text)

### JavaScript

 const { GoogleGenAI } = require('@google/genai');

 const ai = new GoogleGenAI({});

 async function run() {
 // Create the file search store with an optional display name
 const fileSearchStore = await ai.fileSearchStores.create({
 config: { displayName: 'your-fileSearchStore-name' }
 });

 // Upload and import a file into the file search store, supply a file name which will be visible in citations
 let operation = await ai.fileSearchStores.uploadToFileSearchStore({
 file: 'file.txt',
 fileSearchStoreName: fileSearchStore.name,
 config: {
 displayName: 'file-name',
 }
 });

 // Wait until import is complete
 while (!operation.done) {
 await new Promise(resolve => setTimeout(resolve, 5000));
 operation = await ai.operations.get({ operation });
 }

 // Ask a question about the file
 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: "Can you tell me about Robert Graves",
 config: {
 tools: [
 {
 fileSearch: {
 fileSearchStoreNames: [fileSearchStore.name]
 }
 }
 ]
 }
 });

 console.log(response.text);
 }

 run();

Check the API reference for[`uploadToFileSearchStore`](https://ai.google.dev/api/file-search/file-search-stores#method:-media.uploadtofilesearchstore)for more information.

## Importing files

Alternatively, you can upload an existing file and import it to your file store:

### Python

 from google import genai
 from google.genai import types
 import time

 client = genai.Client()

 # Upload the file using the Files API, supply a file name which will be visible in citations
 sample_file = client.files.upload(file='sample.txt', config={'name': 'display_file_name'})

 # Create the file search store with an optional display name
 file_search_store = client.file_search_stores.create(config={'display_name': 'your-fileSearchStore-name'})

 # Import the file into the file search store
 operation = client.file_search_stores.import_file(
 file_search_store_name=file_search_store.name,
 file_name=sample_file.name
 )

 # Wait until import is complete
 while not operation.done:
 time.sleep(5)
 operation = client.operations.get(operation)

 # Ask a question about the file
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="""Can you tell me about Robert Graves""",
 config=types.GenerateContentConfig(
 tools=[
 file_search=(
 file_search_store_names=[file_search_store.name]
 )
 ]
 )
 )

 print(response.text)

### JavaScript

 const { GoogleGenAI } = require('@google/genai');

 const ai = new GoogleGenAI({});

 async function run() {
 // Upload the file using the Files API, supply a file name which will be visible in citations
 const sampleFile = await ai.files.upload({
 file: 'sample.txt',
 config: { name: 'file-name' }
 });

 // Create the file search store with an optional display name
 const fileSearchStore = await ai.fileSearchStores.create({
 config: { displayName: 'your-fileSearchStore-name' }
 });

 // Import the file into the file search store
 let operation = await ai.fileSearchStores.importFile({
 fileSearchStoreName: fileSearchStore.name,
 fileName: sampleFile.name
 });

 // Wait until import is complete
 while (!operation.done) {
 await new Promise(resolve => setTimeout(resolve, 5000));
 operation = await ai.operations.get({ operation: operation });
 }

 // Ask a question about the file
 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: "Can you tell me about Robert Graves",
 config: {
 tools: [
 {
 fileSearch: {
 fileSearchStoreNames: [fileSearchStore.name]
 }
 }
 ]
 }
 });

 console.log(response.text);
 }

 run();

Check the API reference for[`importFile`](https://ai.google.dev/api/file-search/file-search-stores#method:-filesearchstores.importfile)for more information.

## Chunking configuration

When you import a file into a file search store, it's automatically broken down into chunks, embedded, indexed, and uploaded to your file search store. If you need more control over the chunking strategy, you can specify a[`chunking_config`](https://ai.google.dev/api/file-search/file-search-stores#request-body_5)setting to set a maximum number of tokens per chunk and maximum number of overlapping tokens.

### Python

 # Upload and import and upload the file into the file search store with a custom chunking configuration
 operation = client.file_search_stores.upload_to_file_search_store(
 file_search_store_name=file_search_store.name,
 file_name=sample_file.name,
 config={
 'chunking_config': {
 'white_space_config': {
 'max_tokens_per_chunk': 200,
 'max_overlap_tokens': 20
 }
 }
 }
 )

### JavaScript

 // Upload and import and upload the file into the file search store with a custom chunking configuration
 let operation = await ai.fileSearchStores.uploadToFileSearchStore({
 file: 'file.txt',
 fileSearchStoreName: fileSearchStore.name,
 config: {
 displayName: 'file-name',
 chunkingConfig: {
 whiteSpaceConfig: {
 maxTokensPerChunk: 200,
 maxOverlapTokens: 20
 }
 }
 }
 });

To use your file search store, pass it as a tool to the`generateContent`method, as shown in the[Upload](https://ai.google.dev/gemini-api/docs/file-search#upload)and[Import](https://ai.google.dev/gemini-api/docs/file-search#importing-files)examples.

## How it works

File Search uses a technique called semantic search to find information relevant to the user prompt. Unlike traditional keyword-based search, semantic search understands the meaning and context of your query.

When you import a file, it's converted into numerical representations called[embeddings](https://ai.google.dev/gemini-api/docs/embeddings), which capture the semantic meaning of the text. These embeddings are stored in a specialized file search database. When you make a query, it's also converted into an embedding. Then the system performs a file search to find the most similar and relevant document chunks from the file search store.

Here's a breakdown of the process for using the File Search`uploadToFileSearchStore`API:

1. **Create a file search store**: A file search store contains the processed data from your files. It's the persistent container for the embeddings that the semantic search will operate on.

2. **Upload a file and import into a file search store** : Simultaneously upload a file and import the results into your file search store. This creates a temporary`File`object, which is a reference to your raw document. That data is then chunked, converted into file search embeddings, and indexed. The`File`object gets deleted after 48 hours, while the data imported into the file search store will be stored indefinitely until you choose to delete it.

3. **Query with File Search** : Finally, you use the`FileSearch`tool in a`generateContent`call. In the tool configuration, you specify a`FileSearchRetrievalResource`, which points to the`FileSearchStore`you want to search. This tells the model to perform a semantic search on that specific file search store to find relevant information to ground its response.

![The indexing and querying process of File Search](https://ai.google.dev/static/gemini-api/docs/images/File-search.png)The indexing and querying process of File Search

In this diagram, the dotted line from from*Documents* to*Embedding model* (using[`gemini-embedding-001`](https://ai.google.dev/gemini-api/docs/embeddings)) represents the`uploadToFileSearchStore`API (bypassing*File storage* ). Otherwise, using the[Files API](https://ai.google.dev/gemini-api/docs/files)to separately create and then import files moves the indexing process from*Documents* to*File storage* and then to*Embedding model*.

## File search stores

A file search store is a container for your document embeddings. While raw files uploaded through the File API are deleted after 48 hours, the data imported into a file search store is stored indefinitely until you manually delete it. You can create multiple file search stores to organize your documents. The`FileSearchStore`API lets you create, list, get, and delete to manage your file search stores. File search store names are globally scoped.

Here are some examples of how to manage your file search stores:

### Python

 # Create a file search store (including optional display_name for easier reference)
 file_search_store = client.file_search_stores.create(config={'display_name': 'my-file_search-store-123'})

 # List all your file search stores
 for file_search_store in client.file_search_stores.list():
 print(file_search_store)

 # Get a specific file search store by name
 my_file_search_store = client.file_search_stores.get(name='fileSearchStores/my-file_search-store-123')

 # Delete a file search store
 client.file_search_stores.delete(name='fileSearchStores/my-file_search-store-123', config={'force': True})

### JavaScript

 // Create a file search store (including optional display_name for easier reference)
 const fileSearchStore = await ai.fileSearchStores.create({
 config: { displayName: 'my-file_search-store-123' }
 });

 // List all your file search stores
 const fileSearchStores = await ai.fileSearchStores.list();
 for await (const store of fileSearchStores) {
 console.log(store);
 }

 // Get a specific file search store by name
 const myFileSearchStore = await ai.fileSearchStores.get({
 name: 'fileSearchStores/my-file_search-store-123'
 });

 // Delete a file search store
 await ai.fileSearchStores.delete({
 name: 'fileSearchStores/my-file_search-store-123',
 config: { force: true }
 });

The[File Search Documents](https://ai.google.dev/api/file-search/documents)API reference for methods and fields related to managing documents in your file stores.

## File metadata

You can add custom metadata to your files to help filter them or provide additional context. Metadata is a set of key-value pairs.

### Python

 # Import the file into the file search store with custom metadata
 op = client.file_search_stores.import_file(
 file_search_store_name=file_search_store.name,
 file_name=sample_file.name,
 custom_metadata=[
 {"key": "author", "string_value": "Robert Graves"},
 {"key": "year", "numeric_value": 1934}
 ]
 )

### JavaScript

 // Import the file into the file search store with custom metadata
 let operation = await ai.fileSearchStores.importFile({
 fileSearchStoreName: fileSearchStore.name,
 fileName: sampleFile.name,
 config: {
 customMetadata: [
 { key: "author", stringValue: "Robert Graves" },
 { key: "year", numericValue: 1934 }
 ]
 }
 });

This is useful when you have multiple documents in a file search store and want to search only a subset of them.

### Python

 # Use the metadata filter to search within a subset of documents
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="""Tell me about the book 'I, Claudius'""",
 config=types.GenerateContentConfig(
 tools=[
 types.Tool(
 file_search=types.FileSearch(
 file_search_store_names=[file_search_store.name],
 metadata_filter = 'author=Robet Graves',
 )
 )
 ]
 )
 )

 print(response.text)

### JavaScript

 // Use the metadata filter to search within a subset of documents
 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: "Tell me about the book 'I, Claudius'",
 config: {
 tools: [
 {
 fileSearch: {
 fileSearchStoreNames: [fileSearchStore.name],
 metadataFilter: 'author="Robert Graves"',
 }
 }
 ]
 }
 });

 console.log(response.text);

Guidance on implementing list filter syntax for`metadata_filter`can be found at[google.aip.dev/160](https://google.aip.dev/160)

## Citations

When you use File Search, the model's response may include citations that specify which parts of your uploaded documents were used to generate the answer. This helps with fact-checking and verification.

You can access citation information through the`grounding_metadata`attribute of the response.

### Python

 print(response.candidates[0].grounding_metadata)

### JavaScript

 console.log(JSON.stringify(response.candidates?.[0]?.groundingMetadata, null, 2));

## Supported models

The following models support File Search:

- [`gemini-2.5-pro`](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro)
- [`gemini-2.5-flash`](https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash)

## Supported file types

File Search supports a wide range of file formats, listed in the following sections.

### Application file types

- `application/dart`
- `application/ecmascript`
- `application/json`
- `application/ms-java`
- `application/msword`
- `application/pdf`
- `application/sql`
- `application/typescript`
- `application/vnd.curl`
- `application/vnd.dart`
- `application/vnd.ibm.secure-container`
- `application/vnd.jupyter`
- `application/vnd.ms-excel`
- `application/vnd.oasis.opendocument.text`
- `application/vnd.openxmlformats-officedocument.presentationml.presentation`
- `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`
- `application/vnd.openxmlformats-officedocument.wordprocessingml.document`
- `application/vnd.openxmlformats-officedocument.wordprocessingml.template`
- `application/x-csh`
- `application/x-hwp`
- `application/x-hwp-v5`
- `application/x-latex`
- `application/x-php`
- `application/x-powershell`
- `application/x-sh`
- `application/x-shellscript`
- `application/x-tex`
- `application/x-zsh`
- `application/xml`
- `application/zip`

### Text file types

- `text/1d-interleaved-parityfec`
- `text/RED`
- `text/SGML`
- `text/cache-manifest`
- `text/calendar`
- `text/cql`
- `text/cql-extension`
- `text/cql-identifier`
- `text/css`
- `text/csv`
- `text/csv-schema`
- `text/dns`
- `text/encaprtp`
- `text/enriched`
- `text/example`
- `text/fhirpath`
- `text/flexfec`
- `text/fwdred`
- `text/gff3`
- `text/grammar-ref-list`
- `text/hl7v2`
- `text/html`
- `text/javascript`
- `text/jcr-cnd`
- `text/jsx`
- `text/markdown`
- `text/mizar`
- `text/n3`
- `text/parameters`
- `text/parityfec`
- `text/php`
- `text/plain`
- `text/provenance-notation`
- `text/prs.fallenstein.rst`
- `text/prs.lines.tag`
- `text/prs.prop.logic`
- `text/raptorfec`
- `text/rfc822-headers`
- `text/rtf`
- `text/rtp-enc-aescm128`
- `text/rtploopback`
- `text/rtx`
- `text/sgml`
- `text/shaclc`
- `text/shex`
- `text/spdx`
- `text/strings`
- `text/t140`
- `text/tab-separated-values`
- `text/texmacs`
- `text/troff`
- `text/tsv`
- `text/tsx`
- `text/turtle`
- `text/ulpfec`
- `text/uri-list`
- `text/vcard`
- `text/vnd.DMClientScript`
- `text/vnd.IPTC.NITF`
- `text/vnd.IPTC.NewsML`
- `text/vnd.a`
- `text/vnd.abc`
- `text/vnd.ascii-art`
- `text/vnd.curl`
- `text/vnd.debian.copyright`
- `text/vnd.dvb.subtitle`
- `text/vnd.esmertec.theme-descriptor`
- `text/vnd.exchangeable`
- `text/vnd.familysearch.gedcom`
- `text/vnd.ficlab.flt`
- `text/vnd.fly`
- `text/vnd.fmi.flexstor`
- `text/vnd.gml`
- `text/vnd.graphviz`
- `text/vnd.hans`
- `text/vnd.hgl`
- `text/vnd.in3d.3dml`
- `text/vnd.in3d.spot`
- `text/vnd.latex-z`
- `text/vnd.motorola.reflex`
- `text/vnd.ms-mediapackage`
- `text/vnd.net2phone.commcenter.command`
- `text/vnd.radisys.msml-basic-layout`
- `text/vnd.senx.warpscript`
- `text/vnd.sosi`
- `text/vnd.sun.j2me.app-descriptor`
- `text/vnd.trolltech.linguist`
- `text/vnd.wap.si`
- `text/vnd.wap.sl`
- `text/vnd.wap.wml`
- `text/vnd.wap.wmlscript`
- `text/vtt`
- `text/wgsl`
- `text/x-asm`
- `text/x-bibtex`
- `text/x-boo`
- `text/x-c`
- `text/x-c++hdr`
- `text/x-c++src`
- `text/x-cassandra`
- `text/x-chdr`
- `text/x-coffeescript`
- `text/x-component`
- `text/x-csh`
- `text/x-csharp`
- `text/x-csrc`
- `text/x-cuda`
- `text/x-d`
- `text/x-diff`
- `text/x-dsrc`
- `text/x-emacs-lisp`
- `text/x-erlang`
- `text/x-gff3`
- `text/x-go`
- `text/x-haskell`
- `text/x-java`
- `text/x-java-properties`
- `text/x-java-source`
- `text/x-kotlin`
- `text/x-lilypond`
- `text/x-lisp`
- `text/x-literate-haskell`
- `text/x-lua`
- `text/x-moc`
- `text/x-objcsrc`
- `text/x-pascal`
- `text/x-pcs-gcd`
- `text/x-perl`
- `text/x-perl-script`
- `text/x-python`
- `text/x-python-script`
- `text/x-r-markdown`
- `text/x-rsrc`
- `text/x-rst`
- `text/x-ruby-script`
- `text/x-rust`
- `text/x-sass`
- `text/x-scala`
- `text/x-scheme`
- `text/x-script.python`
- `text/x-scss`
- `text/x-setext`
- `text/x-sfv`
- `text/x-sh`
- `text/x-siesta`
- `text/x-sos`
- `text/x-sql`
- `text/x-swift`
- `text/x-tcl`
- `text/x-tex`
- `text/x-vbasic`
- `text/x-vcalendar`
- `text/xml`
- `text/xml-dtd`
- `text/xml-external-parsed-entity`
- `text/yaml`

## Rate limits

The File Search API has the following limits to enforce service stability:

- **Maximum file size / per document limit**: 100 MB
- **Total size of project file search stores** (based on user tier):
 - **Free**: 1 GB
 - **Tier 1**: 10 GB
 - **Tier 2**: 100 GB
 - **Tier 3**: 1 TB
- **Recommendation**: Limit the size of each file search store to under 20 GB to ensure optimal retrieval latencies.

| **Note:** The limit on file search store size is computed on the backend, based on the size of your input plus the embeddings generated and stored with it. This is typically approximately 3 times the size of your input data.

## Pricing

- Developers are charged for embeddings at indexing time based on existing[embeddings pricing](https://ai.google.dev/gemini-api/docs/pricing#gemini-embedding)($0.15 per 1M tokens).
- Storage is free of charge.
- Query time embeddings are free of charge.
- Retrieved document tokens are charged as regular[context tokens](https://ai.google.dev/gemini-api/docs/tokens).

## What's next

- Visit the API reference for[File Search Stores](https://ai.google.dev/api/file-search/file-search-stores)and File Search[Documents](https://ai.google.dev/api/file-search/documents).

[END OF DOCUMENT: GEMINI1BNX91CDID]
---

[START OF DOCUMENT: GEMINI1O1Y8T19V9 | Title: Files.Md]

The Gemini family of artificial intelligence (AI) models is built to handle
various types of input data, including text, images, and audio. Since these
models can handle more than one type or *mode* of data, the Gemini models
are called *multimodal models* or explained as having *multimodal capabilities*.

This guide shows you how to work with media files using the Files API. The
basic operations are the same for audio files, images, videos, documents, and
other supported file types.

For file prompting guidance, check out the [File prompt guide](https://ai.google.dev/gemini-api/docs/files#prompt-guide) section.

## Upload a file

You can use the Files API to upload a media file. Always use the Files API when
the total request size (including the files, text prompt, system instructions,
etc.) is larger than 20 MB.

The following code uploads a file and then uses the file in a call to
`generateContent`.

### Python

 from google import genai

 client = genai.Client()

 myfile = client.files.upload(file="path/to/sample.mp3")

 response = client.models.generate_content(
 model="gemini-2.5-flash", contents=["Describe this audio clip", myfile]
 )

 print(response.text)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";

 const ai = new GoogleGenAI({});

 async function main() {
 const myfile = await ai.files.upload({
 file: "path/to/sample.mp3",
 config: { mimeType: "audio/mpeg" },
 });

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: createUserContent([
 createPartFromUri(myfile.uri, myfile.mimeType),
 "Describe this audio clip",
 ]),
 });
 console.log(response.text);
 }

 await main();

### Go

 file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
 if err != nil {
 log.Fatal(err)
 }
 defer client.DeleteFile(ctx, file.Name)

 model := client.GenerativeModel("gemini-2.5-flash")
 resp, err := model.GenerateContent(ctx,
 genai.FileData{URI: file.URI},
 genai.Text("Describe this audio clip"))
 if err != nil {
 log.Fatal(err)
 }

 printResponse(resp)

### REST

 AUDIO_PATH="path/to/sample.mp3"
 MIME_TYPE=$(file -b --mime-type "${AUDIO_PATH}")
 NUM_BYTES=$(wc -c < "${AUDIO_PATH}")
 DISPLAY_NAME=AUDIO

 tmp_header_file=upload-header.tmp

 # Initial resumable request defining metadata.
 # The upload url is in the response headers dump them to a file.
 curl "${BASE_URL}/upload/v1beta/files" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -D "${tmp_header_file}" \
 -H "X-Goog-Upload-Protocol: resumable" \
 -H "X-Goog-Upload-Command: start" \
 -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
 -H "Content-Type: application/json" \
 -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

 upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
 rm "${tmp_header_file}"

 # Upload the actual bytes.
 curl "${upload_url}" \
 -H "Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Offset: 0" \
 -H "X-Goog-Upload-Command: upload, finalize" \
 --data-binary "@${AUDIO_PATH}" 2> /dev/null > file_info.json

 file_uri=$(jq ".file.uri" file_info.json)
 echo file_uri=$file_uri

 # Now generate content using that file
 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {"text": "Describe this audio clip"},
 {"file_data":{"mime_type": "${MIME_TYPE}", "file_uri": '$file_uri'}}]
 }]
 }' 2> /dev/null > response.json

 cat response.json
 echo

 jq ".candidates[].content.parts[].text" response.json

## Get metadata for a file

You can verify that the API successfully stored the uploaded file and get its
metadata by calling `files.get`.

### Python

 myfile = client.files.upload(file='path/to/sample.mp3')
 file_name = myfile.name
 myfile = client.files.get(name=file_name)
 print(myfile)

### JavaScript

 const myfile = await ai.files.upload({
 file: "path/to/sample.mp3",
 config: { mimeType: "audio/mpeg" },
 });

 const fileName = myfile.name;
 const fetchedFile = await ai.files.get({ name: fileName });
 console.log(fetchedFile);

### Go

 file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
 if err != nil {
 log.Fatal(err)
 }

 gotFile, err := client.GetFile(ctx, file.Name)
 if err != nil {
 log.Fatal(err)
 }
 fmt.Println("Got file:", gotFile.Name)

### REST

 # file_info.json was created in the upload example
 name=$(jq ".file.name" file_info.json)
 # Get the file of interest to check state
 curl https://generativelanguage.googleapis.com/v1beta/files/$name \
 -H "x-goog-api-key: $GEMINI_API_KEY" > file_info.json
 # Print some information about the file you got
 name=$(jq ".file.name" file_info.json)
 echo name=$name
 file_uri=$(jq ".file.uri" file_info.json)
 echo file_uri=$file_uri

## List uploaded files

You can upload multiple files using the Files API. The following code gets
a list of all the files uploaded:

### Python

 print('My files:')
 for f in client.files.list():
 print(' ', f.name)

### JavaScript

 const listResponse = await ai.files.list({ config: { pageSize: 10 } });
 for await (const file of listResponse) {
 console.log(file.name);
 }

### Go

 iter := client.ListFiles(ctx)
 for {
 ifile, err := iter.Next()
 if err == iterator.Done {
 break
 }
 if err != nil {
 log.Fatal(err)
 }
 fmt.Println(ifile.Name)
 }

### REST

 echo "My files: "

 curl "https://generativelanguage.googleapis.com/v1beta/files" \
 -H "x-goog-api-key: $GEMINI_API_KEY"

## Delete uploaded files

Files are automatically deleted after 48 hours. You can also manually delete an
uploaded file:

### Python

 myfile = client.files.upload(file='path/to/sample.mp3')
 client.files.delete(name=myfile.name)

### JavaScript

 const myfile = await ai.files.upload({
 file: "path/to/sample.mp3",
 config: { mimeType: "audio/mpeg" },
 });

 const fileName = myfile.name;
 await ai.files.delete({ name: fileName });

### Go

 file, err := client.UploadFileFromPath(ctx, "path/to/sample.mp3", nil)
 if err != nil {
 log.Fatal(err)
 }
 client.DeleteFile(ctx, file.Name)

### REST

 curl --request "DELETE" https://generativelanguage.googleapis.com/v1beta/files/$name \
 -H "x-goog-api-key: $GEMINI_API_KEY"

## Usage info

You can use the Files API to upload and interact with media files. The Files API
lets you store up to 20 GB of files per project, with a per-file maximum
size of 2 GB. Files are stored for 48 hours. During that time, you can
use the API to get metadata about the files, but you can't download the files.
The Files API is available at no cost in all regions where the Gemini API is
available.

## File prompting strategies

This section provides guidance and best practices for using media files with
prompts for the Gemini API.

Being able to use various types of data in your prompts gives you more
flexibility in terms of what tasks you can tackle with the Gemini API. For
example, you can send the model a photo of a delicious meal and ask it to write
a short blog about the meal.

|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Prompt** | **Response** |
| Write a short, engaging blog post based on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping. <br /> ![Cookies](https://ai.google.dev/static/gemini-api/docs/images/image_prompt.png) | Meal prepping is a great way to save time and money, and it can also help you to eat healthier. This meal is a great example of a healthy and delicious meal that can be easily prepped ahead of time. This meal features brown rice, roasted vegetables, and chicken teriyaki. The brown rice is a healthy and complex carbohydrate that will give you sustained energy. The roasted vegetables are a great way to get your daily dose of vitamins and minerals, and the chicken teriyaki is a delicious and protein-rich option. This meal is also very easy to prepare. Simply cook the brown rice, roast the vegetables, and cook the chicken teriyaki. Once everything is cooked, divide it into meal prep containers and store them in the refrigerator. You can then grab a container and go on busy mornings! If you're looking for a healthy and delicious meal that can be easily prepped ahead of time, this meal is a great option. It's packed with nutrients and flavor, and it's sure to keep you feeling full and satisfied. Here's to healthy and delicious meal prepping! |

If you are having trouble getting the output you want from prompts that use
media files, there are some strategies that can help you get the results you
want. The following sections provide design approaches and troubleshooting
tips for improving prompts that use multimodal input.

You can improve your multimodal prompts by following these best practices:

-

 ### [Prompt design fundamentals](https://ai.google.dev/gemini-api/docs/files#specific-instructions)

 - **Be specific in your instructions**: Craft clear and concise instructions that leave minimal room for misinterpretation.
 - **Add a few examples to your prompt:** Use realistic few-shot examples to illustrate what you want to achieve.
 - **Break it down step-by-step**: Divide complex tasks into manageable sub-goals, guiding the model through the process.
 - **Specify the output format**: In your prompt, ask for the output to be in the format you want, like markdown, JSON, HTML and more.
 - **Put your image first for single-image prompts**: While Gemini can handle image and text inputs in any order, for prompts containing a single image, it might perform better if that image (or video) is placed before the text prompt. However, for prompts that require images to be highly interleaved with texts to make sense, use whatever order is most natural.
-

 ### [Troubleshooting your multimodal prompt](https://ai.google.dev/gemini-api/docs/files#troubleshooting)

 - **If the model is not drawing information from the relevant part of the image:** Drop hints with which aspects of the image you want the prompt to draw information from.
 - **If the model output is too generic (not tailored enough to the image/video input):**At the start of the prompt, try asking the model to describe the image(s) or video before providing the task instruction, or try asking the model to refer to what's in the image.
 - **To troubleshoot which part failed:** Ask the model to describe the image, or ask the model to explain its reasoning, to gauge the model's initial understanding.
 - **If your prompt results in hallucinated content:** Try dialing down the temperature setting or asking the model for shorter descriptions so that it's less likely to extrapolate additional details.
 - **Tuning the sampling parameters:** Experiment with different temperature settings and top-k selections to adjust the model's creativity.

### Be specific in your instructions

Prompts have the most success when they are clear and detailed. If you have a
specific output in mind, it's better to include that requirement in the prompt
to ensure you get the output you want.

For this image of an airport board, asking the model to just "describe this
image" could generate a general description. If you need the model to parse the
time and city from the image, you can include that request directly in your prompt.

| Prompt | Model response |
|---------------------------------------------------------------------------------------------|-----------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/timetable.png) Describe this image. | The image shows an airport arrivals and departures board. |

<br />

| **Updated prompt** | **Improved response** |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![Travel timetable](https://ai.google.dev/static/gemini-api/docs/images/timetable.png) Parse the time and city from the airport board shown in this image into a list. | <br /> - 10:50 Moscow - 11:05 Edinburgh - 11:05 London - 11:10 Bucharest - 11:30 Kiev - 11:35 Dublin - 11:45 East Midlands - 12:15 Sofia - 12:30 London - 12:30 Newcastle - 12:40 St Petersburg - 12:40 London - 12:45 Manchester |

<br />

### Add a few examples

The Gemini model can accept multiple inputs which it can use as examples to understand the
output you want. Adding these examples can help the model identify the patterns and apply the
relationship between the given images and responses to the new example. This is also called
"few-shot" learning.

In the example below, the initial output is written in sentence form, and also contains the
country (Brazil). Suppose you want the output in a different format or style, and you want
only the city, not the country. Adding few-shot examples to your prompt can steer the model
to respond in the way you want.

<br />

| **Prompt** | **Model response** |
|-----------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------|
| ![Travel timetable](https://ai.google.dev/static/gemini-api/docs/images/redeemer.png) Determine the city along with the landmark. | The landmark is the Christ the Redeemer statue in Rio de Janeiro, Brazil. |

<br />

| **Updated prompt** | **Improved response** |
|-----------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------|
| Determine the city along with the landmark. ![colosseum](https://ai.google.dev/static/gemini-api/docs/images/colosseum.png) | city: Rome, landmark: the Colosseum. |
| ![Forbidden City](https://ai.google.dev/static/gemini-api/docs/images/forbidden-city.png) | city: Beijing, landmark: Forbidden City |
| ![forbidden-city](https://ai.google.dev/static/gemini-api/docs/images/redeemer.png) | city: Rio de Janeiro, landmark: Christ the Redeemer statue |

### Break it down step-by-step

For complex tasks like the ones that require both visual understanding and
reasoning, it can be helpful to split the task into smaller, more straightforward
steps. Alternatively, it could also be effective if you
directly ask the model to "think step by step" in your prompt.

| Prompt | Model response |
|----------------------------------------------------------------------------------------------------------------|-----------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg) When will I run out of toilet paper? | Soon, you only have 3 rolls left. |

| **Updated prompt** | **Improved response** |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg) 1. First, count how many toilet paper rolls are in this picture. <!-- --> 2. Then, determine how much toilet paper a typical person uses per day. <!-- --> 3. Calculate how long these rolls of toilet paper will last. <br /> | 1. There are 3 rolls of toilet paper in this picture. <!-- --> 2. A typical person uses about 20 sheets of toilet paper per day. <!-- --> 3. If each roll contains 200 sheets, then each roll will last for about 10 days. Therefore, the 3 rolls will last for about a month. <br /> |

Math problems or other types of word problems are great candidates for asking
the model to think step-by-step.

| **Prompt** | **Response** |
|----------------------------------------------------------------------------------------------------------------|--------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/math.jpg) What is the 4th term in the sequence? <br /> | -135 |

The response from the model is incorrect. Some ways to improve this is to ask
the model to parse the problem before answering the prompt.

| **Prompt** | **Response** |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/math.jpg) Parse the formula in the image first. Then based on the formula, answer "what's the 4th term in the sequence?". <br /> | <br /> *Click to expand the result* $$ b(n) = b(n-1) \* (-3) $$ $$ b(1) = 15 $$ 1. Substitute n = 4 into the formula: 2. $$ b(4) = b(4-1) \* (-3) $$ 3. $$ b(4) = b(3) \* (-3) $$ 4. We don't know the value of b(3), so substitute n = 3 into the formula: 5. $$ b(3) = b(3-1) \* (-3) $$ 6. $$ b(3) = b(2) \* (-3) $$ 7. We don't know the value of b(2), so substitute n = 2 into the formula: 8. $$ b(2) = b(2-1) \* (-3) $$ 9. $$ b(2) = b(1) \* (-3) $$ 10. We know the value of b(1) is 15, so substitute it into the formula: 11. $$ b(2) = 15 \* (-3) $$ 12. $$ b(2) = -45 $$ 13. Now that we know the value of b(2), substitute it into the formula for b(3): 14. $$ b(3) = b(2) \* (-3) $$ 15. $$ b(3) = -45 \* (-3) $$ 16. $$ b(3) = 135 $$ 17. Now that we know the value of b(3), substitute it into the formula for b(4): 18. $$ b(4) = b(3) \* (-3) $$ 19. $$ b(4) = 135 \* (-3) $$ 20. $$ b(4) = -405 $$ Therefore, the 4th term in the sequence is -405. <br /> |

This model response is correct - now to make the response even more streamlined,
you can ask the model to to think step-by-step.

<br />

| **Updated prompt** | **Improved response** |
|-----------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/math.jpg) What is the 4th term in the sequence? Think step by step. | The first term is 15. The second term is b(1) \* (-3) = 15 \* (-3) = -45. The third term is b(2) \* (-3) = -45 \* (-3) = 135. The fourth term is b(3) \* (-3) = 135 \* (-3) = -405. |

<br />

### Specify the output format

A common problem is the need for model output to be in a certain format
(e.g. markdown, JSON, HTML), especially if the model output needs to
be ingested by a downstream task. You can try to produce output in that format
by instructing the model to do so within the prompt. Below are two examples:

| Prompt | Model response |
|-------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/excel-example.png) Parse the table in this image into markdown format | \| Subject \| Status \| Time \| Start date \| Due on \| \| ------- \| ------ \| ---- \| ---------- \| ------ \| \| Math \| Done \| 1.5 hrs \| 11/29/2023 \| 12/1/2023 \| \| Geography \| Done \| 30 min \| 12/2/2023 \| 12/4/2023 \| \| Physics \| In progress \| 2 hrs \| 12/3/2023 \| 12/5/2023 \| \| Biology \| Not started \| 6 hrs \| 11/2/2016 \| 12/6/2023 \| |

| **Prompt** | **Model response** |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/spanish-meal.png) Provide a list of all the following attributes: ingredients, type of cuisine, vegetarian or not, in JSON format | ` ````````json```` ` `{ ` ` "ingredients": [ ` ` "rice", ` ` "shrimp", ` ` "clams", ` ` "mussels", ` ` "peas", ` ` "tomatoes", ` ` "onions", ` ` "garlic", ` ` "olive oil", ` ` "paprika", ` ` "salt", ` ` "pepper" ` ` ], ` ` "type of cuisine": "Spanish", ` ` "vegetarian": false ` ` } ` ` ```` ` <br /> |

### Put your image first for single-image prompts

While Gemini can interpret image and text in any order within a prompt, placing
a single image before the text prompt might lead to better results.
In the following samples, you'll notice the image comes first before the text in
each prompt.

### Troubleshooting your multimodal prompt

You might need to troubleshoot your prompt if you are not getting a helpful
response. Here are a few strategies you could try.

#### If the model is not drawing information from the relevant part of the image

To get a more specific response, you can point out which aspects of the image
you want the prompt to draw information from to inform its response.

| Prompt | Model response |
|--------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/diapers.png) How many days will these diapers last a baby? | The diapers will last for 198 days before they run out. |

| **Updated prompt** | **Improved response** |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/diapers.png) How long will these diapers last before I run out? Use the weight shown on the box to determine the child's age, and use the total number of diapers in the box. Divide the total number by how many diapers the child goes through per day. | The box contains 198 diapers. The box states the diapers are for a child weighing 22-37 lbs. The average child in this weight range is likely between 1 and 2 years old. A 2-year-old typically goes through 6 diapers per day, therefore, the diapers will last around 33 days (198/6). |

#### If the model output is too generic and not tailored enough to the image input

To help the model tailor its response to the image(s), try asking it to describe
the images before performing its reasoning task.

| Prompt | Model response |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/packing.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/mall.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/ocean.jpg) What is in common between these images? | All four images are examples of landscape photography. |

| **Updated prompt** | **Improved response** |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/packing.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/mall.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/ocean.jpg) First, describe what's in each image in detail. What's in common between these images? | The first image shows a person packing a suitcase. The second image shows an empty hallway in a shopping mall. The third image shows a roll of toilet paper. The fourth image shows a dark and stormy sea. The commonality between these images is that they are all examples of liminal spaces. Liminal spaces are places or situations that are in between two other places or situations. They are often characterized by a sense of uncertainty or ambiguity. The first image shows the person in between being at home and being on a trip. The second image shows the hallway in between two stores. The third image is of toilet paper, which can represent a bathroom. A bathroom is a transitional space between private and public realms. It's a place where we prepare ourselves to enter or leave the outside world. The fourth image shows the sea in between being calm and being stormy. Liminal spaces can be both exciting and unsettling. They can be a place of new beginning or a place of endings. They can be a place of reflection or a place of action. |

You can also try explicitly asking the model to refer to what's in the image.

| **Updated prompt** | **Improved response** |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/packing.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/mall.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/toilet-paper.jpg) ![](https://ai.google.dev/static/gemini-api/docs/images/ocean.jpg) What is in common between these images? Refer to what's in the images in your response. | All of the images are related to travel. The first image shows a person packing a suitcase, the second image shows an empty hallway in a shopping mall, the third image shows a stack of toilet paper, and the fourth image shows a dark and stormy sea. These images are all related to travel because they are all things that people need or use when they travel. The first image shows a person packing a suitcase, which is something that people do when they are going on a trip. The second image shows an empty hallway in a shopping mall, which is a place where people often go to buy things for their trips. The third image shows a stack of toilet paper, which is something that people often use when they are traveling. The fourth image shows a dark and stormy sea, which is something that people often encounter when they are traveling. |

The same tips work for creative output from images. In the following example with
the rabbit, explicitly asking the model to include aspects from the image can
improve the output.

| Prompt | Model response |
|---------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/rabbit.jpg) Can you write me a poem about this image? | Little bunny, so soft and brown, Hopping through the field, up and down. Your ears so long, your tail so small, You're the cutest creature of them all. You nibble on the grass, so green and sweet, And hop away when you see me. But I'll always be your friend, Little bunny, until the very end. |

<br />

| **Updated prompt** | **Improved response** |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/rabbit.jpg) Can you write me a descriptive and dramatic poem about this image and include the location? | Little bunny on the green, Ears so long, so keen. What do you see out there, In the distance, where you stare? Is it the ocean blue, Sparkling in the sun's hue? Or the waves that crash and roar, A rhythm that you adore? Little bunny, tell me true, What captures your attention so? Is it the salty breeze, Or the seagulls' cries that please? Perhaps it's the vastness of it all, A world so big and small. A place where dreams can roam, And adventures can be sown. Little bunny, I wonder what you dream, As you sit on the grass, so serene. Do you yearn to explore the deep, Or stay on land, where you can leap? Whatever it may be, little bunny, Keep that spark of wonder burning brightly. For in your dreams and aspirations, Lies a world waiting for your creation. |

<br />

#### Troubleshooting which part of the prompt failed

It can be hard to know whether a prompt failed because the model
didn't **understand the image** to begin with, or if it did understand the image
but did not perform the correct **reasoning steps** afterward.
To disambiguate between those reasons, ask the model to describe what's in the image.

In the following example, if the model responds with a snack that seems surprising
when paired with tea (e.g. popcorn), you can first troubleshoot to determine
whether the model correctly recognized that the image contains tea.

| Prompt | Prompt for troubleshooting |
|------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png) What's a snack I can make in 1 minute that would go well with this? | ![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png) Describe what's in this image. |

Another strategy is to ask the model to explain its reasoning. That can help you
narrow down which part of the reasoning broke down, if any.

| Prompt | Prompt for troubleshooting |
|------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png) What's a snack I can make in 1 minute that would go well with this? | ![](https://ai.google.dev/static/gemini-api/docs/images/tea-set.png) What's a snack I can make in 1 minute that would go well with this? Please explain why. |

## What's next

- Try writing your own multimodal prompts using [Google AI
 Studio](http://aistudio.google.com).
- For information on using the Gemini Files API for uploading media files and including them in your prompts, see the [Vision](https://ai.google.dev/gemini-api/docs/vision), [Audio](https://ai.google.dev/gemini-api/docs/audio), and [Document processing](https://ai.google.dev/gemini-api/docs/document-processing) guides.
- For more guidance on prompt design, like tuning sampling parameters, see the [Prompt strategies](https://ai.google.dev/gemini-api/docs/prompting-strategies) page.

[END OF DOCUMENT: GEMINI1O1Y8T19V9]
---

[START OF DOCUMENT: GEMINI5WBTXZ7SI | Title: Function-Calling.Md]

Function calling lets you connect models to external tools and APIs.
Instead of generating text responses, the model determines when to call specific
functions and provides the necessary parameters to execute real-world actions.
This allows the model to act as a bridge between natural language and real-world
actions and data. Function calling has 3 primary use cases:

- **Augment Knowledge:** Access information from external sources like databases, APIs, and knowledge bases.
- **Extend Capabilities:** Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.
- **Take Actions:** Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices.

Get Weather Schedule Meeting Create Chart

### Python

 from google import genai
 from google.genai import types

 # Define the function declaration for the model
 schedule_meeting_function = {
 "name": "schedule_meeting",
 "description": "Schedules a meeting with specified attendees at a given time and date.",
 "parameters": {
 "type": "object",
 "properties": {
 "attendees": {
 "type": "array",
 "items": {"type": "string"},
 "description": "List of people attending the meeting.",
 },
 "date": {
 "type": "string",
 "description": "Date of the meeting (e.g., '2024-07-29')",
 },
 "time": {
 "type": "string",
 "description": "Time of the meeting (e.g., '15:00')",
 },
 "topic": {
 "type": "string",
 "description": "The subject or topic of the meeting.",
 },
 },
 "required": ["attendees", "date", "time", "topic"],
 },
 }

 # Configure the client and tools
 client = genai.Client()
 tools = types.Tool(function_declarations=[schedule_meeting_function])
 config = types.GenerateContentConfig(tools=[tools])

 # Send request with function declarations
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="Schedule a meeting with Bob and Alice for 03/14/2025 at 10:00 AM about the Q3 planning.",
 config=config,
 )

 # Check for a function call
 if response.candidates[0].content.parts[0].function_call:
 function_call = response.candidates[0].content.parts[0].function_call
 print(f"Function to call: {function_call.name}")
 print(f"Arguments: {function_call.args}")
 # In a real app, you would call your function here:
 # result = schedule_meeting(**function_call.args)
 else:
 print("No function call found in the response.")
 print(response.text)

### JavaScript

 import { GoogleGenAI, Type } from '@google/genai';

 // Configure the client
 const ai = new GoogleGenAI({});

 // Define the function declaration for the model
 const scheduleMeetingFunctionDeclaration = {
 name: 'schedule_meeting',
 description: 'Schedules a meeting with specified attendees at a given time and date.',
 parameters: {
 type: Type.OBJECT,
 properties: {
 attendees: {
 type: Type.ARRAY,
 items: { type: Type.STRING },
 description: 'List of people attending the meeting.',
 },
 date: {
 type: Type.STRING,
 description: 'Date of the meeting (e.g., "2024-07-29")',
 },
 time: {
 type: Type.STRING,
 description: 'Time of the meeting (e.g., "15:00")',
 },
 topic: {
 type: Type.STRING,
 description: 'The subject or topic of the meeting.',
 },
 },
 required: ['attendees', 'date', 'time', 'topic'],
 },
 };

 // Send request with function declarations
 const response = await ai.models.generateContent({
 model: 'gemini-2.5-flash',
 contents: 'Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning.',
 config: {
 tools: [{
 functionDeclarations: [scheduleMeetingFunctionDeclaration]
 }],
 },
 });

 // Check for function calls in the response
 if (response.functionCalls && response.functionCalls.length > 0) {
 const functionCall = response.functionCalls[0]; // Assuming one function call
 console.log(`Function to call: ${functionCall.name}`);
 console.log(`Arguments: ${JSON.stringify(functionCall.args)}`);
 // In a real app, you would call your actual function here:
 // const result = await scheduleMeeting(functionCall.args);
 } else {
 console.log("No function call found in the response.");
 console.log(response.text);
 }

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [
 {
 "role": "user",
 "parts": [
 {
 "text": "Schedule a meeting with Bob and Alice for 03/27/2025 at 10:00 AM about the Q3 planning."
 }
 ]
 }
 ],
 "tools": [
 {
 "functionDeclarations": [
 {
 "name": "schedule_meeting",
 "description": "Schedules a meeting with specified attendees at a given time and date.",
 "parameters": {
 "type": "object",
 "properties": {
 "attendees": {
 "type": "array",
 "items": {"type": "string"},
 "description": "List of people attending the meeting."
 },
 "date": {
 "type": "string",
 "description": "Date of the meeting (e.g., '2024-07-29')"
 },
 "time": {
 "type": "string",
 "description": "Time of the meeting (e.g., '15:00')"
 },
 "topic": {
 "type": "string",
 "description": "The subject or topic of the meeting."
 }
 },
 "required": ["attendees", "date", "time", "topic"]
 }
 }
 ]
 }
 ]
 }'

## How function calling works

![function calling
overview](https://ai.google.dev/static/gemini-api/docs/images/function-calling-overview.png)

Function calling involves a structured interaction between your application, the
model, and external functions. Here's a breakdown of the process:

1. **Define Function Declaration:** Define the function declaration in your application code. Function Declarations describe the function's name, parameters, and purpose to the model.
2. **Call LLM with function declarations:** Send user prompt along with the function declaration(s) to the model. It analyzes the request and determines if a function call would be helpful. If so, it responds with a structured JSON object.
3. **Execute Function Code (Your Responsibility):** The Model *does not* execute the function itself. It's your application's responsibility to process the response and check for Function Call, if
 - **Yes**: Extract the name and args of the function and execute the corresponding function in your application.
 - **No:** The model has provided a direct text response to the prompt (this flow is less emphasized in the example but is a possible outcome).
4. **Create User friendly response:** If a function was executed, capture the result and send it back to the model in a subsequent turn of the conversation. It will use the result to generate a final, user-friendly response that incorporates the information from the function call.

This process can be repeated over multiple turns, allowing for complex
interactions and workflows. The model also supports calling multiple functions
in a single turn ([parallel function
calling](https://ai.google.dev/gemini-api/docs/function-calling#parallel_function_calling)) and in
sequence ([compositional function
calling](https://ai.google.dev/gemini-api/docs/function-calling#compositional_function_calling)).

### Step 1: Define a function declaration

Define a function and its declaration within your application code that allows
users to set light values and make an API request. This function could call
external services or APIs.

### Python

 # Define a function that the model can call to control smart lights
 set_light_values_declaration = {
 "name": "set_light_values",
 "description": "Sets the brightness and color temperature of a light.",
 "parameters": {
 "type": "object",
 "properties": {
 "brightness": {
 "type": "integer",
 "description": "Light level from 0 to 100. Zero is off and 100 is full brightness",
 },
 "color_temp": {
 "type": "string",
 "enum": ["daylight", "cool", "warm"],
 "description": "Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.",
 },
 },
 "required": ["brightness", "color_temp"],
 },
 }

 # This is the actual function that would be called based on the model's suggestion
 def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:
 """Set the brightness and color temperature of a room light. (mock API).

 Args:
 brightness: Light level from 0 to 100. Zero is off and 100 is full brightness
 color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.

 Returns:
 A dictionary containing the set brightness and color temperature.
 """
 return {"brightness": brightness, "colorTemperature": color_temp}

### JavaScript

 import { Type } from '@google/genai';

 // Define a function that the model can call to control smart lights
 const setLightValuesFunctionDeclaration = {
 name: 'set_light_values',
 description: 'Sets the brightness and color temperature of a light.',
 parameters: {
 type: Type.OBJECT,
 properties: {
 brightness: {
 type: Type.NUMBER,
 description: 'Light level from 0 to 100. Zero is off and 100 is full brightness',
 },
 color_temp: {
 type: Type.STRING,
 enum: ['daylight', 'cool', 'warm'],
 description: 'Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.',
 },
 },
 required: ['brightness', 'color_temp'],
 },
 };

 /**

 * Set the brightness and color temperature of a room light. (mock API)
 * @param {number} brightness - Light level from 0 to 100. Zero is off and 100 is full brightness
 * @param {string} color_temp - Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.
 * @return {Object} A dictionary containing the set brightness and color temperature.
 */
 function setLightValues(brightness, color_temp) {
 return {
 brightness: brightness,
 colorTemperature: color_temp
 };
 }

### Step 2: Call the model with function declarations

Once you have defined your function declarations, you can prompt the model to
use them. It analyzes the prompt and function declarations and decides whether
to respond directly or to call a function. If a function is called, the response
object will contain a function call suggestion.

### Python

 from google.genai import types

 # Configure the client and tools
 client = genai.Client()
 tools = types.Tool(function_declarations=[set_light_values_declaration])
 config = types.GenerateContentConfig(tools=[tools])

 # Define user prompt
 contents = [
 types.Content(
 role="user", parts=[types.Part(text="Turn the lights down to a romantic level")]
 )
 ]

 # Send request with function declarations
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents=contents
 config=config,
 )

 print(response.candidates[0].content.parts[0].function_call)

### JavaScript

 import { GoogleGenAI } from '@google/genai';

 // Generation config with function declaration
 const config = {
 tools: [{
 functionDeclarations: [setLightValuesFunctionDeclaration]
 }]
 };

 // Configure the client
 const ai = new GoogleGenAI({});

 // Define user prompt
 const contents = [
 {
 role: 'user',
 parts: [{ text: 'Turn the lights down to a romantic level' }]
 }
 ];

 // Send request with function declarations
 const response = await ai.models.generateContent({
 model: 'gemini-2.5-flash',
 contents: contents,
 config: config
 });

 console.log(response.functionCalls[0]);

The model then returns a `functionCall` object in an OpenAPI compatible
schema specifying how to call one or more of the declared functions in order to
respond to the user's question.

### Python

 id=None args={'color_temp': 'warm', 'brightness': 25} name='set_light_values'

### JavaScript

 {
 name: 'set_light_values',
 args: { brightness: 25, color_temp: 'warm' }
 }

### Step 3: Execute set_light_values function code

Extract the function call details from the model's response, parse the arguments
, and execute the `set_light_values` function.

### Python

 # Extract tool call details, it may not be in the first part.
 tool_call = response.candidates[0].content.parts[0].function_call

 if tool_call.name == "set_light_values":
 result = set_light_values(**tool_call.args)
 print(f"Function execution result: {result}")

### JavaScript

 // Extract tool call details
 const tool_call = response.functionCalls[0]

 let result;
 if (tool_call.name === 'set_light_values') {
 result = setLightValues(tool_call.args.brightness, tool_call.args.color_temp);
 console.log(`Function execution result: ${JSON.stringify(result)}`);
 }

### Step 4: Create user friendly response with function result and call the model again

Finally, send the result of the function execution back to the model so it can
incorporate this information into its final response to the user.

### Python

 from google import genai
 from google.genai import types

 # Create a function response part
 function_response_part = types.Part.from_function_response(
 name=tool_call.name,
 response={"result": result},
 )

 # Append function call and result of the function execution to contents
 contents.append(response.candidates[0].content) # Append the content from the model's response.
 contents.append(types.Content(role="user", parts=[function_response_part])) # Append the function response

 client = genai.Client()
 final_response = client.models.generate_content(
 model="gemini-2.5-flash",
 config=config,
 contents=contents,
 )

 print(final_response.text)

### JavaScript

 // Create a function response part
 const function_response_part = {
 name: tool_call.name,
 response: { result }
 }

 // Append function call and result of the function execution to contents
 contents.push(response.candidates[0].content);
 contents.push({ role: 'user', parts: [{ functionResponse: function_response_part }] });

 // Get the final response from the model
 const final_response = await ai.models.generateContent({
 model: 'gemini-2.5-flash',
 contents: contents,
 config: config
 });

 console.log(final_response.text);

This completes the function calling flow. The model successfully used the
`set_light_values` function to perform the request action of the user.

## Function declarations

When you implement function calling in a prompt, you create a `tools` object,
which contains one or more `function declarations`. You define functions using
JSON, specifically with a [select subset](https://ai.google.dev/api/caching#Schema)
of the [OpenAPI schema](https://spec.openapis.org/oas/v3.0.3#schemaw) format. A
single function declaration can include the following parameters:

- `name` (string): A unique name for the function (`get_weather_forecast`, `send_email`). Use descriptive names without spaces or special characters (use underscores or camelCase).
- `description` (string): A clear and detailed explanation of the function's purpose and capabilities. This is crucial for the model to understand when to use the function. Be specific and provide examples if helpful ("Finds theaters based on location and optionally movie title which is currently playing in theaters.").
- `parameters` (object): Defines the input parameters the function expects.
 - `type` (string): Specifies the overall data type, such as `object`.
 - `properties` (object): Lists individual parameters, each with:
 - `type` (string): The data type of the parameter, such as `string`, `integer`, `boolean, array`.
 - `description` (string): A description of the parameter's purpose and format. Provide examples and constraints ("The city and state, e.g., 'San Francisco, CA' or a zip code e.g., '95616'.").
 - `enum` (array, optional): If the parameter values are from a fixed set, use "enum" to list the allowed values instead of just describing them in the description. This improves accuracy ("enum": \["daylight", "cool", "warm"\]).
 - `required` (array): An array of strings listing the parameter names that are mandatory for the function to operate.

You can also construct FunctionDeclarations from Python functions directly using
`types.FunctionDeclaration.from_callable(client=client, callable=your_function)`.

## Function calling with thinking

Enabling "[thinking](https://ai.google.dev/gemini-api/docs/thinking)" can improve function call
performance by allowing the model to reason through a request before suggesting
function calls. The Gemini API is stateless, the model's reasoning context will
be lost between turns in a multi-turn conversation. To preserve this context,
you can use thought signatures. A thought signature is an encrypted
representation of the model's internal thought process that you pass back to
the model on subsequent turns.

The [standard pattern for multi-turn tool](https://ai.google.dev/gemini-api/docs/function-calling?example=weather#step-4)
use is to append the model's complete previous response to the conversation
history. The `content` object includes the `thought_signatures` automatically.
If you follow this pattern **No code changes are required**.

### Manually managing thought signatures

If you modify the conversation history manually---instead of sending the complete previous response and want to benefit from thinking you must correctly handle the `thought_signature` included in the model's turn.

Follow these rules to ensure the model's context is preserved:

- Always send the `thought_signature` back to the model inside its original `Part`.
- Don't merge a `Part` containing a signature with one that does not. This breaks the positional context of the thought.
- Don't combine two `Parts` that both contain signatures, as the signature strings cannot be merged.

### Inspecting Thought Signatures

While not necessary for implementation, you can inspect the response to see the
`thought_signature` for debugging or educational purposes.

### Python

 import base64
 # After receiving a response from a model with thinking enabled
 # response = client.models.generate_content(...)

 # The signature is attached to the response part containing the function call
 part = response.candidates[0].content.parts[0]
 if part.thought_signature:
 print(base64.b64encode(part.thought_signature).decode("utf-8"))

### JavaScript

 // After receiving a response from a model with thinking enabled
 // const response = await ai.models.generateContent(...)

 // The signature is attached to the response part containing the function call
 const part = response.candidates[0].content.parts[0];
 if (part.thoughtSignature) {
 console.log(part.thoughtSignature);
 }

Learn more about limitations and usage of thought signatures, and about thinking
models in general, on the [Thinking](https://ai.google.dev/gemini-api/docs/thinking#signatures) page.

## Parallel function calling

In addition to single turn function calling, you can also call multiple
functions at once. Parallel function calling lets you execute multiple functions
at once and is used when the functions are not dependent on each other. This is
useful in scenarios like gathering data from multiple independent sources, such
as retrieving customer details from different databases or checking inventory
levels across various warehouses or performing multiple actions such as
converting your apartment into a disco.

### Python

 power_disco_ball = {
 "name": "power_disco_ball",
 "description": "Powers the spinning disco ball.",
 "parameters": {
 "type": "object",
 "properties": {
 "power": {
 "type": "boolean",
 "description": "Whether to turn the disco ball on or off.",
 }
 },
 "required": ["power"],
 },
 }

 start_music = {
 "name": "start_music",
 "description": "Play some music matching the specified parameters.",
 "parameters": {
 "type": "object",
 "properties": {
 "energetic": {
 "type": "boolean",
 "description": "Whether the music is energetic or not.",
 },
 "loud": {
 "type": "boolean",
 "description": "Whether the music is loud or not.",
 },
 },
 "required": ["energetic", "loud"],
 },
 }

 dim_lights = {
 "name": "dim_lights",
 "description": "Dim the lights.",
 "parameters": {
 "type": "object",
 "properties": {
 "brightness": {
 "type": "number",
 "description": "The brightness of the lights, 0.0 is off, 1.0 is full.",
 }
 },
 "required": ["brightness"],
 },
 }

### JavaScript

 import { Type } from '@google/genai';

 const powerDiscoBall = {
 name: 'power_disco_ball',
 description: 'Powers the spinning disco ball.',
 parameters: {
 type: Type.OBJECT,
 properties: {
 power: {
 type: Type.BOOLEAN,
 description: 'Whether to turn the disco ball on or off.'
 }
 },
 required: ['power']
 }
 };

 const startMusic = {
 name: 'start_music',
 description: 'Play some music matching the specified parameters.',
 parameters: {
 type: Type.OBJECT,
 properties: {
 energetic: {
 type: Type.BOOLEAN,
 description: 'Whether the music is energetic or not.'
 },
 loud: {
 type: Type.BOOLEAN,
 description: 'Whether the music is loud or not.'
 }
 },
 required: ['energetic', 'loud']
 }
 };

 const dimLights = {
 name: 'dim_lights',
 description: 'Dim the lights.',
 parameters: {
 type: Type.OBJECT,
 properties: {
 brightness: {
 type: Type.NUMBER,
 description: 'The brightness of the lights, 0.0 is off, 1.0 is full.'
 }
 },
 required: ['brightness']
 }
 };

Configure the function calling mode to allow using all of the specified tools.
To learn more, you can read about
[configuring function calling](https://ai.google.dev/gemini-api/docs/function-calling#function_calling_modes).

### Python

 from google import genai
 from google.genai import types

 # Configure the client and tools
 client = genai.Client()
 house_tools = [
 types.Tool(function_declarations=[power_disco_ball, start_music, dim_lights])
 ]
 config = types.GenerateContentConfig(
 tools=house_tools,
 automatic_function_calling=types.AutomaticFunctionCallingConfig(
 disable=True
 ),
 # Force the model to call 'any' function, instead of chatting.
 tool_config=types.ToolConfig(
 function_calling_config=types.FunctionCallingConfig(mode='ANY')
 ),
 )

 chat = client.chats.create(model="gemini-2.5-flash", config=config)
 response = chat.send_message("Turn this place into a party!")

 # Print out each of the function calls requested from this single call
 print("Example 1: Forced function calling")
 for fn in response.function_calls:
 args = ", ".join(f"{key}={val}" for key, val in fn.args.items())
 print(f"{fn.name}({args})")

### JavaScript

 import { GoogleGenAI } from '@google/genai';

 // Set up function declarations
 const houseFns = [powerDiscoBall, startMusic, dimLights];

 const config = {
 tools: [{
 functionDeclarations: houseFns
 }],
 // Force the model to call 'any' function, instead of chatting.
 toolConfig: {
 functionCallingConfig: {
 mode: 'any'
 }
 }
 };

 // Configure the client
 const ai = new GoogleGenAI({});

 // Create a chat session
 const chat = ai.chats.create({
 model: 'gemini-2.5-flash',
 config: config
 });
 const response = await chat.sendMessage({message: 'Turn this place into a party!'});

 // Print out each of the function calls requested from this single call
 console.log("Example 1: Forced function calling");
 for (const fn of response.functionCalls) {
 const args = Object.entries(fn.args)
 .map(([key, val]) => `${key}=${val}`)
 .join(', ');
 console.log(`${fn.name}(${args})`);
 }

Each of the printed results reflects a single function call that the model has
requested. To send the results back, include the responses in the same order as
they were requested.

The Python SDK supports [automatic function calling](https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only),
which automatically converts Python functions to declarations, handles the
function call execution and response cycle for you. Following is an example for
the disco use case.
**Note:** Automatic Function Calling is a Python SDK only feature at the moment.

### Python

 from google import genai
 from google.genai import types

 # Actual function implementations
 def power_disco_ball_impl(power: bool) -> dict:
 """Powers the spinning disco ball.

 Args:
 power: Whether to turn the disco ball on or off.

 Returns:
 A status dictionary indicating the current state.
 """
 return {"status": f"Disco ball powered {'on' if power else 'off'}"}

 def start_music_impl(energetic: bool, loud: bool) -> dict:
 """Play some music matching the specified parameters.

 Args:
 energetic: Whether the music is energetic or not.
 loud: Whether the music is loud or not.

 Returns:
 A dictionary containing the music settings.
 """
 music_type = "energetic" if energetic else "chill"
 volume = "loud" if loud else "quiet"
 return {"music_type": music_type, "volume": volume}

 def dim_lights_impl(brightness: float) -> dict:
 """Dim the lights.

 Args:
 brightness: The brightness of the lights, 0.0 is off, 1.0 is full.

 Returns:
 A dictionary containing the new brightness setting.
 """
 return {"brightness": brightness}

 # Configure the client
 client = genai.Client()
 config = types.GenerateContentConfig(
 tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]
 )

 # Make the request
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="Do everything you need to this place into party!",
 config=config,
 )

 print("\nExample 2: Automatic function calling")
 print(response.text)
 # I've turned on the disco ball, started playing loud and energetic music, and dimmed the lights to 50% brightness. Let's get this party started!

## Compositional function calling

Compositional or sequential function calling allows Gemini to chain multiple
function calls together to fulfill a complex request. For example, to answer
"Get the temperature in my current location", the Gemini API might first invoke
a `get_current_location()` function followed by a `get_weather()` function that
takes the location as a parameter.

The following example demonstrates how to implement compositional function
calling using the Python SDK and automatic function calling.

### Python

This example uses the automatic function calling feature of the
`google-genai` Python SDK. The SDK automatically converts the Python
functions to the required schema, executes the function calls when requested
by the model, and sends the results back to the model to complete the task.

 import os
 from google import genai
 from google.genai import types

 # Example Functions
 def get_weather_forecast(location: str) -> dict:
 """Gets the current weather temperature for a given location."""
 print(f"Tool Call: get_weather_forecast(location={location})")
 # TODO: Make API call
 print("Tool Response: {'temperature': 25, 'unit': 'celsius'}")
 return {"temperature": 25, "unit": "celsius"} # Dummy response

 def set_thermostat_temperature(temperature: int) -> dict:
 """Sets the thermostat to a desired temperature."""
 print(f"Tool Call: set_thermostat_temperature(temperature={temperature})")
 # TODO: Interact with a thermostat API
 print("Tool Response: {'status': 'success'}")
 return {"status": "success"}

 # Configure the client and model
 client = genai.Client()
 config = types.GenerateContentConfig(
 tools=[get_weather_forecast, set_thermostat_temperature]
 )

 # Make the request
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="If it's warmer than 20C in London, set the thermostat to 20C, otherwise set it to 18C.",
 config=config,
 )

 # Print the final, user-facing response
 print(response.text)

**Expected Output**

When you run the code, you will see the SDK orchestrating the function
calls. The model first calls `get_weather_forecast`, receives the
temperature, and then calls `set_thermostat_temperature` with the correct
value based on the logic in the prompt.

 Tool Call: get_weather_forecast(location=London)
 Tool Response: {'temperature': 25, 'unit': 'celsius'}
 Tool Call: set_thermostat_temperature(temperature=20)
 Tool Response: {'status': 'success'}
 OK. I've set the thermostat to 20C.

### JavaScript

This example shows how to use JavaScript/TypeScript SDK to do comopositional
function calling using a manual execution loop.

 import { GoogleGenAI, Type } from "@google/genai";

 // Configure the client
 const ai = new GoogleGenAI({});

 // Example Functions
 function get_weather_forecast({ location }) {
 console.log(`Tool Call: get_weather_forecast(location=${location})`);
 // TODO: Make API call
 console.log("Tool Response: {'temperature': 25, 'unit': 'celsius'}");
 return { temperature: 25, unit: "celsius" };
 }

 function set_thermostat_temperature({ temperature }) {
 console.log(
 `Tool Call: set_thermostat_temperature(temperature=${temperature})`,
 );
 // TODO: Make API call
 console.log("Tool Response: {'status': 'success'}");
 return { status: "success" };
 }

 const toolFunctions = {
 get_weather_forecast,
 set_thermostat_temperature,
 };

 const tools = [
 {
 functionDeclarations: [
 {
 name: "get_weather_forecast",
 description:
 "Gets the current weather temperature for a given location.",
 parameters: {
 type: Type.OBJECT,
 properties: {
 location: {
 type: Type.STRING,
 },
 },
 required: ["location"],
 },
 },
 {
 name: "set_thermostat_temperature",
 description: "Sets the thermostat to a desired temperature.",
 parameters: {
 type: Type.OBJECT,
 properties: {
 temperature: {
 type: Type.NUMBER,
 },
 },
 required: ["temperature"],
 },
 },
 ],
 },
 ];

 // Prompt for the model
 let contents = [
 {
 role: "user",
 parts: [
 {
 text: "If it's warmer than 20C in London, set the thermostat to 20C, otherwise set it to 18C.",
 },
 ],
 },
 ];

 // Loop until the model has no more function calls to make
 while (true) {
 const result = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents,
 config: { tools },
 });

 if (result.functionCalls && result.functionCalls.length > 0) {
 const functionCall = result.functionCalls[0];

 const { name, args } = functionCall;

 if (!toolFunctions[name]) {
 throw new Error(`Unknown function call: ${name}`);
 }

 // Call the function and get the response.
 const toolResponse = toolFunctions[name](args);

 const functionResponsePart = {
 name: functionCall.name,
 response: {
 result: toolResponse,
 },
 };

 // Send the function response back to the model.
 contents.push({
 role: "model",
 parts: [
 {
 functionCall: functionCall,
 },
 ],
 });
 contents.push({
 role: "user",
 parts: [
 {
 functionResponse: functionResponsePart,
 },
 ],
 });
 } else {
 // No more function calls, break the loop.
 console.log(result.text);
 break;
 }
 }

**Expected Output**

When you run the code, you will see the SDK orchestrating the function
calls. The model first calls `get_weather_forecast`, receives the
temperature, and then calls `set_thermostat_temperature` with the correct
value based on the logic in the prompt.

 Tool Call: get_weather_forecast(location=London)
 Tool Response: {'temperature': 25, 'unit': 'celsius'}
 Tool Call: set_thermostat_temperature(temperature=20)
 Tool Response: {'status': 'success'}
 OK. It's 25C in London, so I've set the thermostat to 20C.

Compositional function calling is a native [Live
API](https://ai.google.dev/gemini-api/docs/live) feature. This means Live API
can handle the function calling similar to the Python SDK.

### Python

 # Light control schemas
 turn_on_the_lights_schema = {'name': 'turn_on_the_lights'}
 turn_off_the_lights_schema = {'name': 'turn_off_the_lights'}

 prompt = """
 Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
 """

 tools = [
 {'code_execution': {}},
 {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]}
 ]

 await run(prompt, tools=tools, modality="AUDIO")

### JavaScript

 // Light control schemas
 const turnOnTheLightsSchema = { name: 'turn_on_the_lights' };
 const turnOffTheLightsSchema = { name: 'turn_off_the_lights' };

 const prompt = `
 Hey, can you write run some python code to turn on the lights, wait 10s and then turn off the lights?
 `;

 const tools = [
 { codeExecution: {} },
 { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] }
 ];

 await run(prompt, tools=tools, modality="AUDIO")

## Function calling modes

The Gemini API lets you control how the model uses the provided tools
(function declarations). Specifically, you can set the mode within
the.`function_calling_config`.

- `AUTO (Default)`: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.
- `ANY`: The model is constrained to always predict a function call and guarantees function schema adherence. If `allowed_function_names` is not specified, the model can choose from any of the provided function declarations. If `allowed_function_names` is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call response to every prompt (if applicable).
- `NONE`: The model is *prohibited* from making function calls. This is
 equivalent to sending a request without any function declarations. Use this to
 temporarily disable function calling without removing your tool definitions.

### Python

 from google.genai import types

 # Configure function calling mode
 tool_config = types.ToolConfig(
 function_calling_config=types.FunctionCallingConfig(
 mode="ANY", allowed_function_names=["get_current_temperature"]
 )
 )

 # Create the generation config
 config = types.GenerateContentConfig(
 tools=[tools], # not defined here.
 tool_config=tool_config,
 )

### JavaScript

 import { FunctionCallingConfigMode } from '@google/genai';

 // Configure function calling mode
 const toolConfig = {
 functionCallingConfig: {
 mode: FunctionCallingConfigMode.ANY,
 allowedFunctionNames: ['get_current_temperature']
 }
 };

 // Create the generation config
 const config = {
 tools: tools, // not defined here.
 toolConfig: toolConfig,
 };

## Automatic function calling (Python only)

When using the Python SDK, you can provide Python functions directly as tools.
The SDK converts these functions into declarations, manages the function call
execution, and handles the response cycle for you. Define your function with
type hints and a docstring. For optimal results, it is recommended to use
[Google-style docstrings.](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods)
The SDK will then automatically:

1. Detect function call responses from the model.
2. Call the corresponding Python function in your code.
3. Send the function's response back to the model.
4. Return the model's final text response.

The SDK currently does not parse argument descriptions into the property
description slots of the generated function declaration. Instead, it sends the
entire docstring as the top-level function description.

### Python

 from google import genai
 from google.genai import types

 # Define the function with type hints and docstring
 def get_current_temperature(location: str) -> dict:
 """Gets the current temperature for a given location.

 Args:
 location: The city and state, e.g. San Francisco, CA

 Returns:
 A dictionary containing the temperature and unit.
 """
 # ... (implementation) ...
 return {"temperature": 25, "unit": "Celsius"}

 # Configure the client
 client = genai.Client()
 config = types.GenerateContentConfig(
 tools=[get_current_temperature]
 ) # Pass the function itself

 # Make the request
 response = client.models.generate_content(
 model="gemini-2.5-flash",
 contents="What's the temperature in Boston?",
 config=config,
 )

 print(response.text) # The SDK handles the function call and returns the final text

You can disable automatic function calling with:

### Python

 config = types.GenerateContentConfig(
 tools=[get_current_temperature],
 automatic_function_calling=types.AutomaticFunctionCallingConfig(disable=True)
 )

### Automatic function schema declaration

The API is able to describe any of the following types. `Pydantic` types are
allowed, as long as the fields defined on them are also composed of allowed
types. Dict types (like `dict[str: int]`) are not well supported here, don't
use them.

### Python

 AllowedType = (
 int | float | bool | str | list['AllowedType'] | pydantic.BaseModel)

To see what the inferred schema looks like, you can convert it using
[`from_callable`](https://googleapis.github.io/python-genai/genai.html#genai.types.FunctionDeclaration.from_callable):

### Python

 from google import genai
 from google.genai import types

 def multiply(a: float, b: float):
 """Returns a * b."""
 return a * b

 client = genai.Client()
 fn_decl = types.FunctionDeclaration.from_callable(callable=multiply, client=client)

 # to_json_dict() provides a clean JSON representation.
 print(fn_decl.to_json_dict())

## Multi-tool use: Combine native tools with function calling

You can enable multiple tools combining native tools with
function calling at the same time. Here's an example that enables two tools,
[Grounding with Google Search](https://ai.google.dev/gemini-api/docs/grounding) and
[code execution](https://ai.google.dev/gemini-api/docs/code-execution), in a request using the
[Live API](https://ai.google.dev/gemini-api/docs/live).
**Note:** Multi-tool use is a-[Live API](https://ai.google.dev/gemini-api/docs/live) only feature at the moment. The `run()` function declaration, which handles the asynchronous websocket setup, is omitted for brevity.

### Python

 # Multiple tasks example - combining lights, code execution, and search
 prompt = """
 Hey, I need you to do three things for me.

 1. Turn on the lights.
 2. Then compute the largest prime palindrome under 100000.
 3. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

 Thanks!
 """

 tools = [
 {'google_search': {}},
 {'code_execution': {}},
 {'function_declarations': [turn_on_the_lights_schema, turn_off_the_lights_schema]} # not defined here.
 ]

 # Execute the prompt with specified tools in audio modality
 await run(prompt, tools=tools, modality="AUDIO")

### JavaScript

 // Multiple tasks example - combining lights, code execution, and search
 const prompt = `
 Hey, I need you to do three things for me.

 1. Turn on the lights.
 2. Then compute the largest prime palindrome under 100000.
 3. Then use Google Search to look up information about the largest earthquake in California the week of Dec 5 2024.

 Thanks!
 `;

 const tools = [
 { googleSearch: {} },
 { codeExecution: {} },
 { functionDeclarations: [turnOnTheLightsSchema, turnOffTheLightsSchema] } // not defined here.
 ];

 // Execute the prompt with specified tools in audio modality
 await run(prompt, {tools: tools, modality: "AUDIO"});

Python developers can try this out in the [Live API Tool Use
notebook](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_LiveAPI_tools.ipynb).

## Model context protocol (MCP)

[Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is
an open standard for connecting AI applications with external tools and data.
MCP provides a common protocol for models to access context, such as functions
(tools), data sources (resources), or predefined prompts.

The Gemini SDKs have built-in support for the MCP, reducing boilerplate code and
offering
[automatic tool calling](https://ai.google.dev/gemini-api/docs/function-calling#automatic_function_calling_python_only)
for MCP tools. When the model generates an MCP tool call, the Python and
JavaScript client SDK can automatically execute the MCP tool and send the
response back to the model in a subsequent request, continuing this loop until
no more tool calls are made by the model.

Here, you can find an example of how to use a local MCP server with Gemini and
`mcp` SDK.

### Python

Make sure the latest version of the
[`mcp` SDK](https://modelcontextprotocol.io/introduction) is installed on
your platform of choice.

 pip install mcp

**Note:** Python supports automatic tool calling by passing in the `ClientSession` into the `tools` parameters. If you want to disable it, you can provide `automatic_function_calling` with disabled `True`.

 import os
 import asyncio
 from datetime import datetime
 from mcp import ClientSession, StdioServerParameters
 from mcp.client.stdio import stdio_client
 from google import genai

 client = genai.Client()

 # Create server parameters for stdio connection
 server_params = StdioServerParameters(
 command="npx", # Executable
 args=["-y", "@philschmid/weather-mcp"], # MCP Server
 env=None, # Optional environment variables
 )

 async def run():
 async with stdio_client(server_params) as (read, write):
 async with ClientSession(read, write) as session:
 # Prompt to get the weather for the current day in London.
 prompt = f"What is the weather in London in {datetime.now().strftime('%Y-%m-%d')}?"

 # Initialize the connection between client and server
 await session.initialize()

 # Send request to the model with MCP function declarations
 response = await client.aio.models.generate_content(
 model="gemini-2.5-flash",
 contents=prompt,
 config=genai.types.GenerateContentConfig(
 temperature=0,
 tools=[session], # uses the session, will automatically call the tool
 # Uncomment if you **don't** want the SDK to automatically call the tool
 # automatic_function_calling=genai.types.AutomaticFunctionCallingConfig(
 # disable=True
 # ),
 ),
 )
 print(response.text)

 # Start the asyncio event loop and run the main function
 asyncio.run(run())

### JavaScript

Make sure the latest version of the `mcp` SDK is installed on your platform
of choice.

 npm install @modelcontextprotocol/sdk

**Note:** JavaScript supports automatic tool calling by wrapping the `client` with `mcpToTool`. If you want to disable it, you can provide `automaticFunctionCalling` with disabled `true`.

 import { GoogleGenAI, FunctionCallingConfigMode , mcpToTool} from '@google/genai';
 import { Client } from "@modelcontextprotocol/sdk/client/index.js";
 import { StdioClientTransport } from "@modelcontextprotocol/sdk/client/stdio.js";

 // Create server parameters for stdio connection
 const serverParams = new StdioClientTransport({
 command: "npx", // Executable
 args: ["-y", "@philschmid/weather-mcp"] // MCP Server
 });

 const client = new Client(
 {
 name: "example-client",
 version: "1.0.0"
 }
 );

 // Configure the client
 const ai = new GoogleGenAI({});

 // Initialize the connection between client and server
 await client.connect(serverParams);

 // Send request to the model with MCP tools
 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: `What is the weather in London in ${new Date().toLocaleDateString()}?`,
 config: {
 tools: [mcpToTool(client)], // uses the session, will automatically call the tool
 // Uncomment if you **don't** want the sdk to automatically call the tool
 // automaticFunctionCalling: {
 // disable: true,
 // },
 },
 });
 console.log(response.text)

 // Close the connection
 await client.close();

### Limitations with built-in MCP support

Built-in MCP support is a [experimental](https://ai.google.dev/gemini-api/docs/models#preview)
feature in our SDKs and has the following limitations:

- Only tools are supported, not resources nor prompts
- It is available for the Python and JavaScript/TypeScript SDK.
- Breaking changes might occur in future releases.

Manual integration of MCP servers is always an option if these limit what you're
building.

## Supported models

This section lists models and their function calling capabilities. Experimental
models are not included. You can find a comprehensive capabilities overview on
the [model overview](https://ai.google.dev/gemini-api/docs/models) page.

| Model | Function Calling | Parallel Function Calling | Compositional Function Calling |
|-----------------------|------------------|---------------------------|--------------------------------|
| Gemini 2.5 Pro |  |  |  |
| Gemini 2.5 Flash |  |  |  |
| Gemini 2.5 Flash-Lite |  |  |  |
| Gemini 2.0 Flash |  |  |  |
| Gemini 2.0 Flash-Lite | X | X | X |

## Best practices

- **Function and Parameter Descriptions:** Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.
- **Naming:** Use descriptive function names (without spaces, periods, or dashes).
- **Strong Typing:** Use specific types (integer, string, enum) for parameters to reduce errors. If a parameter has a limited set of valid values, use an enum.
- **Tool Selection:** While the model can use an arbitrary number of tools, providing too many can increase the risk of selecting an incorrect or suboptimal tool. For best results, aim to provide only the relevant tools for the context or task, ideally keeping the active set to a maximum of 10-20. Consider dynamic tool selection based on conversation context if you have a large total number of tools.
- **Prompt Engineering:**
 - Provide context: Tell the model its role (e.g., "You are a helpful weather assistant.").
 - Give instructions: Specify how and when to use functions (e.g., "Don't guess dates; always use a future date for forecasts.").
 - Encourage clarification: Instruct the model to ask clarifying questions if needed.
- **Temperature:** Use a low temperature (e.g., 0) for more deterministic and reliable function calls.
- **Validation:** If a function call has significant consequences (e.g., placing an order), validate the call with the user before executing it.
- **Error Handling**: Implement robust error handling in your functions to gracefully handle unexpected inputs or API failures. Return informative error messages that the model can use to generate helpful responses to the user.
- **Security:** Be mindful of security when calling external APIs. Use appropriate authentication and authorization mechanisms. Avoid exposing sensitive data in function calls.
- **Token Limits:** Function descriptions and parameters count towards your input token limit. If you're hitting token limits, consider limiting the number of functions or the length of the descriptions, break down complex tasks into smaller, more focused function sets.

## Notes and limitations

- Only a [subset of the OpenAPI
 schema](https://ai.google.dev/api/caching#FunctionDeclaration) is supported.
- Supported parameter types in Python are limited.
- Automatic function calling is a Python SDK feature only.

[END OF DOCUMENT: GEMINI5WBTXZ7SI]
---

