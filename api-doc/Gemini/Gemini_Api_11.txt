
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: GEMINI2NHRMHAKWD (sha256-f5040e0d414d3d58d9d710a3dc82135222f93b90a47b1348631d8af6bc1596ef) | Title: Vercel-Ai-Sdk-Example.Md]
[DocID: GEMINI2IYET955G2 (sha256-e95f884047d2364150a06bf0f3e6e379178d8a60cb394b0f01ea5e6eec1dfe5e) | Title: Video-Understanding.Md]
[DocID: GEMINI8YOUZTXZE (sha256-16ffad2b8c4aabd309e6ed84119cfe48d6d64c3bb4bbb637137acfb539f1c8a5) | Title: Video.Md]
--- END OF TOC ---

[START OF DOCUMENT: GEMINI2NHRMHAKWD | Title: Vercel-Ai-Sdk-Example.Md]

The [AI SDK by Vercel](https://ai-sdk.dev) is a powerful open-source library for
building AI-powered applications, user interfaces, and agents in TypeScript.

This guide will walk you through building a Node.js application with TypeScript
that uses the AI SDK to connect with the Gemini API via the [Google Generative AI Provider](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai) and perform automated market trend analysis. The final
application will:

1. Use Gemini with Google Search to research current market trends.
2. Extract structured data from the research to generate charts.
3. Combine the research and charts into a professional HTML report and save it as a PDF.

## Prerequisites

To complete this guide, you'll need:

- A Gemini API key. You can create one for free in [Google AI Studio](https://aistudio.google.com/apikey).
- [Node.js](https://nodejs.org/en/download) version 18 or later.
- A package manager, such as `npm`, `pnpm`, or `yarn`.

| **Note:** While this guide focuses on a Node.js environment, the AI SDK is also fully compatible with browser-based frameworks like [Next.js](https://nextjs.org/).

## Set up your application

First, create a new directory for your project and initialize it.

### npm

 mkdir market-trend-app
 cd market-trend-app
 npm init -y

### pnpm

 mkdir market-trend-app
 cd market-trend-app
 pnpm init

### yarn

 mkdir market-trend-app
 cd market-trend-app
 yarn init -y

### Install dependencies

Next, install the AI SDK, the Google Generative AI provider, and other
necessary dependencies.

### npm

 npm install ai @ai-sdk/google zod
 npm install -D @types/node tsx typescript && npx tsc --init

To prevent a TypeScript compiler error, comment out the following line in
the generated `tsconfig.json`:

 //"verbatimModuleSyntax": true,

### pnpm

 pnpm add ai @ai-sdk/google zod
 pnpm add -D @types/node tsx typescript

### yarn

 yarn add ai @ai-sdk/google zod
 yarn add -D @types/node tsx typescript && yarn tsc --init

To prevent a TypeScript compiler error, comment out the following line in
the generated `tsconfig.json`:

 //"verbatimModuleSyntax": true,

This application will also use the third-party packages [Puppeteer](https://pptr.dev/)
and [Chart.js](https://www.chartjs.org) for rendering charts and
creating a PDF:

### npm

 npm install puppeteer chart.js
 npm install -D @types/chart.js

### pnpm

 pnpm add puppeteer chart.js
 pnpm add -D @types/chart.js

### yarn

 yarn add puppeteer chart.js
 yarn add -D @types/chart.js

The `puppeteer` package requires running a script to download the Chromium
browser. Your package manager may ask for approval, so ensure you approve the
script when prompted.

### Configure your API key

Set the `GOOGLE_GENERATIVE_AI_API_KEY` environment variable with your Gemini API
key. The Google Generative AI Provider automatically looks for your API key in
this environment variable.

### MacOS/Linux

 export GOOGLE_GENERATIVE_AI_API_KEY="YOUR_API_KEY_HERE"

### Powershell

 setx GOOGLE_GENERATIVE_AI_API_KEY "YOUR_API_KEY_HERE"

## Create your application

Now, let's create the main file for our application. Create a new file named
`main.ts` in your project directory. You'll build up the logic in this file
step-by-step.

For a quick test to ensure everything is set up correctly, add the following
code to `main.ts`. This basic example uses Gemini 2.5 Flash and `generateText`
to get a simple response from Gemini.

 import { google } from "@ai-sdk/google";
 import { generateText } from "ai";

 async function main() {
 const { text } = await generateText({
 model: google("gemini-2.5-flash"),
 prompt: 'What is plant-based milk?',
 });

 console.log(text);
 }

 main().catch(console.error);

Before adding more complexity, let's run this script to verify that your
environment is configured correctly. Run the following command in your terminal:

### npm

 npx tsc && node main.js

### pnpm

 pnpm tsx main.ts

### yarn

 yarn tsc && node main.js

If everything is set up correctly, you'll see Gemini's response printed to the
console.

## Perform market research with Google Search

To get up-to-date information, you can enable the
[Google Search](https://ai.google.dev/gemini-api/docs/google-search) tool for Gemini. When this tool
is active, the model can search the web to answer the prompt and will return
the sources it used.

Replace the content of `main.ts` with the following code to perform the first
step of our analysis.

 import { google } from "@ai-sdk/google";
 import { generateText } from "ai";

 async function main() {
 // Step 1: Search market trends
 const { text: marketTrends, sources } = await generateText({
 model: google("gemini-2.5-flash"),
 tools: {
 google_search: google.tools.googleSearch({}),
 },
 prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
 I need to know the market size, key players and their market share, and primary consumer drivers.
 `,
 });

 console.log("Market trends found:\n", marketTrends);
 // To see the sources, uncomment the following line:
 // console.log("Sources:\n", sources);
 }

 main().catch(console.error);

## Extract chart data

Next, let's process the research text to extract structured data suitable for
charts. Use the AI SDK's `generateObject` function along with a `zod`
schema to define the exact data structure.

Also create a helper function to convert this structured data into a
configuration that `Chart.js` can understand.

Add the following code to `main.ts`. Note the new imports and the added "Step 2".

 import { google } from "@ai-sdk/google";
 import { generateText, generateObject } from "ai";
 import { z } from "zod/v4";
 import { ChartConfiguration } from "chart.js";

 // Helper function to create Chart.js configurations
 function createChartConfig({labels, data, label, type, colors,}: {
 labels: string[];
 data: number[];
 label: string;
 type: "bar" | "line";
 colors: string[];
 }): ChartConfiguration {
 return {
 type: type,
 data: {
 labels: labels,
 datasets: [
 {
 label: label,
 data: data,
 borderWidth: 1,
 ...(type === "bar" && { backgroundColor: colors }),
 ...(type === "line" && colors.length > 0 && { borderColor: colors[0] }),
 },
 ],
 },
 options: {
 animation: { duration: 0 }, // Disable animations for static PDF rendering
 },
 };
 }

 async function main() {
 // Step 1: Search market trends
 const { text: marketTrends, sources } = await generateText({
 model: google("gemini-2.5-flash"),
 tools: {
 google_search: google.tools.googleSearch({}),
 },
 prompt: `Search the web for market trends for plant-based milk in North America for 2024-2025.
 I need to know the market size, key players and their market share, and primary consumer drivers.
 `,
 });

 console.log("Market trends found.");

 // Step 2: Extract chart data
 const { object: chartData } = await generateObject({
 model: google("gemini-2.5-flash"),
 schema: z.object({
 chartConfigurations: z
 .array(
 z.object({
 type: z.enum(["bar", "line"]).describe('The type of chart to generate. Either "bar" or "line"',),
 labels: z.array(z.string()).describe("A list of chart labels"),
 data: z.array(z.number()).describe("A list of the chart data"),
 label: z.string().describe("A label for the chart"),
 colors: z.array(z.string()).describe('A list of colors to use for the chart, e.g. "rgba(255, 99, 132, 0.8)"',),
 }),
 )
 .describe("A list of chart configurations"),
 }),
 prompt: `Given the following market trends text, come up with a list of 1-3 meaningful bar or line charts
 and generate chart data.

 Market Trends:
 ${marketTrends}
 `,
 });

 const chartConfigs = chartData.chartConfigurations.map(createChartConfig);

 console.log("Chart configurations generated.");
 }

 main().catch(console.error);

## Generate the final report

In the final step, instruct Gemini to act as an expert report writer.
Provide it with the market research, the chart configurations, and a clear
set of instructions for building an HTML report. Then, use
[Puppeteer](https://pptr.dev/) to render this HTML and save it as a PDF.

Add the final `puppeteer` import and "Step 3" to your `main.ts` file.

 // ... (imports from previous step)
 import puppeteer from "puppeteer";

 // ... (createChartConfig helper function from previous step)

 async function main() {
 // ... (Step 1 and 2 from previous step)

 // Step 3: Generate the final HTML report and save it as a PDF
 const { text: htmlReport } = await generateText({
 model: google("gemini-2.5-flash"),
 prompt: `You are an expert financial analyst and report writer.
 Your task is to generate a comprehensive market analysis report in HTML format.

 **Instructions:**
 1. Write a full HTML document.
 2. Use the provided "Market Trends" text to write the main body of the report. Structure it with clear headings and paragraphs.
 3. Incorporate the provided "Chart Configurations" to visualize the data. For each chart, you MUST create a unique <canvas> element and a corresponding <script> block to render it using Chart.js.
 4. Reference the "Sources" at the end of the report.
 5. Do not include any placeholder data; use only the information provided.
 6. Return only the raw HTML code.

 **Chart Rendering Snippet:**
 Include this script in the head of the HTML: <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
 For each chart, use a structure like below, ensuring the canvas 'id' is unique for each chart, and apply the correspinding config:

 ---
 <div style="width: 800px; height: 600px;">
 <canvas id="chart1"></canvas>
 </div>
 <script>
 new Chart(document.getElementById('chart1'), config);
 </script>
 ---
 (For the second chart, use 'chart2' and the corresponding config, and so on.)

 **Data:**
 - Market Trends: ${marketTrends}
 - Chart Configurations: ${JSON.stringify(chartConfigs)}
 - Sources: ${JSON.stringify(sources)}
 `,
 });

 // LLMs may wrap the HTML in a markdown code block, so strip it.
 const finalHtml = htmlReport.replace(/^```html\n/, "").replace(/\n```$/, "");

 const browser = await puppeteer.launch();
 const page = await browser.newPage();
 await page.setContent(finalHtml);
 await page.pdf({ path: "report.pdf", format: "A4" });
 await browser.close();

 console.log("\nReport generated successfully: report.pdf");
 }

 main().catch(console.error);

## Run your application

You are now ready to run the application. Execute the following command in
your terminal:

### npm

 npx tsc && node main.js

### pnpm

 pnpm tsx main.ts

### yarn

 yarn tsc && node main.js

You will see logging in your terminal as the script executes each step.
Once complete, a `report.pdf` file containing your market analysis will be
created in your project directory.

Below, you'll see the first two pages of an example PDF report:

![Market analysis report](https://ai.google.dev/static/gemini-api/docs/images/market-research-pdf.jpg)

## Further resources

For more information about building with Gemini and the AI SDK,
explore these resources:

- [AI SDK docs](https://ai-sdk.dev/docs)
- [AI SDK Google Generative AI docs](https://ai-sdk.dev/providers/ai-sdk-providers/google-generative-ai)
- [AI SDK cookbook: Get Started with Gemini 2.5](https://ai-sdk.dev/cookbook/guides/gemini-2-5)

[END OF DOCUMENT: GEMINI2NHRMHAKWD]
---

[START OF DOCUMENT: GEMINI2IYET955G2 | Title: Video-Understanding.Md]

Gemini models can process videos, enabling many frontier developer use cases
that would have historically required domain specific models.
Some of Gemini's vision capabilities include the ability to:

- Describe, segment, and extract information from videos
- Answer questions about video content
- Refer to specific timestamps within a video

Gemini was built to be multimodal from the ground up and we continue to push the
frontier of what is possible. This guide shows how to use the Gemini API to
generate text responses based on video inputs.

## Video input

You can provide videos as input to Gemini in the following ways:

- [Upload a video file](https://ai.google.dev/gemini-api/docs/video-understanding#upload-video) using the File API before making a request to `generateContent`. Use this method for files larger than 20MB, videos longer than approximately 1 minute, or when you want to reuse the file across multiple requests.
- [Pass inline video data](https://ai.google.dev/gemini-api/docs/video-understanding#inline-video) with the request to `generateContent`. Use this method for smaller files (\<20MB) and shorter durations.
- [Pass YouTube URLs](https://ai.google.dev/gemini-api/docs/video-understanding#youtube) as part of your `generateContent` request.

### Upload a video file

You can use the [Files API](https://ai.google.dev/gemini-api/docs/files) to upload a video file.
Always use the Files API when the total request size (including the file, text
prompt, system instructions, etc.) is larger than 20 MB, the video duration is
significant, or if you intend to use the same video in multiple prompts.
The File API accepts video file formats directly.

The following code downloads the sample video, uploads it using the File API,
waits for it to be processed, and then uses the file reference in
a `generateContent` request.

### Python

 from google import genai

 client = genai.Client()

 myfile = client.files.upload(file="path/to/sample.mp4")

 response = client.models.generate_content(
 model="gemini-2.5-flash", contents=[myfile, "Summarize this video. Then create a quiz with an answer key based on the information in this video."]
 )

 print(response.text)

### JavaScript

 import {
 GoogleGenAI,
 createUserContent,
 createPartFromUri,
 } from "@google/genai";

 const ai = new GoogleGenAI({});

 async function main() {
 const myfile = await ai.files.upload({
 file: "path/to/sample.mp4",
 config: { mimeType: "video/mp4" },
 });

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: createUserContent([
 createPartFromUri(myfile.uri, myfile.mimeType),
 "Summarize this video. Then create a quiz with an answer key based on the information in this video.",
 ]),
 });
 console.log(response.text);
 }

 await main();

### Go

 uploadedFile, _ := client.Files.UploadFromPath(ctx, "path/to/sample.mp4", nil)

 parts := []*genai.Part{
 genai.NewPartFromText("Summarize this video. Then create a quiz with an answer key based on the information in this video."),
 genai.NewPartFromURI(uploadedFile.URI, uploadedFile.MIMEType),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())

### REST

 VIDEO_PATH="path/to/sample.mp4"
 MIME_TYPE=$(file -b --mime-type "${VIDEO_PATH}")
 NUM_BYTES=$(wc -c < "${VIDEO_PATH}")
 DISPLAY_NAME=VIDEO

 tmp_header_file=upload-header.tmp

 echo "Starting file upload..."
 curl "https://generativelanguage.googleapis.com/upload/v1beta/files" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -D ${tmp_header_file} \
 -H "X-Goog-Upload-Protocol: resumable" \
 -H "X-Goog-Upload-Command: start" \
 -H "X-Goog-Upload-Header-Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Header-Content-Type: ${MIME_TYPE}" \
 -H "Content-Type: application/json" \
 -d "{'file': {'display_name': '${DISPLAY_NAME}'}}" 2> /dev/null

 upload_url=$(grep -i "x-goog-upload-url: " "${tmp_header_file}" | cut -d" " -f2 | tr -d "\r")
 rm "${tmp_header_file}"

 echo "Uploading video data..."
 curl "${upload_url}" \
 -H "Content-Length: ${NUM_BYTES}" \
 -H "X-Goog-Upload-Offset: 0" \
 -H "X-Goog-Upload-Command: upload, finalize" \
 --data-binary "@${VIDEO_PATH}" 2> /dev/null > file_info.json

 file_uri=$(jq -r ".file.uri" file_info.json)
 echo file_uri=$file_uri

 echo "File uploaded successfully. File URI: ${file_uri}"

 # --- 3. Generate content using the uploaded video file ---
 echo "Generating content from video..."
 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {"file_data":{"mime_type": "'"${MIME_TYPE}"'", "file_uri": "'"${file_uri}"'"}},
 {"text": "Summarize this video. Then create a quiz with an answer key based on the information in this video."}]
 }]
 }' 2> /dev/null > response.json

 jq -r ".candidates[].content.parts[].text" response.json

To learn more about working with media files, see
[Files API](https://ai.google.dev/gemini-api/docs/files).

### Pass video data inline

Instead of uploading a video file using the File API, you can pass smaller
videos directly in the request to `generateContent`. This is suitable for
shorter videos under 20MB total request size.

Here's an example of providing inline video data:

### Python

 from google import genai
 from google.genai import types

 # Only for videos of size <20Mb
 video_file_name = "/path/to/your/video.mp4"
 video_bytes = open(video_file_name, 'rb').read()

 client = genai.Client()
 response = client.models.generate_content(
 model='models/gemini-2.5-flash',
 contents=types.Content(
 parts=[
 types.Part(
 inline_data=types.Blob(data=video_bytes, mime_type='video/mp4')
 ),
 types.Part(text='Please summarize the video in 3 sentences.')
 ]
 )
 )

### JavaScript

 import { GoogleGenAI } from "@google/genai";
 import * as fs from "node:fs";

 const ai = new GoogleGenAI({});
 const base64VideoFile = fs.readFileSync("path/to/small-sample.mp4", {
 encoding: "base64",
 });

 const contents = [
 {
 inlineData: {
 mimeType: "video/mp4",
 data: base64VideoFile,
 },
 },
 { text: "Please summarize the video in 3 sentences." }
 ];

 const response = await ai.models.generateContent({
 model: "gemini-2.5-flash",
 contents: contents,
 });
 console.log(response.text);

### REST

**Note:** If you get an `Argument list too long` error, the base64 encoding of your file might be too long for the curl command line. Use the File API method instead for larger files.

 VIDEO_PATH=/path/to/your/video.mp4

 if [[ "$(base64 --version 2>&1)" = *"FreeBSD"* ]]; then
 B64FLAGS="--input"
 else
 B64FLAGS="-w0"
 fi

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {
 "inline_data": {
 "mime_type":"video/mp4",
 "data": "'$(base64 $B64FLAGS $VIDEO_PATH)'"
 }
 },
 {"text": "Please summarize the video in 3 sentences."}
 ]
 }]
 }' 2> /dev/null

### Pass YouTube URLs

| **Preview:** The YouTube URL feature is in preview and is available at no charge. Pricing and rate limits are likely to change.

You can pass YouTube URLs directly to Gemini API as part of your `generateContent`request as follows:

### Python

 response = client.models.generate_content(
 model='models/gemini-2.5-flash',
 contents=types.Content(
 parts=[
 types.Part(
 file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=9hE5-98ZeCg')
 ),
 types.Part(text='Please summarize the video in 3 sentences.')
 ]
 )
 )

### JavaScript

 import { GoogleGenerativeAI } from "@google/generative-ai";

 const genAI = new GoogleGenerativeAI(process.env.GOOGLE_API_KEY);
 const model = genAI.getGenerativeModel({ model: "gemini-2.5-flash" });
 const result = await model.generateContent([
 "Please summarize the video in 3 sentences.",
 {
 fileData: {
 fileUri: "https://www.youtube.com/watch?v=9hE5-98ZeCg",
 },
 },
 ]);
 console.log(result.response.text());

### Go

 package main

 import (
 "context"
 "fmt"
 "os"
 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 parts := []*genai.Part{
 genai.NewPartFromText("Please summarize the video in 3 sentences."),
 genai.NewPartFromURI("https://www.youtube.com/watch?v=9hE5-98ZeCg","video/mp4"),
 }

 contents := []*genai.Content{
 genai.NewContentFromParts(parts, genai.RoleUser),
 }

 result, _ := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash",
 contents,
 nil,
 )

 fmt.Println(result.Text())
 }

### REST

 curl "https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H 'Content-Type: application/json' \
 -X POST \
 -d '{
 "contents": [{
 "parts":[
 {"text": "Please summarize the video in 3 sentences."},
 {
 "file_data": {
 "file_uri": "https://www.youtube.com/watch?v=9hE5-98ZeCg"
 }
 }
 ]
 }]
 }' 2> /dev/null

**Limitations:**

- For the free tier, you can't upload more than 8 hours of YouTube video per day.
- For the paid tier, there is no limit based on video length.
- For models prior to Gemini 2.5, you can upload only 1 video per request. For Gemini 2.5 and later models, you can upload a maximum of 10 videos per request.
- You can only upload public videos (not private or unlisted videos).

## Refer to timestamps in the content

You can ask questions about specific points in time within the video using
timestamps of the form `MM:SS`.

### Python

 prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?" # Adjusted timestamps for the NASA video

### JavaScript

 const prompt = "What are the examples given at 00:05 and 00:10 supposed to show us?";

### Go

 prompt := []*genai.Part{
 genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
 // Adjusted timestamps for the NASA video
 genai.NewPartFromText("What are the examples given at 00:05 and " +
 "00:10 supposed to show us?"),
 }

### REST

 PROMPT="What are the examples given at 00:05 and 00:10 supposed to show us?"

## Transcribe video and provide visual descriptions

The Gemini models can transcribe and provide visual descriptions of video
content by processing both the audio track and visual frames. For visual
descriptions, the model samples the video at a rate of **1 frame per second**.
This sampling rate may affect the level of detail in the descriptions,
particularly for videos with rapidly changing visuals.

### Python

 prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions."

### JavaScript

 const prompt = "Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions.";

### Go

 prompt := []*genai.Part{
 genai.NewPartFromURI(currentVideoFile.URI, currentVideoFile.MIMEType),
 genai.NewPartFromText("Transcribe the audio from this video, giving timestamps for salient events in the video. Also " +
 "provide visual descriptions."),
 }

### REST

 PROMPT="Transcribe the audio from this video, giving timestamps for salient events in the video. Also provide visual descriptions."

## Customize video processing

You can customize video processing in the Gemini API by setting clipping
intervals or providing custom frame rate sampling.
| **Tip:** Video clipping and frames per second (FPS) are supported by all models, but the quality is significantly higher from 2.5 series models.

### Set clipping intervals

You can clip video by specifying `videoMetadata` with start and end offsets.

### Python

 from google import genai
 from google.genai import types

 client = genai.Client()
 response = client.models.generate_content(
 model='models/gemini-2.5-flash',
 contents=types.Content(
 parts=[
 types.Part(
 file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=XEzRZ35urlk'),
 video_metadata=types.VideoMetadata(
 start_offset='1250s',
 end_offset='1570s'
 )
 ),
 types.Part(text='Please summarize the video in 3 sentences.')
 ]
 )
 )

### JavaScript

 import { GoogleGenAI } from '@google/genai';
 const ai = new GoogleGenAI({});
 const model = 'gemini-2.5-flash';

 async function main() {
 const contents = [
 {
 role: 'user',
 parts: [
 {
 fileData: {
 fileUri: 'https://www.youtube.com/watch?v=9hE5-98ZeCg',
 mimeType: 'video/*',
 },
 videoMetadata: {
 startOffset: '40s',
 endOffset: '80s',
 }
 },
 {
 text: 'Please summarize the video in 3 sentences.',
 },
 ],
 },
 ];

 const response = await ai.models.generateContent({
 model,
 contents,
 });

 console.log(response.text)

 }

 await main();

### Set a custom frame rate

You can set custom frame rate sampling by passing an `fps` argument to
`videoMetadata`.
**Note:** Due to built-in per image based safety checks, the same video may get blocked at some fps and not at others due to different extracted frames.

### Python

 from google import genai
 from google.genai import types

 # Only for videos of size <20Mb
 video_file_name = "/path/to/your/video.mp4"
 video_bytes = open(video_file_name, 'rb').read()

 client = genai.Client()
 response = client.models.generate_content(
 model='models/gemini-2.5-flash',
 contents=types.Content(
 parts=[
 types.Part(
 inline_data=types.Blob(
 data=video_bytes,
 mime_type='video/mp4'),
 video_metadata=types.VideoMetadata(fps=5)
 ),
 types.Part(text='Please summarize the video in 3 sentences.')
 ]
 )
 )

By default 1 frame per second (FPS) is sampled from the video. You might want to
set low FPS (\< 1) for long videos. This is especially useful for mostly static
videos (e.g. lectures). If you want to capture more details in rapidly changing
visuals, consider setting a higher FPS value.

## Supported video formats

Gemini supports the following video format MIME types:

- `video/mp4`
- `video/mpeg`
- `video/mov`
- `video/avi`
- `video/x-flv`
- `video/mpg`
- `video/webm`
- `video/wmv`
- `video/3gpp`

## Technical details about videos

- **Supported models \& context** : All Gemini 2.0 and 2.5 models can process video data.
 - Models with a 2M context window can process videos up to 2 hours long at default media resolution or 6 hours long at low media resolution, while models with a 1M context window can process videos up to 1 hour long at default media resolution or 3 hours long at low media resolution.
- **File API processing** : When using the File API, videos are stored at 1 frame per second (FPS) and audio is processed at 1Kbps (single channel). Timestamps are added every second.
 - These rates are subject to change in the future for improvements in inference.
 - You can override the 1 FPS sampling rate by [setting a custom frame rate](https://ai.google.dev/gemini-api/docs/video-understanding#custom-frame-rate).
- **Token calculation** : Each second of video is tokenized as follows:
 - Individual frames (sampled at 1 FPS):
 - If [`mediaResolution`](https://ai.google.dev/api/generate-content#MediaResolution) is set to low, frames are tokenized at 66 tokens per frame.
 - Otherwise, frames are tokenized at 258 tokens per frame.
 - Audio: 32 tokens per second.
 - Metadata is also included.
 - Total: Approximately 300 tokens per second of video at default media resolution, or 100 tokens per second of video at low media resolution.
- **Timestamp format** : When referring to specific moments in a video within your prompt, use the `MM:SS` format (e.g., `01:15` for 1 minute and 15 seconds).
- **Best practices** :
 - Use only one video per prompt request for optimal results.
 - If combining text and a single video, place the text prompt *after* the video part in the `contents` array.
 - Be aware that fast action sequences might lose detail due to the 1 FPS sampling rate. Consider slowing down such clips if necessary.

## What's next

This guide shows how to upload video files and generate text outputs from video
inputs. To learn more, see the following resources:

- [System instructions](https://ai.google.dev/gemini-api/docs/text-generation#system-instructions): System instructions let you steer the behavior of the model based on your specific needs and use cases.
- [Files API](https://ai.google.dev/gemini-api/docs/files): Learn more about uploading and managing files for use with Gemini.
- [File prompting strategies](https://ai.google.dev/gemini-api/docs/files#prompt-guide): The Gemini API supports prompting with text, image, audio, and video data, also known as multimodal prompting.
- [Safety guidance](https://ai.google.dev/gemini-api/docs/safety-guidance): Sometimes generative AI models produce unexpected outputs, such as outputs that are inaccurate, biased, or offensive. Post-processing and human evaluation are essential to limit the risk of harm from such outputs.

[END OF DOCUMENT: GEMINI2IYET955G2]
---

[START OF DOCUMENT: GEMINI8YOUZTXZE | Title: Video.Md]

<br />

<br />

[Veo 3.1](https://deepmind.google/models/veo/)is Google's state-of-the-art model for generating high-fidelity, 8-second 720p or 1080p videos featuring stunning realism and natively generated audio. You can access this model programmatically using the Gemini API. To learn more about the available Veo model variants, see the[Model Versions](https://ai.google.dev/gemini-api/docs/video#model-versions)section.

Veo 3.1 excels at a wide range of visual and cinematic styles and introduces several new capabilities:

- **Video extension**: Extend videos that were previously generated using Veo.
- **Frame-specific generation**: Generate a video by specifying the first and last frames.
- **Image-based direction**: Use up to three reference images to guide the content of your generated video.

For more information about writing effective text prompts for video generation, see the[Veo prompt guide](https://ai.google.dev/gemini-api/docs/video#prompt-guide)

## Text to video generation

Choose an example to see how to generate a video with dialogue, cinematic realism, or creative animation:

Dialogue \& Sound EffectsCinematic RealismCreative Animation

### Python

 import time
 from google import genai
 from google.genai import types

 client = genai.Client()

 prompt = """A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
 A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'"""

 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 prompt=prompt,
 )

 # Poll the operation status until the video is ready.
 while not operation.done:
 print("Waiting for video generation to complete...")
 time.sleep(10)
 operation = client.operations.get(operation)

 # Download the generated video.
 generated_video = operation.response.generated_videos[0]
 client.files.download(file=generated_video.video)
 generated_video.video.save("dialogue_example.mp4")
 print("Generated video saved to dialogue_example.mp4")

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const ai = new GoogleGenAI({});

 const prompt = `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
 A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`;

 let operation = await ai.models.generateVideos({
 model: "veo-3.1-generate-preview",
 prompt: prompt,
 });

 // Poll the operation status until the video is ready.
 while (!operation.done) {
 console.log("Waiting for video generation to complete...")
 await new Promise((resolve) => setTimeout(resolve, 10000));
 operation = await ai.operations.getVideosOperation({
 operation: operation,
 });
 }

 // Download the generated video.
 ai.files.download({
 file: operation.response.generatedVideos[0].video,
 downloadPath: "dialogue_example.mp4",
 });
 console.log(`Generated video saved to dialogue_example.mp4`);

### Go

 package main

 import (
 "context"
 "log"
 "os"
 "time"

 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 prompt := `A close up of two people staring at a cryptic drawing on a wall, torchlight flickering.
 A man murmurs, 'This must be it. That's the secret code.' The woman looks at him and whispering excitedly, 'What did you find?'`

 operation, _ := client.Models.GenerateVideos(
 ctx,
 "veo-3.1-generate-preview",
 prompt,
 nil,
 nil,
 )

 // Poll the operation status until the video is ready.
 for !operation.Done {
 log.Println("Waiting for video generation to complete...")
 time.Sleep(10 * time.Second)
 operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
 }

 // Download the generated video.
 video := operation.Response.GeneratedVideos[0]
 client.Files.Download(ctx, video.Video, nil)
 fname := "dialogue_example.mp4"
 _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
 log.Printf("Generated video saved to %s\n", fname)
 }

### REST

 # Note: This script uses jq to parse the JSON response.
 # GEMINI API Base URL
 BASE_URL="https://generativelanguage.googleapis.com/v1beta"

 # Send request to generate video and capture the operation name into a variable.
 operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -X "POST" \
 -d '{
 "instances": [{
 "prompt": "A close up of two people staring at a cryptic drawing on a wall, torchlight flickering. A man murmurs, \"This must be it. That'\''s the secret code.\" The woman looks at him and whispering excitedly, \"What did you find?\""
 }
 ]
 }' | jq -r .name)

 # Poll the operation status until the video is ready
 while true; do
 # Get the full JSON status and store it in a variable.
 status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

 # Check the "done" field from the JSON stored in the variable.
 is_done=$(echo "${status_response}" | jq .done)

 if [ "${is_done}" = "true" ]; then
 # Extract the download URI from the final response.
 video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
 echo "Downloading video from: ${video_uri}"

 # Download the video using the URI and API key and follow redirects.
 curl -L -o dialogue_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
 break
 fi
 # Wait for 5 seconds before checking again.
 sleep 10
 done

## Image to video generation

The following code demonstrates generating an image using[Gemini 2.5 Flash Image aka Nano Banana](https://ai.google.dev/gemini-api/docs/image-generation), then using that image as the starting frame for generating a video with Veo 3.1.

### Python

 import time
 from google import genai

 client = genai.Client()

 prompt = "Panning wide shot of a calico kitten sleeping in the sunshine"

 # Step 1: Generate an image with Nano Banana.
 image = client.models.generate_content(
 model="gemini-2.5-flash-image",
 contents=prompt,
 config={"response_modalities":['IMAGE']}
 )

 # Step 2: Generate video with Veo 3.1 using the image.
 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 prompt=prompt,
 image=image.parts[0].as_image(),
 )

 # Poll the operation status until the video is ready.
 while not operation.done:
 print("Waiting for video generation to complete...")
 time.sleep(10)
 operation = client.operations.get(operation)

 # Download the video.
 video = operation.response.generated_videos[0]
 client.files.download(file=video.video)
 video.video.save("veo3_with_image_input.mp4")
 print("Generated video saved to veo3_with_image_input.mp4")

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const ai = new GoogleGenAI({});

 const prompt = "Panning wide shot of a calico kitten sleeping in the sunshine";

 // Step 1: Generate an image with Nano Banana.
 const imageResponse = await ai.models.generateContent({
 model: "gemini-2.5-flash-image",
 prompt: prompt,
 });

 // Step 2: Generate video with Veo 3.1 using the image.
 let operation = await ai.models.generateVideos({
 model: "veo-3.1-generate-preview",
 prompt: prompt,
 image: {
 imageBytes: imageResponse.generatedImages[0].image.imageBytes,
 mimeType: "image/png",
 },
 });

 // Poll the operation status until the video is ready.
 while (!operation.done) {
 console.log("Waiting for video generation to complete...")
 await new Promise((resolve) => setTimeout(resolve, 10000));
 operation = await ai.operations.getVideosOperation({
 operation: operation,
 });
 }

 // Download the video.
 ai.files.download({
 file: operation.response.generatedVideos[0].video,
 downloadPath: "veo3_with_image_input.mp4",
 });
 console.log(`Generated video saved to veo3_with_image_input.mp4`);

### Go

 package main

 import (
 "context"
 "log"
 "os"
 "time"

 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 prompt := "Panning wide shot of a calico kitten sleeping in the sunshine"

 // Step 1: Generate an image with Nano Banana.
 imageResponse, err := client.Models.GenerateContent(
 ctx,
 "gemini-2.5-flash-image",
 prompt,
 nil, // GenerateImagesConfig
 )
 if err != nil {
 log.Fatal(err)
 }

 // Step 2: Generate video with Veo 3.1 using the image.
 operation, err := client.Models.GenerateVideos(
 ctx,
 "veo-3.1-generate-preview",
 prompt,
 imageResponse.GeneratedImages[0].Image,
 nil, // GenerateVideosConfig
 )
 if err != nil {
 log.Fatal(err)
 }

 // Poll the operation status until the video is ready.
 for !operation.Done {
 log.Println("Waiting for video generation to complete...")
 time.Sleep(10 * time.Second)
 operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
 }

 // Download the video.
 video := operation.Response.GeneratedVideos[0]
 client.Files.Download(ctx, video.Video, nil)
 fname := "veo3_with_image_input.mp4"
 _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
 log.Printf("Generated video saved to %s\n", fname)
 }

### Using reference images

| **Note:** This feature is available for Veo 3.1 models only

Veo 3.1 now accepts up to 3 reference images to guide your generated video's content. Provide images of a person, character, or product to preserve the subject's appearance in the output video.

For example, using these three images generated with[Nano Banana](https://ai.google.dev/gemini-api/docs/image-generation)as references with a[well-written prompt](https://ai.google.dev/gemini-api/docs/video#use-reference-images)creates the following video:

|```dress_image```|```woman_image```|```glasses_image```|
|----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------|
| ![High-fashion flamingo dress with layers of pink and fuchsia feathers](https://storage.googleapis.com/generativeai-downloads/images/flamingo.png) | ![Beautiful woman with dark hair and warm brown eyes](https://storage.googleapis.com/generativeai-downloads/images/flamingo_woman.png) | ![Whimsical pink, heart-shaped sunglasses](https://storage.googleapis.com/generativeai-downloads/images/flamingo_glasses.png) |

### Python

 import time
 from google import genai

 client = genai.Client()

 prompt = "The video opens with a medium, eye-level shot of a beautiful woman with dark hair and warm brown eyes. She wears a magnificent, high-fashion flamingo dress with layers of pink and fuchsia feathers, complemented by whimsical pink, heart-shaped sunglasses. She walks with serene confidence through the crystal-clear, shallow turquoise water of a sun-drenched lagoon. The camera slowly pulls back to a medium-wide shot, revealing the breathtaking scene as the dress's long train glides and floats gracefully on the water's surface behind her. The cinematic, dreamlike atmosphere is enhanced by the vibrant colors of the dress against the serene, minimalist landscape, capturing a moment of pure elegance and high-fashion fantasy."

 dress_reference = types.VideoGenerationReferenceImage(
 image=dress_image, # Generated separately with Nano Banana
 reference_type="asset"
 )

 sunglasses_reference = types.VideoGenerationReferenceImage(
 image=glasses_image, # Generated separately with Nano Banana
 reference_type="asset"
 )

 woman_reference = types.VideoGenerationReferenceImage(
 image=woman_image, # Generated separately with Nano Banana
 reference_type="asset"
 )

 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 prompt=prompt,
 config=types.GenerateVideosConfig(
 reference_images=[dress_reference, glasses_reference, woman_reference],
 ),
 )

 # Poll the operation status until the video is ready.
 while not operation.done:
 print("Waiting for video generation to complete...")
 time.sleep(10)
 operation = client.operations.get(operation)

 # Download the video.
 video = operation.response.generated_videos[0]
 client.files.download(file=video.video)
 video.video.save("veo3.1_with_reference_images.mp4")
 print("Generated video saved to veo3.1_with_reference_images.mp4")

### Using first and last frames

| **Note:** This feature is available for Veo 3.1 models only

Veo 3.1 lets you create videos using interpolation, or specifying the first and last frames of the video. For information about writing effective text prompts for video generation, see the[Veo prompt guide](https://ai.google.dev/gemini-api/docs/video#use-reference-images).

### Python

 import time
 from google import genai

 client = genai.Client()

 prompt = "A cinematic, haunting video. A ghostly woman with long white hair and a flowing dress swings gently on a rope swing beneath a massive, gnarled tree in a foggy, moonlit clearing. The fog thickens and swirls around her, and she slowly fades away, vanishing completely. The empty swing is left swaying rhythmically on its own in the eerie silence."

 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 prompt=prompt,
 image=first_image, # Generated separately with Nano Banana
 config=types.GenerateVideosConfig(
 last_frame=last_image # Generated separately with Nano Banana
 ),
 )

 # Poll the operation status until the video is ready.
 while not operation.done:
 print("Waiting for video generation to complete...")
 time.sleep(10)
 operation = client.operations.get(operation)

 # Download the video.
 video = operation.response.generated_videos[0]
 client.files.download(file=video.video)
 video.video.save("veo3.1_with_interpolation.mp4")
 print("Generated video saved to veo3.1_with_interpolation.mp4")

|```first_image```|```last_image```| *veo3.1_with_interpolation.mp4* |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ![A ghostly woman with long white hair and a flowing dress swings gently on a rope swing](https://storage.googleapis.com/generativeai-downloads/images/ghost_girl.png) | ![The ghostly woman vanishes from the swing](https://storage.googleapis.com/generativeai-downloads/images/empty_tree.png) | ![A cinematic, haunting video of an eerie woman disappearing from a swing in the mist](https://storage.googleapis.com/generativeai-downloads/images/creepy_swing.gif) |

## Extending Veo videos

| **Note:** This feature is available for Veo 3.1 models only

Use Veo 3.1 to extend videos that you previously generated with Veo by 7 seconds and up to 20 times.

Input video limitations:

- Veo-generated videos only up to 141 seconds long.
- Gemini API only supports video extensions for Veo-generated videos.
- Input videos are expected to have a certain length, aspect ratio, and dimensions:
 - Aspect ratio: 9:16 or 16:9
 - Resolution: 720p
 - Video length: 141 seconds or less

The output of the extension is a single video combining the user input video and the generated extended video for up to 148 seconds of video.

This example takes the Veo-generated video*butterfly_video* , shown here with its original prompt, and extends it using the`video`parameter and a new prompt:

| Prompt | Output:`butterfly_video` |
|-----------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| An origami butterfly flaps its wings and flies out of the french doors into the garden. | ![Origami butterfly flaps its wings and flies out of the french doors into the garden.](https://storage.googleapis.com/generativeai-downloads/images/Butterfly_original.gif) |

### Python

 import time
 from google import genai

 client = genai.Client()

 prompt = "Track the butterfly into the garden as it lands on an orange origami flower. A fluffy white puppy runs up and gently pats the flower."

 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 video=butterfly_video,
 prompt=prompt,
 config=types.GenerateVideosConfig(
 number_of_videos=1,
 resolution="720p"
 ),
 )

 # Poll the operation status until the video is ready.
 while not operation.done:
 print("Waiting for video generation to complete...")
 time.sleep(10)
 operation = client.operations.get(operation)

 # Download the video.
 video = operation.response.generated_videos[0]
 client.files.download(file=video.video)
 video.video.save("veo3.1_extension.mp4")
 print("Generated video saved to veo3.1_extension.mp4")

For information about writing effective text prompts for video generation, see the[Veo prompt guide](https://ai.google.dev/gemini-api/docs/video#extend-prompt).

## Handling asynchronous operations

Video generation is a computationally intensive task. When you send a request to the API, it starts a long-running job and immediately returns an`operation`object. You must then poll until the video is ready, which is indicated by the`done`status being true.

The core of this process is a polling loop, which periodically checks the job's status.

### Python

 import time
 from google import genai
 from google.genai import types

 client = genai.Client()

 # After starting the job, you get an operation object.
 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 prompt="A cinematic shot of a majestic lion in the savannah.",
 )

 # Alternatively, you can use operation.name to get the operation.
 operation = types.GenerateVideosOperation(name=operation.name)

 # This loop checks the job status every 10 seconds.
 while not operation.done:
 time.sleep(10)
 # Refresh the operation object to get the latest status.
 operation = client.operations.get(operation)

 # Once done, the result is in operation.response.
 # ... process and download your video ...

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const ai = new GoogleGenAI({});

 // After starting the job, you get an operation object.
 let operation = await ai.models.generateVideos({
 model: "veo-3.1-generate-preview",
 prompt: "A cinematic shot of a majestic lion in the savannah.",
 });

 // Alternatively, you can use operation.name to get the operation.
 // operation = types.GenerateVideosOperation(name=operation.name)

 // This loop checks the job status every 10 seconds.
 while (!operation.done) {
 await new Promise((resolve) => setTimeout(resolve, 1000));
 // Refresh the operation object to get the latest status.
 operation = await ai.operations.getVideosOperation({ operation });
 }

 // Once done, the result is in operation.response.
 // ... process and download your video ...

## Veo API parameters and specifications

These are the parameters you can set in your API request to control the video generation process.

| Parameter | Description | Veo 3.1 \& Veo 3.1 Fast | Veo 3 \& Veo 3 Fast | Veo 2 |
|--------------------|---------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------|
| `prompt` | The text description for the video. Supports audio cues. | `string` | `string` | `string` |
| `negativePrompt` | Text describing what not to include in the video. | `string` | `string` | `string` |
| `image` | An initial image to animate. | `Image`object | `Image`object | `Image`object |
| `lastFrame` | The final image for an interpolation video to transition. Must be used in combination with the`image`parameter. | `Image`object | `Image`object | `Image`object |
| `referenceImages` | Up to three images to be used as style and content references. | `VideoGenerationReferenceImage`object (Veo 3.1 only) | n/a | n/a |
| `video` | Video to be used for video extension. | `Video`object | n/a | n/a |
| `aspectRatio` | The video's aspect ratio. | `"16:9"`(default, 720p \& 1080p), `"9:16"`(720p \& 1080p) | `"16:9"`(default, 720p \& 1080p), `"9:16"`(720p \& 1080p) | `"16:9"`(default, 720p), `"9:16"`(720p) |
| `resolution` | The video's aspect ratio. | `"720p"`(default), `"1080p"`(only supports 8s duration) `"720p"`only for extension | `"720p"`(default), `"1080p"`(16:9 only) | Unsupported |
| `durationSeconds` | Length of the generated video. | `"4"`,`"6"`,`"8"`. Must be "8" when using extension or interpolation (supports both 16:9 and 9:16), and when using`referenceImages`(only supports 16:9) | `"4"`,`"6"`,`"8"` | `"5"`,`"6"`,`"8"` |
| `personGeneration` | Controls the generation of people. (See[Limitations](https://ai.google.dev/gemini-api/docs/video#limitations)for region restrictions) | Text-to-video \& Extension: `"allow_all"`only Image-to-video, Interpolation, \& Reference images: `"allow_adult"`only | Text-to-video: `"allow_all"`only Image-to-video: `"allow_adult"`only | Text-to-video: `"allow_all"`,`"allow_adult"`,`"dont_allow"` Image-to-video: `"allow_adult"`, and`"dont_allow"` |

Note that the`seed`parameter is also available for Veo 3 models. It doesn't guarantee determinism, but slightly improves it.

You can customize your video generation by setting parameters in your request. For example you can specify`negativePrompt`to guide the model.

### Python

 import time
 from google import genai
 from google.genai import types

 client = genai.Client()

 operation = client.models.generate_videos(
 model="veo-3.1-generate-preview",
 prompt="A cinematic shot of a majestic lion in the savannah.",
 config=types.GenerateVideosConfig(negative_prompt="cartoon, drawing, low quality"),
 )

 # Poll the operation status until the video is ready.
 while not operation.done:
 print("Waiting for video generation to complete...")
 time.sleep(10)
 operation = client.operations.get(operation)

 # Download the generated video.
 generated_video = operation.response.generated_videos[0]
 client.files.download(file=generated_video.video)
 generated_video.video.save("parameters_example.mp4")
 print("Generated video saved to parameters_example.mp4")

### JavaScript

 import { GoogleGenAI } from "@google/genai";

 const ai = new GoogleGenAI({});

 let operation = await ai.models.generateVideos({
 model: "veo-3.1-generate-preview",
 prompt: "A cinematic shot of a majestic lion in the savannah.",
 config: {
 aspectRatio: "16:9",
 negativePrompt: "cartoon, drawing, low quality"
 },
 });

 // Poll the operation status until the video is ready.
 while (!operation.done) {
 console.log("Waiting for video generation to complete...")
 await new Promise((resolve) => setTimeout(resolve, 10000));
 operation = await ai.operations.getVideosOperation({
 operation: operation,
 });
 }

 // Download the generated video.
 ai.files.download({
 file: operation.response.generatedVideos[0].video,
 downloadPath: "parameters_example.mp4",
 });
 console.log(`Generated video saved to parameters_example.mp4`);

### Go

 package main

 import (
 "context"
 "log"
 "os"
 "time"

 "google.golang.org/genai"
 )

 func main() {
 ctx := context.Background()
 client, err := genai.NewClient(ctx, nil)
 if err != nil {
 log.Fatal(err)
 }

 videoConfig := &genai.GenerateVideosConfig{
 AspectRatio: "16:9",
 NegativePrompt: "cartoon, drawing, low quality",
 }

 operation, _ := client.Models.GenerateVideos(
 ctx,
 "veo-3.1-generate-preview",
 "A cinematic shot of a majestic lion in the savannah.",
 nil,
 videoConfig,
 )

 // Poll the operation status until the video is ready.
 for !operation.Done {
 log.Println("Waiting for video generation to complete...")
 time.Sleep(10 * time.Second)
 operation, _ = client.Operations.GetVideosOperation(ctx, operation, nil)
 }

 // Download the generated video.
 video := operation.Response.GeneratedVideos[0]
 client.Files.Download(ctx, video.Video, nil)
 fname := "parameters_example.mp4"
 _ = os.WriteFile(fname, video.Video.VideoBytes, 0644)
 log.Printf("Generated video saved to %s\n", fname)
 }

### REST

 # Note: This script uses jq to parse the JSON response.
 # GEMINI API Base URL
 BASE_URL="https://generativelanguage.googleapis.com/v1beta"

 # Send request to generate video and capture the operation name into a variable.
 operation_name=$(curl -s "${BASE_URL}/models/veo-3.1-generate-preview:predictLongRunning" \
 -H "x-goog-api-key: $GEMINI_API_KEY" \
 -H "Content-Type: application/json" \
 -X "POST" \
 -d '{
 "instances": [{
 "prompt": "A cinematic shot of a majestic lion in the savannah."
 }
 ],
 "parameters": {
 "aspectRatio": "16:9",
 "negativePrompt": "cartoon, drawing, low quality"
 }
 }' | jq -r .name)

 # Poll the operation status until the video is ready
 while true; do
 # Get the full JSON status and store it in a variable.
 status_response=$(curl -s -H "x-goog-api-key: $GEMINI_API_KEY" "${BASE_URL}/${operation_name}")

 # Check the "done" field from the JSON stored in the variable.
 is_done=$(echo "${status_response}" | jq .done)

 if [ "${is_done}" = "true" ]; then
 # Extract the download URI from the final response.
 video_uri=$(echo "${status_response}" | jq -r '.response.generateVideoResponse.generatedSamples[0].video.uri')
 echo "Downloading video from: ${video_uri}"

 # Download the video using the URI and API key and follow redirects.
 curl -L -o parameters_example.mp4 -H "x-goog-api-key: $GEMINI_API_KEY" "${video_uri}"
 break
 fi
 # Wait for 5 seconds before checking again.
 sleep 10
 done

## Veo prompt guide

This section contains examples of videos you can create using Veo, and shows you how to modify prompts to produce distinct results.

### Safety filters

Veo applies safety filters across Gemini to help ensure that generated videos and uploaded photos don't contain offensive content. Prompts that violate our[terms and guidelines](https://ai.google.dev/gemini-api/docs/usage-policies#abuse-monitoring)are blocked.

### Prompt writing basics

Good prompts are descriptive and clear. To get the most out of Veo, start with identifying your core idea, refine your idea by adding keywords and modifiers, and incorporate video-specific terminology into your prompts.

The following elements should be included in your prompt:

- **Subject** : The object, person, animal, or scenery that you want in your video, such as*cityscape* ,*nature* ,*vehicles* , or*puppies*.
- **Action** : What the subject is doing (for example,*walking* ,*running* , or*turning their head*).
- **Style** : Specify creative direction using specific film style keywords, such as*sci-fi* ,*horror film* ,*film noir* , or animated styles like*cartoon*.
- **Camera positioning and motion** : \[Optional\] Control the camera's location and movement using terms like*aerial view* ,*eye-level* ,*top-down shot* ,*dolly shot* , or*worms eye*.
- **Composition** : \[Optional\] How the shot is framed, such as*wide shot* ,*close-up* ,*single-shot* or*two-shot*.
- **Focus and lens effects** : \[Optional\] Use terms like*shallow focus* ,*deep focus* ,*soft focus* ,*macro lens* , and*wide-angle lens*to achieve specific visual effects.
- **Ambiance** : \[Optional\] How the color and light contribute to the scene, such as*blue tones* ,*night* , or*warm tones*.

#### More tips for writing prompts

- **Use descriptive language**: Use adjectives and adverbs to paint a clear picture for Veo.
- **Enhance the facial details** : Specify facial details as a focus of the photo like using the word*portrait*in the prompt.

*For more comprehensive prompting strategies, visit[Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro).*

### Prompting for audio

With Veo 3, you can provide cues for sound effects, ambient noise, and dialogue. The model captures the nuance of these cues to generate a synchronized soundtrack.

- **Dialogue:**Use quotes for specific speech. (Example: "This must be the key," he murmured.)
- **Sound Effects (SFX):**Explicitly describe sounds. (Example: tires screeching loudly, engine roaring.)
- **Ambient Noise:**Describe the environment's soundscape. (Example: A faint, eerie hum resonates in the background.)

These videos demonstrate prompting Veo 3's audio generation with increasing levels of detail.

| **Prompt** | **Generated output** |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|
| **More detail (Dialogue and ambience)** A wide shot of a misty Pacific Northwest forest. Two exhausted hikers, a man and a woman, push through ferns when the man stops abruptly, staring at a tree. Close-up: Fresh, deep claw marks are gouged into the tree's bark. Man: (Hand on his hunting knife) "That's no ordinary bear." Woman: (Voice tight with fear, scanning the woods) "Then what is it?" A rough bark, snapping twigs, footsteps on the damp earth. A lone bird chirps. | ![Two people in the woods encounter signs of a bear.](https://storage.googleapis.com/generativeai-downloads/images/Scary_Bear.gif) |
| **Less detail (Dialogue)** Paper Cut-Out Animation. New Librarian: "Where do you keep the forbidden books?" Old Curator: "We don't. They keep us." | ![Animated librarians discussing forbidden books](https://storage.googleapis.com/generativeai-downloads/images/Library.gif) |

Try out these prompts yourself to hear the audio![Try Veo 3](https://deepmind.google/models/veo/)

### Prompting with reference images

You can use one or more images as inputs to guide your generated videos, using Veo's[image-to-video](https://ai.google.dev/gemini-api/docs/video#generate-from-images)capabilities. Veo uses the input image as the initial frame. Select an image closest to what you envision as the first scene of your video to animate everyday objects, bring drawings and paintings to life, and add movement and sound to nature scenes.

| **Prompt** | **Generated output** |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Input image (Generated by Nano Banana)** A hyperrealistic macro photo of tiny, miniature surfers riding ocean waves inside a rustic stone bathroom sink. A vintage brass faucet is running, creating the perpetual surf. Surreal, whimsical, bright natural lighting. | ![Tiny, miniature surfers riding ocean waves inside a rustic stone bathroom sink.](https://storage.googleapis.com/generativeai-downloads/images/Sink_Surfers.png) |
| **Output Video (Generated by Veo 3.1)** A surreal, cinematic macro video. Tiny surfers ride perpetual, rolling waves inside a stone bathroom sink. A running vintage brass faucet generates the endless surf. The camera slowly pans across the whimsical, sunlit scene as the miniature figures expertly carve the turquoise water. | ![Tiny surfers circling the waves in a bathroom sink.](https://storage.googleapis.com/generativeai-downloads/images/sink_surfers.gif) |

Veo 3.1 lets you reference images or ingredients to direct your generated video's content. Provide up to three asset images of a single person, character, or product. Veo preserves the subject's appearance in the output video.

| **Prompt** | **Generated output** |
|-----------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|
| **Reference image (Generated by Nano Banana)** A deep sea angler fish lurks in the deep dark water, teeth bared and bait glowing. | ![A dark and glowing angler fish](https://storage.googleapis.com/generativeai-downloads/images/angler_fish.png) |
| **Reference image (Generated by Nano Banana)** A pink child's princess costume complete with a wand and tiara, on a plain product background. | ![A childs pink princess constume](https://storage.googleapis.com/generativeai-downloads/images/princess_dress.png) |
| **Output Video (Generated by Veo 3.1)** Create a silly cartoon version of the fish wearing the costume, swimming and waving the wand around. | ![An angler fish wearing a princess costume](https://storage.googleapis.com/generativeai-downloads/images/angler_princess.gif) |

Using Veo 3.1, you can also generate videos by specifying the first and last frames of the video.

| **Prompt** | **Generated output** |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
| **First image (Generated by Nano Banana)** A high quality photorealistic front image of a ginger cat driving a red convertible racing car on the French riviera coast. | ![A ginger cat driving a red convertible racing car](https://storage.googleapis.com/generativeai-downloads/images/ginger_race_cat.jpeg) |
| **Last image (Generated by Nano Banana)** Show what happens when the car takes off from a cliff. | ![A ginger cat driving a red convertible goes off a cliff](https://storage.googleapis.com/generativeai-downloads/images/race_cat_cliff.jpeg) |
| **Output Video (Generated by Veo 3.1)** Optional | ![A cat drives of a cliff and takes off](https://storage.googleapis.com/generativeai-downloads/images/race_cat_cliff.gif) |

This feature gives you precise control over your shot's composition by letting you define the starting and ending frame. Upload an image or use a frame from a previous video generation to make sure your scene begins and concludes exactly as you envision it.

### Prompting for extension

To extend your Veo-generated video with Veo 3.1, use the video as an input along with an optional text prompt. Extend finalizes the final second or 24 frames of your video and continues the action.

Note that voice is not able to be effectively extended if it's not present in the last 1 second of video.

| **Prompt** | **Generated output** |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Input video (Generated by Veo 3.1)** The paraglider takes off from the top of the mountain and starts gliding down the mountains overlooking the flower covered valleys below. | ![A paraglider takes off from the top of a mountain](https://storage.googleapis.com/generativeai-downloads/images/Paraglider.gif) |
| **Output Video (Generated by Veo 3.1)** Extend this video with the paraglider slowly descending. | ![A paraglider takes off from the top of a mountain, then slowly descends](https://storage.googleapis.com/generativeai-downloads/images/Paraglider_Extend.gif) |

### Example prompts and output

This section presents several prompts, highlighting how descriptive details can elevate the outcome of each video.

#### Icicles

This video demonstrates how you can use the elements of[prompt writing basics](https://ai.google.dev/gemini-api/docs/video#basics)in your prompt.

| **Prompt** | **Generated output** |
|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|
| Close up shot (composition) of melting icicles (subject) on a frozen rock wall (context) with cool blue tones (ambiance), zoomed in (camera motion) maintaining close-up detail of water drips (action). | ![Dripping icicles with a blue background.](https://storage.googleapis.com/generativeai-downloads/images/Icicles.gif) |

#### Man on the phone

These videos demonstrate how you can revise your prompt with increasingly specific details to get Veo to refine the output to your liking.

| **Prompt** | **Generated output** |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| **Less detail** The camera dollies to show a close up of a desperate man in a green trench coat. He's making a call on a rotary-style wall phone with a green neon light. It looks like a movie scene. | ![Man talking on the phone.](https://storage.googleapis.com/generativeai-downloads/images/Desperate_Man.gif) |
| **More detail** A close-up cinematic shot follows a desperate man in a weathered green trench coat as he dials a rotary phone mounted on a gritty brick wall, bathed in the eerie glow of a green neon sign. The camera dollies in, revealing the tension in his jaw and the desperation etched on his face as he struggles to make the call. The shallow depth of field focuses on his furrowed brow and the black rotary phone, blurring the background into a sea of neon colors and indistinct shadows, creating a sense of urgency and isolation. | ![Man talking on the phone](https://storage.googleapis.com/generativeai-downloads/images/detail_call.gif) |

#### Snow leopard

| **Prompt** | **Generated output** |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| **Simple prompt:** A cute creature with snow leopard-like fur is walking in winter forest, 3D cartoon style render. | ![Snow leopard is lethargic.](https://storage.googleapis.com/generativeai-downloads/images/snowleopard.gif) |
| **Detailed prompt:** Create a short 3D animated scene in a joyful cartoon style. A cute creature with snow leopard-like fur, large expressive eyes, and a friendly, rounded form happily prances through a whimsical winter forest. The scene should feature rounded, snow-covered trees, gentle falling snowflakes, and warm sunlight filtering through the branches. The creature's bouncy movements and wide smile should convey pure delight. Aim for an upbeat, heartwarming tone with bright, cheerful colors and playful animation. | ![Snow leopard is running faster.](https://storage.googleapis.com/generativeai-downloads/images/snow-run.gif) |

### Examples by writing elements

These examples show you how to refine your prompts by each basic element.

#### Subject and context

Specify the main focus (subject) and the background or environment (context).

| **Prompt** | **Generated output** |
|---------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------|
| An architectural rendering of a white concrete apartment building with flowing organic shapes, seamlessly blending with lush greenery and futuristic elements | ![Placeholder.](https://storage.googleapis.com/generativeai-downloads/images/architecture.gif) |
| A satellite floating through outer space with the moon and some stars in the background. | ![Satellite floating in the atmosphere.](https://storage.googleapis.com/generativeai-downloads/images/satellite.gif) |

#### Action

Specify what the subject is doing (e.g., walking, running, or turning their head).

| **Prompt** | **Generated output** |
|------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------|
| A wide shot of a woman walking along the beach, looking content and relaxed towards the horizon at sunset. | ![Sunset is absolutely beautiful.](https://storage.googleapis.com/generativeai-downloads/images/sunset.gif) |

#### Style

Add keywords to steer the generation toward a specific aesthetic (e.g., surreal, vintage, futuristic, film noir).

| **Prompt** | **Generated output** |
|-----------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------|
| Film noir style, man and woman walk on the street, mystery, cinematic, black and white. | ![Film noir style is absolutely beautiful.](https://storage.googleapis.com/generativeai-downloads/images/noir.gif) |

#### Camera motion and composition

Specify how the camera moves (POV shot, aerial view, tracking drone view) and how the shot is framed (wide shot, close-up, low angle).

| **Prompt** | **Generated output** |
|--------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------|
| A POV shot from a vintage car driving in the rain, Canada at night, cinematic. | ![Sunset is absolutely beautiful.](https://storage.googleapis.com/generativeai-downloads/images/car-pov.gif) |
| Extreme close-up of a an eye with city reflected in it. | ![Sunset is absolutely beautiful.](https://storage.googleapis.com/generativeai-downloads/images/eye.gif) |

#### Ambiance

Color palettes and lighting influence the mood. Try terms like "muted orange warm tones," "natural light," "sunrise," or "cool blue tones."

| **Prompt** | **Generated output** |
|---------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|
| A close-up of a girl holding adorable golden retriever puppy in the park, sunlight. | ![A puppy in a young girl's arms.](https://ai.google.dev/static/gemini-api/docs/video/images/ambiance_puppy.gif) |
| Cinematic close-up shot of a sad woman riding a bus in the rain, cool blue tones, sad mood. | ![A woman riding on a bus that feels sad.](https://ai.google.dev/static/gemini-api/docs/video/images/ambiance_sad.gif) |

### Negative prompts

Negative prompts specify elements you*don't*want in the video.

-  Don't use instructive language like*no* or*don't*. (e.g., "No walls").
-  Do describe what you don't want to see. (e.g., "wall, frame").

| **Prompt** | **Generated output** |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------|
| **Without Negative Prompt:** Generate a short, stylized animation of a large, solitary oak tree with leaves blowing vigorously in a strong wind... \[truncated\] | ![Tree with using words.](https://ai.google.dev/static/gemini-api/docs/video/images/tree_with_no_negative.gif) |
| **With Negative Prompt:** \[Same prompt\] Negative prompt: urban background, man-made structures, dark, stormy, or threatening atmosphere. | ![Tree with no negative words.](https://ai.google.dev/static/gemini-api/docs/video/images/tree_with_negative.gif) |

### Aspect ratios

Veo lets you specify the aspect ratio for your video.

| **Prompt** | **Generated output** |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Widescreen (16:9)** Create a video with a tracking drone view of a man driving a red convertible car in Palm Springs, 1970s, warm sunlight, long shadows. | ![A man driving a red convertible car in Palm Springs, 1970s style.](https://ai.google.dev/static/gemini-api/docs/video/images/widescreen_palm_springs.gif) |
| **Portrait (9:16)** Create a video highlighting the smooth motion of a majestic Hawaiian waterfall within a lush rainforest. Focus on realistic water flow, detailed foliage, and natural lighting to convey tranquility. Capture the rushing water, misty atmosphere, and dappled sunlight filtering through the dense canopy. Use smooth, cinematic camera movements to showcase the waterfall and its surroundings. Aim for a peaceful, realistic tone, transporting the viewer to the serene beauty of the Hawaiian rainforest. | ![A majestic Hawaiian waterfall in a lush rainforest.](https://ai.google.dev/static/gemini-api/docs/video/images/waterfall.gif) |

## Limitations

- **Request latency:**Min: 11 seconds; Max: 6 minutes (during peak hours).
- **Regional limitations:** In EU, UK, CH, MENA locations, the following are the allowed values for`personGeneration`:
 - Veo 3:`allow_adult`only.
 - Veo 2:`dont_allow`and`allow_adult`. Default is`dont_allow`.
- **Video retention:**Generated videos are stored on the server for 2 days, after which they are removed. To save a local copy, you must download your video within 2 days of generation. Extended videos are treated as newly generated videos.
- **Watermarking:** Videos created by Veo are watermarked using[SynthID](https://deepmind.google/technologies/synthid/), our tool for watermarking and identifying AI-generated content. Videos can be verified using the[SynthID](https://deepmind.google/science/synthid/)verification platform.
- **Safety:**Generated videos are passed through safety filters and memorization checking processes that help mitigate privacy, copyright and bias risks.
- **Audio error:**Veo 3.1 will sometimes block a video from generating because of safety filters or other processing issues with the audio. You will not be charged if your video is blocked from generating.

## Model features

| Feature | Description | Veo 3.1 \& Veo 3.1 Fast | Veo 3 \& Veo 3 Fast | Veo 2 |
|------------------------|-----------------------------------------|----------------------------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------------|
| **Audio** | Natively generates audio with video. | Natively generates audio with video. |  Always on |  Silent only |
| **Input Modalities** | The type of input used for generation. | Text-to-Video, Image-to-Video, Video-to-Video | Text-to-Video, Image-to-Video | Text-to-Video, Image-to-Video |
| **Resolution** | The output resolution of the video. | 720p \& 1080p (8s length only) 720p only when using video extension. | 720p \& 1080p (16:9 only) | 720p |
| **Frame Rate** | The output frame rate of the video. | 24fps | 24fps | 24fps |
| **Video Duration** | Length of the generated video. | 8 seconds, 6 seconds, 4 seconds 8 seconds only when using reference images | 8 seconds | 5-8 seconds |
| **Videos per Request** | Number of videos generated per request. | 1 | 1 | 1 or 2 |
| **Status \& Details** | Model availability and further details. | [Preview](https://ai.google.dev/gemini-api/docs/models#preview) | [Stable](https://ai.google.dev/gemini-api/docs/models#veo-3) | [Stable](https://ai.google.dev/gemini-api/docs/models#latest-stable) |

## Model versions

Check out the[Pricing](https://ai.google.dev/gemini-api/docs/pricing#veo-3.1)and[Rate limits](https://ai.google.dev/gemini-api/docs/rate-limits)pages for more Veo model-specific usage details.

Veo Fast versions allow developers to create videos with sound while maintaining high quality and optimizing for speed and business use cases. They're ideal for backend services that programmatically generate ads, tools for rapid A/B testing of creative concepts, or apps that need to quickly produce social media content.

### Veo 3.1 Preview

| Property | Description |
|-----------------------------|---------------------------------------------------|
| id_cardModel code | **Gemini API** `veo-3.1-generate-preview` |
| saveSupported data types | **Input** Text, Image **Output** Video with audio |
| token_autoLimits | **Text input** 1,024 tokens **Output video** 1 |
| calendar_monthLatest update | September 2025 |

### Veo 3.1 Fast Preview

| Property | Description |
|-----------------------------|---------------------------------------------------|
| id_cardModel code | **Gemini API** `veo-3.1-fast-generate-preview` |
| saveSupported data types | **Input** Text, Image **Output** Video with audio |
| token_autoLimits | **Text input** 1,024 tokens **Output video** 1 |
| calendar_monthLatest update | September 2025 |

### Veo 3

| Property | Description |
|-----------------------------|---------------------------------------------------|
| id_cardModel code | **Gemini API** `veo-3.0-generate-001` |
| saveSupported data types | **Input** Text, Image **Output** Video with audio |
| token_autoLimits | **Text input** 1,024 tokens **Output video** 1 |
| calendar_monthLatest update | July 2025 |

### Veo 3 Fast

| Property | Description |
|-----------------------------|---------------------------------------------------|
| id_cardModel code | **Gemini API** `veo-3.0-fast-generate-001` |
| saveSupported data types | **Input** Text, Image **Output** Video with audio |
| token_autoLimits | **Text input** 1,024 tokens **Output video** 1 |
| calendar_monthLatest update | July 2025 |

### Veo 2

| Property | Description |
|-----------------------------|------------------------------------------------------------------------------------------------------------------------|
| id_cardModel code | **Gemini API** `veo-2.0-generate-001` |
| saveSupported data types | **Input** Text, image **Output** Video |
| token_autoLimits | **Text input** N/A **Image input** Any image resolution and aspect ratio up to 20MB file size **Output video** Up to 2 |
| calendar_monthLatest update | April 2025 |

## What's next

- Get started with the Veo 3.1 API by experimenting in the[Veo Quickstart Colab](https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/Get_started_Veo.ipynb)and the[Veo 3.1 applet](https://aistudio.google.com/apps/bundled/veo_studio).
- Learn how to write even better prompts with our[Introduction to prompt design](https://ai.google.dev/gemini-api/docs/prompting-intro).

[END OF DOCUMENT: GEMINI8YOUZTXZE]
---

