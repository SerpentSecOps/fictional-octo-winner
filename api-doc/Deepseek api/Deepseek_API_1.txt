
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: DEEPSEEKY1DNLF5RU (sha256-5755c13dcbfaa5aa36571ce5beec988c7a5dfcf8869bda326dcebd7db78b725c) | Title: Authentication]
[DocID: DEEPSEEK1J007V3UHY (sha256-8d1e5b4eb8264bee8c3ed3a412871ffd89a107aaec7aa0c2fd581afc8f07f73b) | Title: Context-Caching]
[DocID: DEEPSEEK2FO59X9U8F (sha256-e0f1b86cd64f744fa2cc81ab012e2970c20e503bb88a631bd1d6ef565fd461be) | Title: Deepseek-Chat-Model]
[DocID: DEEPSEEK2PGPEC99SP (sha256-fa125cdcadd9891d3f107cd274fc3177fb5ba152172ff719a067158b5d5ff4f4) | Title: Deepseek-Reasoner-Model]
--- END OF TOC ---

[START OF DOCUMENT: DEEPSEEKY1DNLF5RU | Title: Authentication]

# Authentication Setup

## Obtaining Your API Key

1. Visit the DeepSeek platform dashboard
2. Navigate to the API keys section
3. Generate a new API key
4. Store it securely (never commit to version control)

## Base URLs

- **Standard**: `https://api.deepseek.com`
- **OpenAI Compatible**: `https://api.deepseek.com/v1`

## Authentication Format

Include your API key in the Authorization header:```
Authorization: Bearer YOUR_API_KEY
```## Security Best Practices

- Never commit API keys to version control
- Store keys in environment variables
- Rotate keys periodically
- Use separate keys for development and production
- Revoke keys immediately if compromised

[END OF DOCUMENT: DEEPSEEKY1DNLF5RU]
---

[START OF DOCUMENT: DEEPSEEK1J007V3UHY | Title: Context-Caching]

# Context Caching

## Overview

DeepSeek's context caching system automatically caches frequently used content to reduce costs by up to 90%.

## How It Works

When you send similar prompts or reuse content:
1. **First Request**: Content is processed normally (cache miss)
2. **Subsequent Requests**: Cached content is retrieved (cache hit)
3. **Cost Savings**: Cache hits cost only $0.028 per 1M tokens vs $0.28

## Cost Comparison

| Cache Status | Cost per 1M Tokens | Savings |
|--------------|-------------------|---------|
| **Cache Miss** | $0.28 | Baseline |
| **Cache Hit** | $0.028 | **90% off** |

## API Response Format

The API response includes detailed cache usage statistics:```json
{
  "usage": {
    "prompt_tokens": 15000,
    "completion_tokens": 500,
    "total_tokens": 15500,
    "prompt_cache_hit_tokens": 12000,
    "prompt_cache_miss_tokens": 3000
  }
}
```### Field Descriptions

- **prompt_tokens**: Total input tokens
- **completion_tokens**: Total output tokens
- **total_tokens**: Sum of prompt + completion
- **prompt_cache_hit_tokens**: Tokens retrieved from cache
- **prompt_cache_miss_tokens**: Tokens not in cache

## Example Calculation

**Scenario**: Processing a 15,000-token prompt with 12,000 tokens cached

**Cache Hit Cost**:```
12,000 / 1,000,000 × $0.028 = $0.000336
```**Cache Miss Cost**:```
3,000 / 1,000,000 × $0.28 = $0.00084
```**Total Input Cost**: $0.001176

**Without Caching**:```
15,000 / 1,000,000 × $0.28 = $0.0042
```**Savings**: $0.0042 - $0.001176 = **$0.003024 (72% savings)**

## Use Cases for Caching

### Document Processing
Process the same document multiple times with different queries:```python
base_document = "..." # Large document (10,000 tokens)

# First query - cache miss
response1 = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": base_document},
        {"role": "user", "content": "Summarize this"}
    ]
)

# Second query - cache hit!
response2 = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": base_document},
        {"role": "user", "content": "Extract key points"}
    ]
)
```### System Prompts
Reuse the same system prompt across many requests:```python
system_prompt = "You are an expert Python programmer..."

# All requests with this system prompt benefit from caching
for user_query in queries:
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_query}
        ]
    )
```### Multi-Turn Conversations
Maintain conversation history efficiently:```python
conversation = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Hello"},
    {"role": "assistant", "content": "Hi there!"},
    # Previous messages cached in subsequent turns
]

conversation.append({"role": "user", "content": "Tell me about AI"})
response = client.chat.completions.create(
    model="deepseek-chat",
    messages=conversation
)
```## Best Practices

### 1. Reuse Content
Keep consistent prompts and system messages to maximize cache hits.

### 2. Structure Prompts Strategically
Place reusable content at the beginning:```python
# Good - cacheable content first
messages = [
    {"role": "system", "content": long_instructions},  # Cached
    {"role": "user", "content": variable_query}        # New each time
]
```### 3. Monitor Cache Performance
Track cache hit rates in your usage statistics:```python
usage = response.usage
cache_hit_rate = usage.prompt_cache_hit_tokens / usage.prompt_tokens
print(f"Cache hit rate: {cache_hit_rate * 100:.1f}%")
```### 4. Batch Similar Requests
Group requests with similar content together to maximize caching benefits.

## Cache Behavior

### What Gets Cached
- System prompts
- User messages
- Document content
- Conversation history

### Cache Duration
- Automatic cache management
- Recently used content stays cached
- Rarely used content may be evicted

### Cache Scope
- Per-account caching
- Not shared across accounts
- Automatic and transparent

[END OF DOCUMENT: DEEPSEEK1J007V3UHY]
---

[START OF DOCUMENT: DEEPSEEK2FO59X9U8F | Title: Deepseek-Chat-Model]

# deepseek-chat Model

## Overview

**deepseek-chat (DeepSeek-V3.2-Exp)** - Standard chat completion model for general-purpose tasks.

## Specifications

- **Context Window**: 128,000 tokens
- **Max Output**: 4,096 tokens (default) / 8,192 tokens (maximum)
- **Mode**: Non-thinking mode (direct responses)

## Supported Features

### JSON Output Formatting
Generate structured JSON responses for data extraction and API integration.

### Function Calling
Integrate with external tools and APIs through function calls.

### Chat Prefix Completion
Continue existing conversations or complete partial responses.

### Fill-in-the-Middle (FIM) Completion
Complete code or text with context from both before and after the insertion point.

## Use Cases

### General Conversation
- Customer support chatbots
- Virtual assistants
- Q&A systems

### Content Generation
- Article writing
- Email composition
- Creative writing

### Structured Data Extraction
- Parse unstructured text
- Extract entities
- Generate JSON from descriptions

### Tool Integration
- API integration
- Database queries
- External service calls

## Example Usage```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Explain quantum computing"}
    ],
    temperature=0.7,
    max_tokens=500
)

print(response.choices[0].message.content)
```## Performance Characteristics

- **Speed**: Fast response times
- **Cost**: Lower cost per token
- **Quality**: High quality for general tasks
- **Reliability**: Consistent outputs

## When to Use

Choose **deepseek-chat** when:
- You need fast responses
- Using function calling features
- Standard conversation or generation tasks
- Cost optimization is a priority
- You don't need explicit reasoning steps

[END OF DOCUMENT: DEEPSEEK2FO59X9U8F]
---

[START OF DOCUMENT: DEEPSEEK2PGPEC99SP | Title: Deepseek-Reasoner-Model]

# deepseek-reasoner Model

## Overview

**deepseek-reasoner (DeepSeek-V3.2-Exp)** - Advanced reasoning model with Chain of Thought capabilities.

## Specifications

- **Context Window**: 128,000 tokens
- **Max Output**: 32,768 tokens (default) / 65,536 tokens (maximum)
- **Mode**: Thinking mode (shows reasoning process)

## Supported Features

### JSON Output Formatting
Generate structured JSON responses with reasoning included.

### Chat Prefix Completion
Continue existing conversations with maintained reasoning context.

### Chain of Thought Analysis
Explicit step-by-step reasoning process visible in responses.

## Important Note

**Function Calling**: Requests with function calling are automatically redirected to `deepseek-chat` for processing.

## Use Cases

### Complex Problem Solving
- Multi-step problems
- Strategy development
- Complex decision making

### Multi-Step Reasoning
- Logical deduction
- Planning and scheduling
- Algorithm design

### Mathematical Calculations
- Advanced mathematics
- Statistical analysis
- Proofs and derivations

### Logic Puzzles
- Constraint satisfaction
- Game theory
- Puzzle solving

## Example Usage```python
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://api.deepseek.com"
)

response = client.chat.completions.create(
    model="deepseek-reasoner",
    messages=[
        {
            "role": "user",
            "content": "If a train travels 120 km in 2 hours, then speeds up and travels 180 km in 1.5 hours, what is the average speed?"
        }
    ],
    max_tokens=10000
)

# Access reasoning process
print("Reasoning:", response.choices[0].message.reasoning_content)
print("\nAnswer:", response.choices[0].message.content)
```## Performance Characteristics

- **Speed**: Slower than deepseek-chat (due to reasoning)
- **Cost**: Higher token usage (includes reasoning tokens)
- **Quality**: Superior for complex tasks
- **Transparency**: Shows thinking process

## Output Structure

The model provides two types of content:

### reasoning_content
The model's internal reasoning process and steps.

### content
The final answer or conclusion.

## When to Use

Choose **deepseek-reasoner** when:
- Complex reasoning is required
- You need transparent Chain of Thought analysis
- Problem requires step-by-step thinking
- Quality and accuracy are more important than speed
- You want to verify the reasoning process
- Educational or explanatory purposes

## When NOT to Use

Avoid **deepseek-reasoner** when:
- Simple queries or conversations
- Function calling is required (use deepseek-chat)
- Speed is critical
- Cost optimization is priority
- Reasoning transparency not needed

[END OF DOCUMENT: DEEPSEEK2PGPEC99SP]
---

