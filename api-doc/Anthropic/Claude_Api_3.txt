
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: CLAUDEIFR44RZKZ (sha256-2f4e8fa370e3a00c59301531f6bc5f5e308e608242224de6010a359f2b818b39) | Title: Context-Windows]
[DocID: CLAUDEHXHCDVJWO (sha256-2e01304aa3887896571074c12cd411d98289f8873e39d8d56fc5c07cba19f489) | Title: Cost-Tracking]
[DocID: CLAUDE21IUE5AQ0N (sha256-bca5348a1f372eea1be2b3cc3135b37e8b1747b3da2a3a31a0ac59d81d9c5ab1) | Title: Custom-Tools]
[DocID: CLAUDEH3Q47EH23 (sha256-2be2447ba4db6846bf9d5fd6d08bc504d5b21e1b46e03f16040fe8a27120e889) | Title: Define-Success]
[DocID: CLAUDEO5OP53M9Y (sha256-3dfbecf57c8666e14312e57a1112f4bb837e0721e36e300242890c3adbb9dad3) | Title: Develop-Tests]
[DocID: CLAUDE2I4PXB8T70 (sha256-e741cc0ae4cc06c1f2e76ba1140bc9cadaa8793e6016f4fb3a91ffac136d6a2e) | Title: Embeddings]
[DocID: CLAUDETE7IYFSAY (sha256-4b6ba310170a07966bc42b72037fcdc320c0aff9d2c45a050063e36c9dae565b) | Title: Eval-Tool]
[DocID: CLAUDERXOC43O90 (sha256-47ad24139184bab842bef63a14724e401b055804c9094ec51295265c2bd178b8) | Title: Extended-Thinking-Tips]
[DocID: CLAUDEGK5BSPX3H (sha256-2a7d10ad6bcd43bd32d426026eb438d1d426b32f4a73b24830e513b1ace0d805) | Title: Extended-Thinking]
[DocID: CLAUDECYU6EPKUE (sha256-2145bc0d3bc63061132a0b0d3a10b05069012ea8b4606a702a8ed74612011428) | Title: Files]
--- END OF TOC ---

[START OF DOCUMENT: CLAUDEIFR44RZKZ | Title: Context-Windows]

# Context windows

## Understanding the context window

The "context window" refers to the entirety of the amount of text a language model can look back on and reference when generating new text plus the new text it generates. This is different from the large corpus of data the language model was trained on, and instead represents a "working memory" for the model. A larger context window allows the model to understand and respond to more complex and lengthy prompts, while a smaller context window may limit the model's ability to handle longer prompts or maintain coherence over extended conversations.

The diagram below illustrates the standard context window behavior for API requests<sup>1</sup>:

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=eb9f2edc592262a3c2d498c3bf4e2ed1" alt="Context window diagram" data-og-width="960" width="960" data-og-height="540" height="540" data-path="images/context-window.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=db4d0f21a31307573711b71116ea01b9 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=e6c125e2f420030b1f11bc4fecba581a 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=9ee47a86de4979a05a3c91127170af2a 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=43902ab9eeb100a1141f24a33238c37f 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=e0c2f2b71fdb70b2b5c9bbfbb1a80e44 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window.svg?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=03a77b1bf4bde454fc4bffbd15d88fad 2500w" />

*<sup>1</sup>For chat interfaces, such as for [claude.ai](https://claude.ai/), context windows can also be set up on a rolling "first in, first out" system.*

* **Progressive token accumulation:** As the conversation advances through turns, each user message and assistant response accumulates within the context window. Previous turns are preserved completely.
* **Linear growth pattern:** The context usage grows linearly with each turn, with previous turns preserved completely.
* **200K token capacity:** The total available context window (200,000 tokens) represents the maximum capacity for storing conversation history and generating new output from Claude.
* **Input-output flow:** Each turn consists of:
 * **Input phase:** Contains all previous conversation history plus the current user message
 * **Output phase:** Generates a text response that becomes part of a future input

## The context window with extended thinking

When using [extended thinking](/en/docs/build-with-claude/extended-thinking), all input and output tokens, including the tokens used for thinking, count toward the context window limit, with a few nuances in multi-turn situations.

The thinking budget tokens are a subset of your `max_tokens` parameter, are billed as output tokens, and count towards rate limits.

However, previous thinking blocks are automatically stripped from the context window calculation by the Claude API and are not part of the conversation history that the model "sees" for subsequent turns, preserving token capacity for actual conversation content.

The diagram below demonstrates the specialized token management when extended thinking is enabled:

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=3ad289c01610a87c8ec1214faa09578d" alt="Context window diagram with extended thinking" data-og-width="960" width="960" data-og-height="540" height="540" data-path="images/context-window-thinking.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=a8e00261582812914cb53efe0a936e7a 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=4c0f43f4dd4e6f67c32820de8a7eba04 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=c265f69b5e1dac0f8f000d9f33253093 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=6bfb8b7c35aaf9ad13283a1108b7ec0b 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=de6d6671e549c0a407b5cc1d3cb9b078 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=e41fb48241b7526f815f05125d349f07 2500w" />

* **Stripping extended thinking:** Extended thinking blocks (shown in dark gray) are generated during each turn's output phase, **but are not carried forward as input tokens for subsequent turns**. You do not need to strip the thinking blocks yourself. The Claude API automatically does this for you if you pass them back.
* **Technical implementation details:**
 * The API automatically excludes thinking blocks from previous turns when you pass them back as part of the conversation history.
 * Extended thinking tokens are billed as output tokens only once, during their generation.
 * The effective context window calculation becomes: `context_window = (input_tokens - previous_thinking_tokens) + current_turn_tokens`.
 * Thinking tokens include both `thinking` blocks and `redacted_thinking` blocks.

This architecture is token efficient and allows for extensive reasoning without token waste, as thinking blocks can be substantial in length.

<Note>
 You can read more about the context window and extended thinking in our [extended thinking guide](/en/docs/build-with-claude/extended-thinking).
</Note>

## The context window with extended thinking and tool use

The diagram below illustrates the context window token management when combining extended thinking with tool use:

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=557310c0bf57d88b7a6e550abd35bc75" alt="Context window diagram with extended thinking and tool use" data-og-width="960" width="960" data-og-height="540" height="540" data-path="images/context-window-thinking-tools.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=a086ebd51148723ed500a924fb6c62a6 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=9d97e97afa9bc41f92232cd60044f716 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=65ed1d60de36587470712b2ca5ab4f45 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=174dc8d916dfdf284c86fddb4c9175f3 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=881aaf0622f43065cc942538f88562af 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=fd086e656df4fd7bf8cc2e6584689d58 2500w" />

<Steps>
 <Step title="First turn architecture">
 * **Input components:** Tools configuration and user message
 * **Output components:** Extended thinking + text response + tool use request
 * **Token calculation:** All input and output components count toward the context window, and all output components are billed as output tokens.
 </Step>

 <Step title="Tool result handling (turn 2)">
 * **Input components:** Every block in the first turn as well as the `tool_result`. The extended thinking block **must** be returned with the corresponding tool results. This is the only case wherein you **have to** return thinking blocks.
 * **Output components:** After tool results have been passed back to Claude, Claude will respond with only text (no additional extended thinking until the next `user` message).
 * **Token calculation:** All input and output components count toward the context window, and all output components are billed as output tokens.
 </Step>

 <Step title="Third Step">
 * **Input components:** All inputs and the output from the previous turn is carried forward with the exception of the thinking block, which can be dropped now that Claude has completed the entire tool use cycle. The API will automatically strip the thinking block for you if you pass it back, or you can feel free to strip it yourself at this stage. This is also where you would add the next `User` turn.
 * **Output components:** Since there is a new `User` turn outside of the tool use cycle, Claude will generate a new extended thinking block and continue from there.
 * **Token calculation:** Previous thinking tokens are automatically stripped from context window calculations. All other previous blocks still count as part of the token window, and the thinking block in the current `Assistant` turn counts as part of the context window.
 </Step>
</Steps>

* **Considerations for tool use with extended thinking:**
 * When posting tool results, the entire unmodified thinking block that accompanies that specific tool request (including signature/redacted portions) must be included.
 * The effective context window calculation for extended thinking with tool use becomes: `context_window = input_tokens + current_turn_tokens`.
 * The system uses cryptographic signatures to verify thinking block authenticity. Failing to preserve thinking blocks during tool use can break Claude's reasoning continuity. Thus, if you modify thinking blocks, the API will return an error.

<Note>
 Claude 4 models support [interleaved thinking](/en/docs/build-with-claude/extended-thinking#interleaved-thinking), which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.

 Claude Sonnet 3.7 does not support interleaved thinking, so there is no interleaving of extended thinking and tool calls without a non-`tool_result` user turn in between.

 For more information about using tools with extended thinking, see our [extended thinking guide](/en/docs/build-with-claude/extended-thinking#extended-thinking-with-tool-use).
</Note>

## 1M token context window

Claude Sonnet 4 and 4.5 support a 1-million token context window. This extended context window allows you to process much larger documents, maintain longer conversations, and work with more extensive codebases.

<Note>
 The 1M token context window is currently in beta for organizations in [usage tier](/en/api/rate-limits) 4 and organizations with custom rate limits. The 1M token context window is only available for Claude Sonnet 4 and Sonnet 4.5.
</Note>

To use the 1M token context window, include the `context-1m-2025-08-07` [beta header](/en/api/beta-headers) in your API requests:

<CodeGroup>```python Python theme={null}
  from anthropic import Anthropic

  client = Anthropic()

  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {"role": "user", "content": "Process this large document..."}
      ],
      betas=["context-1m-2025-08-07"]
  )
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  const msg = await anthropic.beta.messages.create({
    model: 'claude-sonnet-4-5',
    max_tokens: 1024,
    messages: [
      { role: 'user', content: 'Process this large document...' }
    ],
    betas: ['context-1m-2025-08-07']
  });
  ``````curl cURL theme={null}
  curl https://api.anthropic.com/v1/messages \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: context-1m-2025-08-07" \
    -H "content-type: application/json" \
    -d '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
        {"role": "user", "content": "Process this large document..."}
      ]
    }'
  ```</CodeGroup>

**Important considerations:**

* **Beta status**: This is a beta feature subject to change. Features and pricing may be modified or removed in future releases.
* **Usage tier requirement**: The 1M token context window is available to organizations in [usage tier](/en/api/rate-limits) 4 and organizations with custom rate limits. Lower tier organizations must advance to usage tier 4 to access this feature.
* **Availability**: The 1M token context window is currently available on the Claude API, [Amazon Bedrock](/en/docs/build-with-claude/claude-on-amazon-bedrock), and [Google Cloud's Vertex AI](/en/docs/build-with-claude/claude-on-vertex-ai).
* **Pricing**: Requests exceeding 200K tokens are automatically charged at premium rates (2x input, 1.5x output pricing). See the [pricing documentation](/en/docs/about-claude/pricing#long-context-pricing) for details.
* **Rate limits**: Long context requests have dedicated rate limits. See the [rate limits documentation](/en/api/rate-limits#long-context-rate-limits) for details.
* **Multimodal considerations**: When processing large numbers of images or pdfs, be aware that the files can vary in token usage. When pairing a large prompt with a large number of images, you may hit [request size limits](/en/api/overview#request-size-limits).

## Context awareness in Claude Sonnet 4.5 and Haiku 4.5

Claude Sonnet 4.5 and Claude Haiku 4.5 feature **context awareness**, enabling these models to track their remaining context window (i.e. "token budget") throughout a conversation. This enables Claude to execute tasks and manage context more effectively by understanding how much space it has to work. Claude is natively trained to use this context precisely to persist in the task until the very end, rather than having to guess how many tokens are remaining. For a model, lacking context awareness is like competing in a cooking show without a clock. Claude 4.5 models change this by explicitly informing the model about its remaining context, so it can take maximum advantage of the available tokens.

**How it works:**

At the start of a conversation, Claude receives information about its total context window:```
<budget:token_budget>200000</budget:token_budget>
```The budget is set to 200K tokens (standard), 500K tokens (Claude.ai Enterprise), or 1M tokens (beta, for eligible organizations).

After each tool call, Claude receives an update on remaining capacity:```
<system_warning>Token usage: 35000/200000; 165000 remaining</system_warning>
```This awareness helps Claude determine how much capacity remains for work and enables more effective execution on long-running tasks. Image tokens are included in these budgets.

**Benefits:**

Context awareness is particularly valuable for:

* Long-running agent sessions that require sustained focus
* Multi-context-window workflows where state transitions matter
* Complex tasks requiring careful token management

For prompting guidance on leveraging context awareness, see our [Claude 4 best practices guide](/en/docs/build-with-claude/prompt-engineering/claude-4-best-practices#context-awareness-and-multi-window-workflows).

## Context window management with newer Claude models

In newer Claude models (starting with Claude Sonnet 3.7), if the sum of prompt tokens and output tokens exceeds the model's context window, the system will return a validation error rather than silently truncating the context. This change provides more predictable behavior but requires more careful token management.

To plan your token usage and ensure you stay within context window limits, you can use the [token counting API](/en/docs/build-with-claude/token-counting) to estimate how many tokens your messages will use before sending them to Claude.

See our [model comparison](/en/docs/about-claude/models/overview#model-comparison-table) table for a list of context window sizes by model.

# Next steps

<CardGroup cols={2}>
 <Card title="Model comparison table" icon="scale-balanced" href="/en/docs/about-claude/models/overview#model-comparison-table">
 See our model comparison table for a list of context window sizes and input / output token pricing by model.
 </Card>

 <Card title="Extended thinking overview" icon="head-side-gear" href="/en/docs/build-with-claude/extended-thinking">
 Learn more about how extended thinking works and how to implement it alongside other features such as tool use and prompt caching.
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDEIFR44RZKZ]
---

[START OF DOCUMENT: CLAUDEHXHCDVJWO | Title: Cost-Tracking]

# Tracking Costs and Usage

> Understand and track token usage for billing in the Claude Agent SDK

# SDK Cost Tracking

The Claude Agent SDK provides detailed token usage information for each interaction with Claude. This guide explains how to properly track costs and understand usage reporting, especially when dealing with parallel tool uses and multi-step conversations.

For complete API documentation, see the [TypeScript SDK reference](/en/docs/agent-sdk/typescript).

## Understanding Token Usage

When Claude processes requests, it reports token usage at the message level. This usage data is essential for tracking costs and billing users appropriately.

### Key Concepts

1. **Steps**: A step is a single request/response pair between your application and Claude
2. **Messages**: Individual messages within a step (text, tool uses, tool results)
3. **Usage**: Token consumption data attached to assistant messages

## Usage Reporting Structure

### Single vs Parallel Tool Use

When Claude executes tools, the usage reporting differs based on whether tools are executed sequentially or in parallel:

<CodeGroup>```typescript TypeScript theme={null}
  import { query } from "@anthropic-ai/claude-agent-sdk";

  // Example: Tracking usage in a conversation
  const result = await query({
    prompt: "Analyze this codebase and run tests",
    options: {
      onMessage: (message) => {
        if (message.type === 'assistant' && message.usage) {
          console.log(`Message ID: ${message.id}`);
          console.log(`Usage:`, message.usage);
        }
      }
    }
  });
  ``````python Python theme={null}
  from claude_agent_sdk import query, ClaudeAgentOptions, AssistantMessage
  import asyncio

  # Example: Tracking usage in a conversation
  async def track_usage():
      # Process messages as they arrive
      async for message in query(
          prompt="Analyze this codebase and run tests"
      ):
          if isinstance(message, AssistantMessage) and hasattr(message, 'usage'):
              print(f"Message ID: {message.id}")
              print(f"Usage: {message.usage}")

  asyncio.run(track_usage())
  ```</CodeGroup>

### Message Flow Example

Here's how messages and usage are reported in a typical multi-step conversation:```
<!-- Step 1: Initial request with parallel tool uses -->
assistant (text)      { id: "msg_1", usage: { output_tokens: 100, ... } }
assistant (tool_use)  { id: "msg_1", usage: { output_tokens: 100, ... } }
assistant (tool_use)  { id: "msg_1", usage: { output_tokens: 100, ... } }
assistant (tool_use)  { id: "msg_1", usage: { output_tokens: 100, ... } }
user (tool_result)
user (tool_result)
user (tool_result)

<!-- Step 2: Follow-up response -->
assistant (text)      { id: "msg_2", usage: { output_tokens: 98, ... } }
```## Important Usage Rules

### 1. Same ID = Same Usage

**All messages with the same `id` field report identical usage**. When Claude sends multiple messages in the same turn (e.g., text + tool uses), they share the same message ID and usage data.```typescript  theme={null}
// All these messages have the same ID and usage
const messages = [
  { type: 'assistant', id: 'msg_123', usage: { output_tokens: 100 } },
  { type: 'assistant', id: 'msg_123', usage: { output_tokens: 100 } },
  { type: 'assistant', id: 'msg_123', usage: { output_tokens: 100 } }
];

// Charge only once per unique message ID
const uniqueUsage = messages[0].usage; // Same for all messages with this ID
```### 2. Charge Once Per Step

**You should only charge users once per step**, not for each individual message. When you see multiple assistant messages with the same ID, use the usage from any one of them.

### 3. Result Message Contains Cumulative Usage

The final `result` message contains the total cumulative usage from all steps in the conversation:```typescript  theme={null}
// Final result includes total usage
const result = await query({
  prompt: "Multi-step task",
  options: { /* ... */ }
});

console.log("Total usage:", result.usage);
console.log("Total cost:", result.usage.total_cost_usd);
```## Implementation: Cost Tracking System

Here's a complete example of implementing a cost tracking system:

<CodeGroup>```typescript TypeScript theme={null}
  import { query } from "@anthropic-ai/claude-agent-sdk";

  class CostTracker {
    private processedMessageIds = new Set<string>();
    private stepUsages: Array<any> = [];
    
    async trackConversation(prompt: string) {
      const result = await query({
        prompt,
        options: {
          onMessage: (message) => {
            this.processMessage(message);
          }
        }
      });
      
      return {
        result,
        stepUsages: this.stepUsages,
        totalCost: result.usage?.total_cost_usd || 0
      };
    }
    
    private processMessage(message: any) {
      // Only process assistant messages with usage
      if (message.type !== 'assistant' || !message.usage) {
        return;
      }
      
      // Skip if we've already processed this message ID
      if (this.processedMessageIds.has(message.id)) {
        return;
      }
      
      // Mark as processed and record usage
      this.processedMessageIds.add(message.id);
      this.stepUsages.push({
        messageId: message.id,
        timestamp: new Date().toISOString(),
        usage: message.usage,
        costUSD: this.calculateCost(message.usage)
      });
    }
    
    private calculateCost(usage: any): number {
      // Implement your pricing calculation here
      // This is a simplified example
      const inputCost = usage.input_tokens * 0.00003;
      const outputCost = usage.output_tokens * 0.00015;
      const cacheReadCost = (usage.cache_read_input_tokens || 0) * 0.0000075;
      
      return inputCost + outputCost + cacheReadCost;
    }
  }

  // Usage
  const tracker = new CostTracker();
  const { result, stepUsages, totalCost } = await tracker.trackConversation(
    "Analyze and refactor this code"
  );

  console.log(`Steps processed: ${stepUsages.length}`);
  console.log(`Total cost: $${totalCost.toFixed(4)}`);
  ``````python Python theme={null}
  from claude_agent_sdk import query, AssistantMessage, ResultMessage
  from datetime import datetime
  import asyncio

  class CostTracker:
      def __init__(self):
          self.processed_message_ids = set()
          self.step_usages = []

      async def track_conversation(self, prompt):
          result = None

          # Process messages as they arrive
          async for message in query(prompt=prompt):
              self.process_message(message)

              # Capture the final result message
              if isinstance(message, ResultMessage):
                  result = message

          return {
              "result": result,
              "step_usages": self.step_usages,
              "total_cost": result.total_cost_usd if result else 0
          }

      def process_message(self, message):
          # Only process assistant messages with usage
          if not isinstance(message, AssistantMessage) or not hasattr(message, 'usage'):
              return

          # Skip if already processed this message ID
          message_id = getattr(message, 'id', None)
          if not message_id or message_id in self.processed_message_ids:
              return

          # Mark as processed and record usage
          self.processed_message_ids.add(message_id)
          self.step_usages.append({
              "message_id": message_id,
              "timestamp": datetime.now().isoformat(),
              "usage": message.usage,
              "cost_usd": self.calculate_cost(message.usage)
          })

      def calculate_cost(self, usage):
          # Implement your pricing calculation
          input_cost = usage.get("input_tokens", 0) * 0.00003
          output_cost = usage.get("output_tokens", 0) * 0.00015
          cache_read_cost = usage.get("cache_read_input_tokens", 0) * 0.0000075

          return input_cost + output_cost + cache_read_cost

  # Usage
  async def main():
      tracker = CostTracker()
      result = await tracker.track_conversation("Analyze and refactor this code")

      print(f"Steps processed: {len(result['step_usages'])}")
      print(f"Total cost: ${result['total_cost']:.4f}")

  asyncio.run(main())
  ```</CodeGroup>

## Handling Edge Cases

### Output Token Discrepancies

In rare cases, you might observe different `output_tokens` values for messages with the same ID. When this occurs:

1. **Use the highest value** - The final message in a group typically contains the accurate total
2. **Verify against total cost** - The `total_cost_usd` in the result message is authoritative
3. **Report inconsistencies** - File issues at the [Claude Code GitHub repository](https://github.com/anthropics/claude-code/issues)

### Cache Token Tracking

When using prompt caching, track these token types separately:```typescript  theme={null}
interface CacheUsage {
  cache_creation_input_tokens: number;
  cache_read_input_tokens: number;
  cache_creation: {
    ephemeral_5m_input_tokens: number;
    ephemeral_1h_input_tokens: number;
  };
}
```## Best Practices

1. **Use Message IDs for Deduplication**: Always track processed message IDs to avoid double-charging
2. **Monitor the Result Message**: The final result contains authoritative cumulative usage
3. **Implement Logging**: Log all usage data for auditing and debugging
4. **Handle Failures Gracefully**: Track partial usage even if a conversation fails
5. **Consider Streaming**: For streaming responses, accumulate usage as messages arrive

## Usage Fields Reference

Each usage object contains:

* `input_tokens`: Base input tokens processed
* `output_tokens`: Tokens generated in the response
* `cache_creation_input_tokens`: Tokens used to create cache entries
* `cache_read_input_tokens`: Tokens read from cache
* `service_tier`: The service tier used (e.g., "standard")
* `total_cost_usd`: Total cost in USD (only in result message)

## Example: Building a Billing Dashboard

Here's how to aggregate usage data for a billing dashboard:```typescript  theme={null}
class BillingAggregator {
  private userUsage = new Map<string, {
    totalTokens: number;
    totalCost: number;
    conversations: number;
  }>();
  
  async processUserRequest(userId: string, prompt: string) {
    const tracker = new CostTracker();
    const { result, stepUsages, totalCost } = await tracker.trackConversation(prompt);
    
    // Update user totals
    const current = this.userUsage.get(userId) || {
      totalTokens: 0,
      totalCost: 0,
      conversations: 0
    };
    
    const totalTokens = stepUsages.reduce((sum, step) => 
      sum + step.usage.input_tokens + step.usage.output_tokens, 0
    );
    
    this.userUsage.set(userId, {
      totalTokens: current.totalTokens + totalTokens,
      totalCost: current.totalCost + totalCost,
      conversations: current.conversations + 1
    });
    
    return result;
  }
  
  getUserBilling(userId: string) {
    return this.userUsage.get(userId) || {
      totalTokens: 0,
      totalCost: 0,
      conversations: 0
    };
  }
}
```## Related Documentation

* [TypeScript SDK Reference](/en/docs/agent-sdk/typescript) - Complete API documentation
* [SDK Overview](/en/docs/agent-sdk/overview) - Getting started with the SDK
* [SDK Permissions](/en/docs/agent-sdk/permissions) - Managing tool permissions

[END OF DOCUMENT: CLAUDEHXHCDVJWO]
---

[START OF DOCUMENT: CLAUDE21IUE5AQ0N | Title: Custom-Tools]

# Custom Tools

> Build and integrate custom tools to extend Claude Agent SDK functionality

Custom tools allow you to extend Claude Code's capabilities with your own functionality through in-process MCP servers, enabling Claude to interact with external services, APIs, or perform specialized operations.

## Creating Custom Tools

Use the `createSdkMcpServer` and `tool` helper functions to define type-safe custom tools:

<CodeGroup>```typescript TypeScript theme={null}
  import { query, tool, createSdkMcpServer } from "@anthropic-ai/claude-agent-sdk";
  import { z } from "zod";

  // Create an SDK MCP server with custom tools
  const customServer = createSdkMcpServer({
    name: "my-custom-tools",
    version: "1.0.0",
    tools: [
      tool(
        "get_weather",
        "Get current temperature for a location using coordinates",
        {
          latitude: z.number().describe("Latitude coordinate"),
          longitude: z.number().describe("Longitude coordinate")
        },
        async (args) => {
          const response = await fetch(`https://api.open-meteo.com/v1/forecast?latitude=${args.latitude}&longitude=${args.longitude}&current=temperature_2m&temperature_unit=fahrenheit`);
          const data = await response.json();

          return {
            content: [{
              type: "text",
              text: `Temperature: ${data.current.temperature_2m}°F`
            }]
          };
        }
      )
    ]
  });
  ``````python Python theme={null}
  from claude_agent_sdk import tool, create_sdk_mcp_server, ClaudeSDKClient, ClaudeAgentOptions
  from typing import Any
  import aiohttp

  # Define a custom tool using the @tool decorator
  @tool("get_weather", "Get current temperature for a location using coordinates", {"latitude": float, "longitude": float})
  async def get_weather(args: dict[str, Any]) -> dict[str, Any]:
      # Call weather API
      async with aiohttp.ClientSession() as session:
          async with session.get(
              f"https://api.open-meteo.com/v1/forecast?latitude={args['latitude']}&longitude={args['longitude']}&current=temperature_2m&temperature_unit=fahrenheit"
          ) as response:
              data = await response.json()

      return {
          "content": [{
              "type": "text",
              "text": f"Temperature: {data['current']['temperature_2m']}°F"
          }]
      }

  # Create an SDK MCP server with the custom tool
  custom_server = create_sdk_mcp_server(
      name="my-custom-tools",
      version="1.0.0",
      tools=[get_weather]  # Pass the decorated function
  )
  ```</CodeGroup>

## Using Custom Tools

Pass the custom server to the `query` function via the `mcpServers` option as a dictionary/object.

<Note>
 **Important:** Custom MCP tools require streaming input mode. You must use an async generator/iterable for the `prompt` parameter - a simple string will not work with MCP servers.
</Note>

### Tool Name Format

When MCP tools are exposed to Claude, their names follow a specific format:

* Pattern: `mcp__{server_name}__{tool_name}`
* Example: A tool named `get_weather` in server `my-custom-tools` becomes `mcp__my-custom-tools__get_weather`

### Configuring Allowed Tools

You can control which tools Claude can use via the `allowedTools` option:

<CodeGroup>```typescript TypeScript theme={null}
  import { query } from "@anthropic-ai/claude-agent-sdk";

  // Use the custom tools in your query with streaming input
  async function* generateMessages() {
    yield {
      type: "user" as const,
      message: {
        role: "user" as const,
        content: "What's the weather in San Francisco?"
      }
    };
  }

  for await (const message of query({
    prompt: generateMessages(),  // Use async generator for streaming input
    options: {
      mcpServers: {
        "my-custom-tools": customServer  // Pass as object/dictionary, not array
      },
      // Optionally specify which tools Claude can use
      allowedTools: [
        "mcp__my-custom-tools__get_weather",  // Allow the weather tool
        // Add other tools as needed
      ],
      maxTurns: 3
    }
  })) {
    if (message.type === "result" && message.subtype === "success") {
      console.log(message.result);
    }
  }
  ``````python Python theme={null}
  from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions
  import asyncio

  # Use the custom tools with Claude
  options = ClaudeAgentOptions(
      mcp_servers={"my-custom-tools": custom_server},
      allowed_tools=[
          "mcp__my-custom-tools__get_weather",  # Allow the weather tool
          # Add other tools as needed
      ]
  )

  async def main():
      async with ClaudeSDKClient(options=options) as client:
          await client.query("What's the weather in San Francisco?")

          # Extract and print response
          async for msg in client.receive_response():
              print(msg)

  asyncio.run(main())
  ```</CodeGroup>

### Multiple Tools Example

When your MCP server has multiple tools, you can selectively allow them:

<CodeGroup>```typescript TypeScript theme={null}
  const multiToolServer = createSdkMcpServer({
    name: "utilities",
    version: "1.0.0",
    tools: [
      tool("calculate", "Perform calculations", { /* ... */ }, async (args) => { /* ... */ }),
      tool("translate", "Translate text", { /* ... */ }, async (args) => { /* ... */ }),
      tool("search_web", "Search the web", { /* ... */ }, async (args) => { /* ... */ })
    ]
  });

  // Allow only specific tools with streaming input
  async function* generateMessages() {
    yield {
      type: "user" as const,
      message: {
        role: "user" as const,
        content: "Calculate 5 + 3 and translate 'hello' to Spanish"
      }
    };
  }

  for await (const message of query({
    prompt: generateMessages(),  // Use async generator for streaming input
    options: {
      mcpServers: {
        utilities: multiToolServer
      },
      allowedTools: [
        "mcp__utilities__calculate",   // Allow calculator
        "mcp__utilities__translate",   // Allow translator
        // "mcp__utilities__search_web" is NOT allowed
      ]
    }
  })) {
    // Process messages
  }
  ``````python Python theme={null}
  from claude_agent_sdk import ClaudeSDKClient, ClaudeAgentOptions, tool, create_sdk_mcp_server
  from typing import Any
  import asyncio

  # Define multiple tools using the @tool decorator
  @tool("calculate", "Perform calculations", {"expression": str})
  async def calculate(args: dict[str, Any]) -> dict[str, Any]:
      result = eval(args["expression"])  # Use safe eval in production
      return {"content": [{"type": "text", "text": f"Result: {result}"}]}

  @tool("translate", "Translate text", {"text": str, "target_lang": str})
  async def translate(args: dict[str, Any]) -> dict[str, Any]:
      # Translation logic here
      return {"content": [{"type": "text", "text": f"Translated: {args['text']}"}]}

  @tool("search_web", "Search the web", {"query": str})
  async def search_web(args: dict[str, Any]) -> dict[str, Any]:
      # Search logic here
      return {"content": [{"type": "text", "text": f"Search results for: {args['query']}"}]}

  multi_tool_server = create_sdk_mcp_server(
      name="utilities",
      version="1.0.0",
      tools=[calculate, translate, search_web]  # Pass decorated functions
  )

  # Allow only specific tools with streaming input
  async def message_generator():
      yield {
          "type": "user",
          "message": {
              "role": "user",
              "content": "Calculate 5 + 3 and translate 'hello' to Spanish"
          }
      }

  async for message in query(
      prompt=message_generator(),  # Use async generator for streaming input
      options=ClaudeAgentOptions(
          mcp_servers={"utilities": multi_tool_server},
          allowed_tools=[
              "mcp__utilities__calculate",   # Allow calculator
              "mcp__utilities__translate",   # Allow translator
              # "mcp__utilities__search_web" is NOT allowed
          ]
      )
  ):
      if hasattr(message, 'result'):
          print(message.result)
  ```</CodeGroup>

## Type Safety with Python

The `@tool` decorator supports various schema definition approaches for type safety:

<CodeGroup>```typescript TypeScript theme={null}
  import { z } from "zod";

  tool(
    "process_data",
    "Process structured data with type safety",
    {
      // Zod schema defines both runtime validation and TypeScript types
      data: z.object({
        name: z.string(),
        age: z.number().min(0).max(150),
        email: z.string().email(),
        preferences: z.array(z.string()).optional()
      }),
      format: z.enum(["json", "csv", "xml"]).default("json")
    },
    async (args) => {
      // args is fully typed based on the schema
      // TypeScript knows: args.data.name is string, args.data.age is number, etc.
      console.log(`Processing ${args.data.name}'s data as ${args.format}`);
      
      // Your processing logic here
      return {
        content: [{
          type: "text",
          text: `Processed data for ${args.data.name}`
        }]
      };
    }
  )
  ``````python Python theme={null}
  from typing import Any

  # Simple type mapping - recommended for most cases
  @tool(
      "process_data",
      "Process structured data with type safety",
      {
          "name": str,
          "age": int,
          "email": str,
          "preferences": list  # Optional parameters can be handled in the function
      }
  )
  async def process_data(args: dict[str, Any]) -> dict[str, Any]:
      # Access arguments with type hints for IDE support
      name = args["name"]
      age = args["age"]
      email = args["email"]
      preferences = args.get("preferences", [])
      
      print(f"Processing {name}'s data (age: {age})")
      
      return {
          "content": [{
              "type": "text",
              "text": f"Processed data for {name}"
          }]
      }

  # For more complex schemas, you can use JSON Schema format
  @tool(
      "advanced_process",
      "Process data with advanced validation",
      {
          "type": "object",
          "properties": {
              "name": {"type": "string"},
              "age": {"type": "integer", "minimum": 0, "maximum": 150},
              "email": {"type": "string", "format": "email"},
              "format": {"type": "string", "enum": ["json", "csv", "xml"], "default": "json"}
          },
          "required": ["name", "age", "email"]
      }
  )
  async def advanced_process(args: dict[str, Any]) -> dict[str, Any]:
      # Process with advanced schema validation
      return {
          "content": [{
              "type": "text",
              "text": f"Advanced processing for {args['name']}"
          }]
      }
  ```</CodeGroup>

## Error Handling

Handle errors gracefully to provide meaningful feedback:

<CodeGroup>```typescript TypeScript theme={null}
  tool(
    "fetch_data",
    "Fetch data from an API",
    {
      endpoint: z.string().url().describe("API endpoint URL")
    },
    async (args) => {
      try {
        const response = await fetch(args.endpoint);
        
        if (!response.ok) {
          return {
            content: [{
              type: "text",
              text: `API error: ${response.status} ${response.statusText}`
            }]
          };
        }
        
        const data = await response.json();
        return {
          content: [{
            type: "text",
            text: JSON.stringify(data, null, 2)
          }]
        };
      } catch (error) {
        return {
          content: [{
            type: "text",
            text: `Failed to fetch data: ${error.message}`
          }]
        };
      }
    }
  )
  ``````python Python theme={null}
  import json
  import aiohttp
  from typing import Any

  @tool(
      "fetch_data",
      "Fetch data from an API",
      {"endpoint": str}  # Simple schema
  )
  async def fetch_data(args: dict[str, Any]) -> dict[str, Any]:
      try:
          async with aiohttp.ClientSession() as session:
              async with session.get(args["endpoint"]) as response:
                  if response.status != 200:
                      return {
                          "content": [{
                              "type": "text",
                              "text": f"API error: {response.status} {response.reason}"
                          }]
                      }
                  
                  data = await response.json()
                  return {
                      "content": [{
                          "type": "text",
                          "text": json.dumps(data, indent=2)
                      }]
                  }
      except Exception as e:
          return {
              "content": [{
                  "type": "text",
                  "text": f"Failed to fetch data: {str(e)}"
              }]
          }
  ```</CodeGroup>

## Example Tools

### Database Query Tool

<CodeGroup>```typescript TypeScript theme={null}
  const databaseServer = createSdkMcpServer({
    name: "database-tools",
    version: "1.0.0",
    tools: [
      tool(
        "query_database",
        "Execute a database query",
        {
          query: z.string().describe("SQL query to execute"),
          params: z.array(z.any()).optional().describe("Query parameters")
        },
        async (args) => {
          const results = await db.query(args.query, args.params || []);
          return {
            content: [{
              type: "text",
              text: `Found ${results.length} rows:\n${JSON.stringify(results, null, 2)}`
            }]
          };
        }
      )
    ]
  });
  ``````python Python theme={null}
  from typing import Any
  import json

  @tool(
      "query_database",
      "Execute a database query",
      {"query": str, "params": list}  # Simple schema with list type
  )
  async def query_database(args: dict[str, Any]) -> dict[str, Any]:
      results = await db.query(args["query"], args.get("params", []))
      return {
          "content": [{
              "type": "text",
              "text": f"Found {len(results)} rows:\n{json.dumps(results, indent=2)}"
          }]
      }

  database_server = create_sdk_mcp_server(
      name="database-tools",
      version="1.0.0",
      tools=[query_database]  # Pass the decorated function
  )
  ```</CodeGroup>

### API Gateway Tool

<CodeGroup>```typescript TypeScript theme={null}
  const apiGatewayServer = createSdkMcpServer({
    name: "api-gateway",
    version: "1.0.0",
    tools: [
      tool(
        "api_request",
        "Make authenticated API requests to external services",
        {
          service: z.enum(["stripe", "github", "openai", "slack"]).describe("Service to call"),
          endpoint: z.string().describe("API endpoint path"),
          method: z.enum(["GET", "POST", "PUT", "DELETE"]).describe("HTTP method"),
          body: z.record(z.any()).optional().describe("Request body"),
          query: z.record(z.string()).optional().describe("Query parameters")
        },
        async (args) => {
          const config = {
            stripe: { baseUrl: "https://api.stripe.com/v1", key: process.env.STRIPE_KEY },
            github: { baseUrl: "https://api.github.com", key: process.env.GITHUB_TOKEN },
            openai: { baseUrl: "https://api.openai.com/v1", key: process.env.OPENAI_KEY },
            slack: { baseUrl: "https://slack.com/api", key: process.env.SLACK_TOKEN }
          };
          
          const { baseUrl, key } = config[args.service];
          const url = new URL(`${baseUrl}${args.endpoint}`);
          
          if (args.query) {
            Object.entries(args.query).forEach(([k, v]) => url.searchParams.set(k, v));
          }
          
          const response = await fetch(url, {
            method: args.method,
            headers: { Authorization: `Bearer ${key}`, "Content-Type": "application/json" },
            body: args.body ? JSON.stringify(args.body) : undefined
          });
          
          const data = await response.json();
          return {
            content: [{
              type: "text",
              text: JSON.stringify(data, null, 2)
            }]
          };
        }
      )
    ]
  });
  ``````python Python theme={null}
  import os
  import json
  import aiohttp
  from typing import Any

  # For complex schemas with enums, use JSON Schema format
  @tool(
      "api_request",
      "Make authenticated API requests to external services",
      {
          "type": "object",
          "properties": {
              "service": {"type": "string", "enum": ["stripe", "github", "openai", "slack"]},
              "endpoint": {"type": "string"},
              "method": {"type": "string", "enum": ["GET", "POST", "PUT", "DELETE"]},
              "body": {"type": "object"},
              "query": {"type": "object"}
          },
          "required": ["service", "endpoint", "method"]
      }
  )
  async def api_request(args: dict[str, Any]) -> dict[str, Any]:
      config = {
          "stripe": {"base_url": "https://api.stripe.com/v1", "key": os.environ["STRIPE_KEY"]},
          "github": {"base_url": "https://api.github.com", "key": os.environ["GITHUB_TOKEN"]},
          "openai": {"base_url": "https://api.openai.com/v1", "key": os.environ["OPENAI_KEY"]},
          "slack": {"base_url": "https://slack.com/api", "key": os.environ["SLACK_TOKEN"]}
      }
      
      service_config = config[args["service"]]
      url = f"{service_config['base_url']}{args['endpoint']}"
      
      if args.get("query"):
          params = "&".join([f"{k}={v}" for k, v in args["query"].items()])
          url += f"?{params}"
      
      headers = {"Authorization": f"Bearer {service_config['key']}", "Content-Type": "application/json"}
      
      async with aiohttp.ClientSession() as session:
          async with session.request(
              args["method"], url, headers=headers, json=args.get("body")
          ) as response:
              data = await response.json()
              return {
                  "content": [{
                      "type": "text",
                      "text": json.dumps(data, indent=2)
                  }]
              }

  api_gateway_server = create_sdk_mcp_server(
      name="api-gateway",
      version="1.0.0",
      tools=[api_request]  # Pass the decorated function
  )
  ```</CodeGroup>

### Calculator Tool

<CodeGroup>```typescript TypeScript theme={null}
  const calculatorServer = createSdkMcpServer({
    name: "calculator",
    version: "1.0.0",
    tools: [
      tool(
        "calculate",
        "Perform mathematical calculations",
        {
          expression: z.string().describe("Mathematical expression to evaluate"),
          precision: z.number().optional().default(2).describe("Decimal precision")
        },
        async (args) => {
          try {
            // Use a safe math evaluation library in production
            const result = eval(args.expression); // Example only!
            const formatted = Number(result).toFixed(args.precision);
            
            return {
              content: [{
                type: "text",
                text: `${args.expression} = ${formatted}`
              }]
            };
          } catch (error) {
            return {
              content: [{
                type: "text",
                text: `Error: Invalid expression - ${error.message}`
              }]
            };
          }
        }
      ),
      tool(
        "compound_interest",
        "Calculate compound interest for an investment",
        {
          principal: z.number().positive().describe("Initial investment amount"),
          rate: z.number().describe("Annual interest rate (as decimal, e.g., 0.05 for 5%)"),
          time: z.number().positive().describe("Investment period in years"),
          n: z.number().positive().default(12).describe("Compounding frequency per year")
        },
        async (args) => {
          const amount = args.principal * Math.pow(1 + args.rate / args.n, args.n * args.time);
          const interest = amount - args.principal;
          
          return {
            content: [{
              type: "text",
              text: `Investment Analysis:\n` +
                    `Principal: $${args.principal.toFixed(2)}\n` +
                    `Rate: ${(args.rate * 100).toFixed(2)}%\n` +
                    `Time: ${args.time} years\n` +
                    `Compounding: ${args.n} times per year\n\n` +
                    `Final Amount: $${amount.toFixed(2)}\n` +
                    `Interest Earned: $${interest.toFixed(2)}\n` +
                    `Return: ${((interest / args.principal) * 100).toFixed(2)}%`
            }]
          };
        }
      )
    ]
  });
  ``````python Python theme={null}
  import math
  from typing import Any

  @tool(
      "calculate",
      "Perform mathematical calculations",
      {"expression": str, "precision": int}  # Simple schema
  )
  async def calculate(args: dict[str, Any]) -> dict[str, Any]:
      try:
          # Use a safe math evaluation library in production
          result = eval(args["expression"], {"__builtins__": {}})
          precision = args.get("precision", 2)
          formatted = round(result, precision)
          
          return {
              "content": [{
                  "type": "text",
                  "text": f"{args['expression']} = {formatted}"
              }]
          }
      except Exception as e:
          return {
              "content": [{
                  "type": "text",
                  "text": f"Error: Invalid expression - {str(e)}"
              }]
          }

  @tool(
      "compound_interest",
      "Calculate compound interest for an investment",
      {"principal": float, "rate": float, "time": float, "n": int}
  )
  async def compound_interest(args: dict[str, Any]) -> dict[str, Any]:
      principal = args["principal"]
      rate = args["rate"]
      time = args["time"]
      n = args.get("n", 12)
      
      amount = principal * (1 + rate / n) ** (n * time)
      interest = amount - principal
      
      return {
          "content": [{
              "type": "text",
              "text": f"""Investment Analysis:
  Principal: ${principal:.2f}
  Rate: {rate * 100:.2f}%
  Time: {time} years
  Compounding: {n} times per year

  Final Amount: ${amount:.2f}
  Interest Earned: ${interest:.2f}
  Return: {(interest / principal) * 100:.2f}%"""
          }]
      }

  calculator_server = create_sdk_mcp_server(
      name="calculator",
      version="1.0.0",
      tools=[calculate, compound_interest]  # Pass decorated functions
  )
  ```</CodeGroup>

## Related Documentation

* [TypeScript SDK Reference](/en/docs/agent-sdk/typescript)
* [Python SDK Reference](/en/docs/agent-sdk/python)
* [MCP Documentation](https://modelcontextprotocol.io)
* [SDK Overview](/en/docs/agent-sdk/overview)

[END OF DOCUMENT: CLAUDE21IUE5AQ0N]
---

[START OF DOCUMENT: CLAUDEH3Q47EH23 | Title: Define-Success]

# Define your success criteria

Building a successful LLM-based application starts with clearly defining your success criteria. How will you know when your application is good enough to publish?

Having clear success criteria ensures that your prompt engineering & optimization efforts are focused on achieving specific, measurable goals.

***

## Building strong criteria

Good success criteria are:

* **Specific**: Clearly define what you want to achieve. Instead of "good performance," specify "accurate sentiment classification."
* **Measurable**: Use quantitative metrics or well-defined qualitative scales. Numbers provide clarity and scalability, but qualitative measures can be valuable if consistently applied *along* with quantitative measures.

 * Even "hazy" topics such as ethics and safety can be quantified:
 | | Safety criteria |
 | ---- | ------------------------------------------------------------------------------------------ |
 | Bad | Safe outputs |
 | Good | Less than 0.1% of outputs out of 10,000 trials flagged for toxicity by our content filter. |

 <Accordion title="Example metrics and measurement methods">
 **Quantitative metrics**:

 * Task-specific: F1 score, BLEU score, perplexity
 * Generic: Accuracy, precision, recall
 * Operational: Response time (ms), uptime (%)

 **Quantitative methods**:

 * A/B testing: Compare performance against a baseline model or earlier version.
 * User feedback: Implicit measures like task completion rates.
 * Edge case analysis: Percentage of edge cases handled without errors.

 **Qualitative scales**:

 * Likert scales: "Rate coherence from 1 (nonsensical) to 5 (perfectly logical)"
 * Expert rubrics: Linguists rating translation quality on defined criteria
 </Accordion>
* **Achievable**: Base your targets on industry benchmarks, prior experiments, AI research, or expert knowledge. Your success metrics should not be unrealistic to current frontier model capabilities.
* **Relevant**: Align your criteria with your application's purpose and user needs. Strong citation accuracy might be critical for medical apps but less so for casual chatbots.

<Accordion title="Example task fidelity criteria for sentiment analysis">
 | | Criteria |
 | ---- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
 | Bad | The model should classify sentiments well |
 | Good | Our sentiment analysis model should achieve an F1 score of at least 0.85 (Measurable, Specific) on a held-out test set\* of 10,000 diverse Twitter posts (Relevant), which is a 5% improvement over our current baseline (Achievable). |

 \**More on held-out test sets in the next section*
</Accordion>

***

## Common success criteria to consider

Here are some criteria that might be important for your use case. This list is non-exhaustive.

<AccordionGroup>
 <Accordion title="Task fidelity">
 How well does the model need to perform on the task? You may also need to consider edge case handling, such as how well the model needs to perform on rare or challenging inputs.
 </Accordion>

 <Accordion title="Consistency">
 How similar does the model's responses need to be for similar types of input? If a user asks the same question twice, how important is it that they get semantically similar answers?
 </Accordion>

 <Accordion title="Relevance and coherence">
 How well does the model directly address the user's questions or instructions? How important is it for the information to be presented in a logical, easy to follow manner?
 </Accordion>

 <Accordion title="Tone and style">
 How well does the model's output style match expectations? How appropriate is its language for the target audience?
 </Accordion>

 <Accordion title="Privacy preservation">
 What is a successful metric for how the model handles personal or sensitive information? Can it follow instructions not to use or share certain details?
 </Accordion>

 <Accordion title="Context utilization">
 How effectively does the model use provided context? How well does it reference and build upon information given in its history?
 </Accordion>

 <Accordion title="Latency">
 What is the acceptable response time for the model? This will depend on your application's real-time requirements and user expectations.
 </Accordion>

 <Accordion title="Price">
 What is your budget for running the model? Consider factors like the cost per API call, the size of the model, and the frequency of usage.
 </Accordion>
</AccordionGroup>

Most use cases will need multidimensional evaluation along several success criteria.

<Accordion title="Example multidimensional criteria for sentiment analysis">
 | | Criteria |
 | ---- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
 | Bad | The model should classify sentiments well |
 | Good | On a held-out test set of 10,000 diverse Twitter posts, our sentiment analysis model should achieve:<br />- an F1 score of at least 0.85<br />- 99.5% of outputs are non-toxic<br />- 90% of errors are would cause inconvenience, not egregious error\*<br />- 95% response time \< 200ms |

 \**In reality, we would also define what "inconvenience" and "egregious" means.*
</Accordion>

***

## Next steps

<CardGroup cols={2}>
 <Card title="Brainstorm criteria" icon="link" href="https://claude.ai/">
 Brainstorm success criteria for your use case with Claude on claude.ai.<br /><br />**Tip**: Drop this page into the chat as guidance for Claude!
 </Card>

 <Card title="Design evaluations" icon="link" href="/en/docs/build-with-claude/prompt-engineering/be-clear-and-direct">
 Learn to build strong test sets to gauge Claude's performance against your criteria.
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDEH3Q47EH23]
---

[START OF DOCUMENT: CLAUDEO5OP53M9Y | Title: Develop-Tests]

# Create strong empirical evaluations

After defining your success criteria, the next step is designing evaluations to measure LLM performance against those criteria. This is a vital part of the prompt engineering cycle.

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=72e3d1e26bc86aab09b6652a1b456407" alt="" data-og-width="3558" width="3558" data-og-height="1182" height="1182" data-path="images/how-to-prompt-eng.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=a710205481192fa01f13094bda8d7e5d 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=44ea32b2d008b4fb2a0f6b972937fceb 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=12607f3577a156763e4fda4137dfcc7d 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=4c3dbff00bbafec3542ef04e0b781037 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=80ee60f2986e703f5aad4f27115a1bea 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/how-to-prompt-eng.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=a9d9d0a8a42361c0644c2d527caef5dc 2500w" />

This guide focuses on how to develop your test cases.

## Building evals and test cases

### Eval design principles

1. **Be task-specific**: Design evals that mirror your real-world task distribution. Don't forget to factor in edge cases!
 <Accordion title="Example edge cases">
 * Irrelevant or nonexistent input data
 * Overly long input data or user input
 * \[Chat use cases] Poor, harmful, or irrelevant user input
 * Ambiguous test cases where even humans would find it hard to reach an assessment consensus
 </Accordion>
2. **Automate when possible**: Structure questions to allow for automated grading (e.g., multiple-choice, string match, code-graded, LLM-graded).
3. **Prioritize volume over quality**: More questions with slightly lower signal automated grading is better than fewer questions with high-quality human hand-graded evals.

### Example evals

<AccordionGroup>
 <Accordion title="Task fidelity (sentiment analysis) - exact match evaluation">
 **What it measures**: Exact match evals measure whether the model's output exactly matches a predefined correct answer. It's a simple, unambiguous metric that's perfect for tasks with clear-cut, categorical answers like sentiment analysis (positive, negative, neutral).

 **Example eval test cases**: 1000 tweets with human-labeled sentiments.```python  theme={null}
    import anthropic

    tweets = [
        {"text": "This movie was a total waste of time. 👎", "sentiment": "negative"},
        {"text": "The new album is 🔥! Been on repeat all day.", "sentiment": "positive"},
        {"text": "I just love it when my flight gets delayed for 5 hours. #bestdayever", "sentiment": "negative"},  # Edge case: Sarcasm
        {"text": "The movie's plot was terrible, but the acting was phenomenal.", "sentiment": "mixed"},  # Edge case: Mixed sentiment
        # ... 996 more tweets
    ]

    client = anthropic.Anthropic()

    def get_completion(prompt: str):
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=50,
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text

    def evaluate_exact_match(model_output, correct_answer):
        return model_output.strip().lower() == correct_answer.lower()

    outputs = [get_completion(f"Classify this as 'positive', 'negative', 'neutral', or 'mixed': {tweet['text']}") for tweet in tweets]
    accuracy = sum(evaluate_exact_match(output, tweet['sentiment']) for output, tweet in zip(outputs, tweets)) / len(tweets)
    print(f"Sentiment Analysis Accuracy: {accuracy * 100}%")
    ```</Accordion>

 <Accordion title="Consistency (FAQ bot) - cosine similarity evaluation">
 **What it measures**: Cosine similarity measures the similarity between two vectors (in this case, sentence embeddings of the model's output using SBERT) by computing the cosine of the angle between them. Values closer to 1 indicate higher similarity. It's ideal for evaluating consistency because similar questions should yield semantically similar answers, even if the wording varies.

 **Example eval test cases**: 50 groups with a few paraphrased versions each.```python  theme={null}
    from sentence_transformers import SentenceTransformer
    import numpy as np
    import anthropic

    faq_variations = [
        {"questions": ["What's your return policy?", "How can I return an item?", "Wut's yur retrn polcy?"], "answer": "Our return policy allows..."},  # Edge case: Typos
        {"questions": ["I bought something last week, and it's not really what I expected, so I was wondering if maybe I could possibly return it?", "I read online that your policy is 30 days but that seems like it might be out of date because the website was updated six months ago, so I'm wondering what exactly is your current policy?"], "answer": "Our return policy allows..."},  # Edge case: Long, rambling question
        {"questions": ["I'm Jane's cousin, and she said you guys have great customer service. Can I return this?", "Reddit told me that contacting customer service this way was the fastest way to get an answer. I hope they're right! What is the return window for a jacket?"], "answer": "Our return policy allows..."},  # Edge case: Irrelevant info
        # ... 47 more FAQs
    ]

    client = anthropic.Anthropic()

    def get_completion(prompt: str):
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=2048,
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text

    def evaluate_cosine_similarity(outputs):
        model = SentenceTransformer('all-MiniLM-L6-v2')
        embeddings = [model.encode(output) for output in outputs]

        cosine_similarities = np.dot(embeddings, embeddings.T) / (np.linalg.norm(embeddings, axis=1) * np.linalg.norm(embeddings, axis=1).T)
        return np.mean(cosine_similarities)

    for faq in faq_variations:
        outputs = [get_completion(question) for question in faq["questions"]]
        similarity_score = evaluate_cosine_similarity(outputs)
        print(f"FAQ Consistency Score: {similarity_score * 100}%")
    ```</Accordion>

 <Accordion title="Relevance and coherence (summarization) - ROUGE-L evaluation">
 **What it measures**: ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation - Longest Common Subsequence) evaluates the quality of generated summaries. It measures the length of the longest common subsequence between the candidate and reference summaries. High ROUGE-L scores indicate that the generated summary captures key information in a coherent order.

 **Example eval test cases**: 200 articles with reference summaries.```python  theme={null}
    from rouge import Rouge
    import anthropic

    articles = [
        {"text": "In a groundbreaking study, researchers at MIT...", "summary": "MIT scientists discover a new antibiotic..."},
        {"text": "Jane Doe, a local hero, made headlines last week for saving... In city hall news, the budget... Meteorologists predict...", "summary": "Community celebrates local hero Jane Doe while city grapples with budget issues."},  # Edge case: Multi-topic
        {"text": "You won't believe what this celebrity did! ... extensive charity work ...", "summary": "Celebrity's extensive charity work surprises fans"},  # Edge case: Misleading title
        # ... 197 more articles
    ]

    client = anthropic.Anthropic()

    def get_completion(prompt: str):
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=1024,
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text

    def evaluate_rouge_l(model_output, true_summary):
        rouge = Rouge()
        scores = rouge.get_scores(model_output, true_summary)
        return scores[0]['rouge-l']['f']  # ROUGE-L F1 score

    outputs = [get_completion(f"Summarize this article in 1-2 sentences:\n\n{article['text']}") for article in articles]
    relevance_scores = [evaluate_rouge_l(output, article['summary']) for output, article in zip(outputs, articles)]
    print(f"Average ROUGE-L F1 Score: {sum(relevance_scores) / len(relevance_scores)}")
    ```</Accordion>

 <Accordion title="Tone and style (customer service) - LLM-based Likert scale">
 **What it measures**: The LLM-based Likert scale is a psychometric scale that uses an LLM to judge subjective attitudes or perceptions. Here, it's used to rate the tone of responses on a scale from 1 to 5. It's ideal for evaluating nuanced aspects like empathy, professionalism, or patience that are difficult to quantify with traditional metrics.

 **Example eval test cases**: 100 customer inquiries with target tone (empathetic, professional, concise).```python  theme={null}
    import anthropic

    inquiries = [
        {"text": "This is the third time you've messed up my order. I want a refund NOW!", "tone": "empathetic"},  # Edge case: Angry customer
        {"text": "I tried resetting my password but then my account got locked...", "tone": "patient"},  # Edge case: Complex issue
        {"text": "I can't believe how good your product is. It's ruined all others for me!", "tone": "professional"},  # Edge case: Compliment as complaint
        # ... 97 more inquiries
    ]

    client = anthropic.Anthropic()

    def get_completion(prompt: str):
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=2048,
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text

    def evaluate_likert(model_output, target_tone):
        tone_prompt = f"""Rate this customer service response on a scale of 1-5 for being {target_tone}:
        <response>{model_output}</response>
        1: Not at all {target_tone}
        5: Perfectly {target_tone}
        Output only the number."""

        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output 
        response = client.messages.create(model="claude-sonnet-4-5", max_tokens=50, messages=[{"role": "user", "content": tone_prompt}])
        return int(response.content[0].text.strip())

    outputs = [get_completion(f"Respond to this customer inquiry: {inquiry['text']}") for inquiry in inquiries]
    tone_scores = [evaluate_likert(output, inquiry['tone']) for output, inquiry in zip(outputs, inquiries)]
    print(f"Average Tone Score: {sum(tone_scores) / len(tone_scores)}")
    ```</Accordion>

 <Accordion title="Privacy preservation (medical chatbot) - LLM-based binary classification">
 **What it measures**: Binary classification determines if an input belongs to one of two classes. Here, it's used to classify whether a response contains PHI or not. This method can understand context and identify subtle or implicit forms of PHI that rule-based systems might miss.

 **Example eval test cases**: 500 simulated patient queries, some with PHI.```python  theme={null}
    import anthropic

    patient_queries = [
        {"query": "What are the side effects of Lisinopril?", "contains_phi": False},
        {"query": "Can you tell me why John Doe, DOB 5/12/1980, was prescribed Metformin?", "contains_phi": True},  # Edge case: Explicit PHI
        {"query": "If my friend Alice, who was born on July 4, 1985, had diabetes, what...", "contains_phi": True},  # Edge case: Hypothetical PHI
        {"query": "I'm worried about my son. He's been prescribed the same medication as his father last year.", "contains_phi": True},  # Edge case: Implicit PHI
        # ... 496 more queries
    ]

    client = anthropic.Anthropic()

    def get_completion(prompt: str):
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=1024,
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text

    def evaluate_binary(model_output, query_contains_phi):
        if not query_contains_phi:
            return True

        binary_prompt = """Does this response contain or reference any Personal Health Information (PHI)?
        PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual's physical or mental health condition, the provision of healthcare to that individual, or payment for such care.
        Key aspects of PHI include:
        - Identifiers: Names, addresses, birthdates, Social Security numbers, medical record numbers, etc.
        - Health data: Diagnoses, treatment plans, test results, medication records, etc.
        - Financial information: Insurance details, payment records, etc.
        - Communication: Notes from healthcare providers, emails or messages about health.

        <response>{model_output}</response>
        Output only 'yes' or 'no'."""

        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output
        response = client.messages.create(model="claude-sonnet-4-5", max_tokens=50, messages=[{"role": "user", "content": binary_prompt}])
        return response.content[0].text.strip().lower() == "no"

    outputs = [get_completion(f"You are a medical assistant. Never reveal any PHI in your responses. PHI refers to any individually identifiable health data that is created, used, or disclosed in the course of providing healthcare services. This includes information related to an individual's physical or mental health condition, the provision of healthcare to that individual, or payment for such care. Here is the question: {query['query']}") for query in patient_queries]
    privacy_scores = [evaluate_binary(output, query['contains_phi']) for output, query in zip(outputs, patient_queries)]
    print(f"Privacy Preservation Score: {sum(privacy_scores) / len(privacy_scores) * 100}%")
    ```</Accordion>

 <Accordion title="Context utilization (conversation assistant) - LLM-based ordinal scale">
 **What it measures**: Similar to the Likert scale, the ordinal scale measures on a fixed, ordered scale (1-5). It's perfect for evaluating context utilization because it can capture the degree to which the model references and builds upon the conversation history, which is key for coherent, personalized interactions.

 **Example eval test cases**: 100 multi-turn conversations with context-dependent questions.```python  theme={null}
    import anthropic

    conversations = [
        [
            {"role": "user", "content": "I just got a new pomeranian!"},
            {"role": "assistant", "content": "Congratulations on your new furry friend! Is this your first dog?"},
            {"role": "user", "content": "Yes, it is. I named her Luna."},
            {"role": "assistant", "content": "Luna is a lovely name! As a first-time dog owner, you might have some questions. What would you like to know about caring for Luna?"},
            ...
            {"role": "user", "content": "What should I know about caring for a dog of this specific breed?"}  # Edge case: Relies on context from much earlier
        ],
        [
            {"role": "user", "content": "I'm reading 'To Kill a Mockingbird' for my book club."},
            {"role": "assistant", "content": "Great choice! 'To Kill a Mockingbird' by Harper Lee is a classic that explores themes of racial injustice and moral growth. How are you finding it so far?"},
            {"role": "user", "content": "It's powerful. Hey, when was Scout's birthday again?"},  # Edge case: Abrupt topic shift
            {"role": "assistant", "content": "I apologize, but I don't recall the exact date of Scout's birthday being mentioned in 'To Kill a Mockingbird'. The novel doesn't focus on such specific details. Instead, it centers on Scout's growth and experiences over several years. Is there a particular part of the book or Scout's development that you're interested in discussing?"},
            {"role": "user", "content": "Oh, right. Well, can you suggest a recipe for a classic Southern cake?"}  # Edge case: Another topic shift
        ],
        # ... 98 more conversations
    ]

    client = anthropic.Anthropic()

    def get_completion(prompt: str):
        message = client.messages.create(
            model="claude-sonnet-4-5",
            max_tokens=1024,
            messages=[
            {"role": "user", "content": prompt}
            ]
        )
        return message.content[0].text

    def evaluate_ordinal(model_output, conversation):
        ordinal_prompt = f"""Rate how well this response utilizes the conversation context on a scale of 1-5:
        <conversation>
        {"".join(f"{turn['role']}: {turn['content']}\\n" for turn in conversation[:-1])}
        </conversation>
        <response>{model_output}</response>
        1: Completely ignores context
        5: Perfectly utilizes context
        Output only the number and nothing else."""

        # Generally best practice to use a different model to evaluate than the model used to generate the evaluated output
        response = client.messages.create(model="claude-sonnet-4-5", max_tokens=50, messages=[{"role": "user", "content": ordinal_prompt}])
        return int(response.content[0].text.strip())

    outputs = [get_completion(conversation) for conversation in conversations]
    context_scores = [evaluate_ordinal(output, conversation) for output, conversation in zip(outputs, conversations)]
    print(f"Average Context Utilization Score: {sum(context_scores) / len(context_scores)}")
    ```</Accordion>
</AccordionGroup>

<Tip>Writing hundreds of test cases can be hard to do by hand! Get Claude to help you generate more from a baseline set of example test cases.</Tip>
<Tip>If you don't know what eval methods might be useful to assess for your success criteria, you can also brainstorm with Claude!</Tip>

***

## Grading evals

When deciding which method to use to grade evals, choose the fastest, most reliable, most scalable method:

1. **Code-based grading**: Fastest and most reliable, extremely scalable, but also lacks nuance for more complex judgements that require less rule-based rigidity.
 * Exact match: `output == golden_answer`
 * String match: `key_phrase in output`

2. **Human grading**: Most flexible and high quality, but slow and expensive. Avoid if possible.

3. **LLM-based grading**: Fast and flexible, scalable and suitable for complex judgement. Test to ensure reliability first then scale.

### Tips for LLM-based grading

* **Have detailed, clear rubrics**: "The answer should always mention 'Acme Inc.' in the first sentence. If it does not, the answer is automatically graded as 'incorrect.'"
 <Note>A given use case, or even a specific success criteria for that use case, might require several rubrics for holistic evaluation.</Note>
* **Empirical or specific**: For example, instruct the LLM to output only 'correct' or 'incorrect', or to judge from a scale of 1-5. Purely qualitative evaluations are hard to assess quickly and at scale.
* **Encourage reasoning**: Ask the LLM to think first before deciding an evaluation score, and then discard the reasoning. This increases evaluation performance, particularly for tasks requiring complex judgement.

<Accordion title="Example: LLM-based grading">```python  theme={null}
  import anthropic

  def build_grader_prompt(answer, rubric):
      return f"""Grade this answer based on the rubric:
      <rubric>{rubric}</rubric>
      <answer>{answer}</answer>
      Think through your reasoning in <thinking> tags, then output 'correct' or 'incorrect' in <result> tags.""

  def grade_completion(output, golden_answer):
      grader_response = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=2048,
          messages=[{"role": "user", "content": build_grader_prompt(output, golden_answer)}]
      ).content[0].text

      return "correct" if "correct" in grader_response.lower() else "incorrect"

  # Example usage
  eval_data = [
      {"question": "Is 42 the answer to life, the universe, and everything?", "golden_answer": "Yes, according to 'The Hitchhiker's Guide to the Galaxy'."},
      {"question": "What is the capital of France?", "golden_answer": "The capital of France is Paris."}
  ]

  def get_completion(prompt: str):
      message = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=1024,
          messages=[
          {"role": "user", "content": prompt}
          ]
      )
      return message.content[0].text

  outputs = [get_completion(q["question"]) for q in eval_data]
  grades = [grade_completion(output, a["golden_answer"]) for output, a in zip(outputs, eval_data)]
  print(f"Score: {grades.count('correct') / len(grades) * 100}%")
  ```</Accordion>

## Next steps

<CardGroup cols={2}>
 <Card title="Brainstorm evaluations" icon="link" href="/en/docs/build-with-claude/prompt-engineering/overview">
 Learn how to craft prompts that maximize your eval scores.
 </Card>

 <Card title="Evals cookbook" icon="link" href="https://github.com/anthropics/anthropic-cookbook/blob/main/misc/building%5Fevals.ipynb">
 More code examples of human-, code-, and LLM-graded evals.
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDEO5OP53M9Y]
---

[START OF DOCUMENT: CLAUDE2I4PXB8T70 | Title: Embeddings]

# Embeddings

> Text embeddings are numerical representations of text that enable measuring semantic similarity. This guide introduces embeddings, their applications, and how to use embedding models for tasks like search, recommendations, and anomaly detection.

## Before implementing embeddings

When selecting an embeddings provider, there are several factors you can consider depending on your needs and preferences:

* Dataset size & domain specificity: size of the model training dataset and its relevance to the domain you want to embed. Larger or more domain-specific data generally produces better in-domain embeddings
* Inference performance: embedding lookup speed and end-to-end latency. This is a particularly important consideration for large scale production deployments
* Customization: options for continued training on private data, or specialization of models for very specific domains. This can improve performance on unique vocabularies

## How to get embeddings with Anthropic

Anthropic does not offer its own embedding model. One embeddings provider that has a wide variety of options and capabilities encompassing all of the above considerations is Voyage AI.

Voyage AI makes state-of-the-art embedding models and offers customized models for specific industry domains such as finance and healthcare, or bespoke fine-tuned models for individual customers.

The rest of this guide is for Voyage AI, but we encourage you to assess a variety of embeddings vendors to find the best fit for your specific use case.

## Available Models

Voyage recommends using the following text embedding models:

| Model | Context Length | Embedding Dimension | Description |
| ------------------ | -------------- | ------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `voyage-3-large` | 32,000 | 1024 (default), 256, 512, 2048 | The best general-purpose and multilingual retrieval quality. See [blog post](https://blog.voyageai.com/2025/01/07/voyage-3-large/) for details. |
| `voyage-3.5` | 32,000 | 1024 (default), 256, 512, 2048 | Optimized for general-purpose and multilingual retrieval quality. See [blog post](https://blog.voyageai.com/2025/05/20/voyage-3-5/) for details. |
| `voyage-3.5-lite` | 32,000 | 1024 (default), 256, 512, 2048 | Optimized for latency and cost. See [blog post](https://blog.voyageai.com/2025/05/20/voyage-3-5/) for details. |
| `voyage-code-3` | 32,000 | 1024 (default), 256, 512, 2048 | Optimized for **code** retrieval. See [blog post](https://blog.voyageai.com/2024/12/04/voyage-code-3/) for details. |
| `voyage-finance-2` | 32,000 | 1024 | Optimized for **finance** retrieval and RAG. See [blog post](https://blog.voyageai.com/2024/06/03/domain-specific-embeddings-finance-edition-voyage-finance-2/) for details. |
| `voyage-law-2` | 16,000 | 1024 | Optimized for **legal** and **long-context** retrieval and RAG. Also improved performance across all domains. See [blog post](https://blog.voyageai.com/2024/04/15/domain-specific-embeddings-and-retrieval-legal-edition-voyage-law-2/) for details. |

Additionally, the following multimodal embedding models are recommended:

| Model | Context Length | Embedding Dimension | Description |
| --------------------- | -------------- | ------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `voyage-multimodal-3` | 32000 | 1024 | Rich multimodal embedding model that can vectorize interleaved text and content-rich images, such as screenshots of PDFs, slides, tables, figures, and more. See [blog post](https://blog.voyageai.com/2024/11/12/voyage-multimodal-3/) for details. |

Need help deciding which text embedding model to use? Check out the [FAQ](https://docs.voyageai.com/docs/faq#what-embedding-models-are-available-and-which-one-should-i-use\&ref=anthropic).

## Getting started with Voyage AI

To access Voyage embeddings:

1. Sign up on Voyage AI's website
2. Obtain an API key
3. Set the API key as an environment variable for convenience:```bash  theme={null}
export VOYAGE_API_KEY="<your secret key>"
```You can obtain the embeddings by either using the official [`voyageai` Python package](https://github.com/voyage-ai/voyageai-python) or HTTP requests, as described below.

### Voyage Python library

The `voyageai` package can be installed using the following command:```bash  theme={null}
pip install -U voyageai
```Then, you can create a client object and start using it to embed your texts:```python  theme={null}
import voyageai

vo = voyageai.Client()
# This will automatically use the environment variable VOYAGE_API_KEY.
# Alternatively, you can use vo = voyageai.Client(api_key="<your secret key>")

texts = ["Sample text 1", "Sample text 2"]

result = vo.embed(texts, model="voyage-3.5", input_type="document")
print(result.embeddings[0])
print(result.embeddings[1])
````result.embeddings` will be a list of two embedding vectors, each containing 1024 floating-point numbers. After running the above code, the two embeddings will be printed on the screen:```
[-0.013131560757756233, 0.019828535616397858, ...]   # embedding for "Sample text 1"
[-0.0069352793507277966, 0.020878976210951805, ...]  # embedding for "Sample text 2"
```When creating the embeddings, you can specify a few other arguments to the `embed()` function.

For more information on the Voyage python package, see [the Voyage documentation](https://docs.voyageai.com/docs/embeddings#python-api).

### Voyage HTTP API

You can also get embeddings by requesting Voyage HTTP API. For example, you can send an HTTP request through the `curl` command in a terminal:```bash  theme={null}
curl https://api.voyageai.com/v1/embeddings \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $VOYAGE_API_KEY" \
  -d '{
    "input": ["Sample text 1", "Sample text 2"],
    "model": "voyage-3.5"
  }'
```The response you would get is a JSON object containing the embeddings and the token usage:```json  theme={null}
{
  "object": "list",
  "data": [
    {
      "embedding": [-0.013131560757756233, 0.019828535616397858, ...],
      "index": 0
    },
    {
      "embedding": [-0.0069352793507277966, 0.020878976210951805, ...],
      "index": 1
    }
  ],
  "model": "voyage-3.5",
  "usage": {
    "total_tokens": 10
  }
}

```For more information on the Voyage HTTP API, see [the Voyage documentation](https://docs.voyageai.com/reference/embeddings-api).

### AWS Marketplace

Voyage embeddings are available on [AWS Marketplace](https://aws.amazon.com/marketplace/seller-profile?id=seller-snt4gb6fd7ljg). Instructions for accessing Voyage on AWS are available [here](https://docs.voyageai.com/docs/aws-marketplace-model-package?ref=anthropic).

## Quickstart example

Now that we know how to get embeddings, let's see a brief example.

Suppose we have a small corpus of six documents to retrieve from```python  theme={null}
documents = [
    "The Mediterranean diet emphasizes fish, olive oil, and vegetables, believed to reduce chronic diseases.",
    "Photosynthesis in plants converts light energy into glucose and produces essential oxygen.",
    "20th-century innovations, from radios to smartphones, centered on electronic advancements.",
    "Rivers provide water, irrigation, and habitat for aquatic species, vital for ecosystems.",
    "Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.",
    "Shakespeare's works, like 'Hamlet' and 'A Midsummer Night's Dream,' endure in literature."
]

```We will first use Voyage to convert each of them into an embedding vector```python  theme={null}
import voyageai

vo = voyageai.Client()

# Embed the documents
doc_embds = vo.embed(
    documents, model="voyage-3.5", input_type="document"
).embeddings
```The embeddings will allow us to do semantic search / retrieval in the vector space. Given an example query,```python  theme={null}
query = "When is Apple's conference call scheduled?"
```we convert it into an embedding, and conduct a nearest neighbor search to find the most relevant document based on the distance in the embedding space.```python  theme={null}
import numpy as np

# Embed the query
query_embd = vo.embed(
    [query], model="voyage-3.5", input_type="query"
).embeddings[0]

# Compute the similarity
# Voyage embeddings are normalized to length 1, therefore dot-product
# and cosine similarity are the same.
similarities = np.dot(doc_embds, query_embd)

retrieved_id = np.argmax(similarities)
print(documents[retrieved_id])
```Note that we use `input_type="document"` and `input_type="query"` for embedding the document and query, respectively. More specification can be found [here](/en/docs/build-with-claude/embeddings#voyage-python-package).

The output would be the 5th document, which is indeed the most relevant to the query:```
Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET.
```If you are looking for a detailed set of cookbooks on how to do RAG with embeddings, including vector databases, check out our [RAG cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/third_party/Pinecone/rag_using_pinecone.ipynb).

## FAQ

<AccordionGroup>
 <Accordion title="Why do Voyage embeddings have superior quality?">
 Embedding models rely on powerful neural networks to capture and compress semantic context, similar to generative models. Voyage's team of experienced AI researchers optimizes every component of the embedding process, including:

 * Model architecture
 * Data collection
 * Loss functions
 * Optimizer selection

 Learn more about Voyage's technical approach on their [blog](https://blog.voyageai.com/).
 </Accordion>

 <Accordion title="What embedding models are available and which should I use?">
 For general-purpose embedding, we recommend:

 * `voyage-3-large`: Best quality
 * `voyage-3.5-lite`: Lowest latency and cost
 * `voyage-3.5`: Balanced performance with superior retrieval quality at a competitive price point

 For retrieval, use the `input_type` parameter to specify whether the text is a query or document type.

 Domain-specific models:

 * Legal tasks: `voyage-law-2`
 * Code and programming documentation: `voyage-code-3`
 * Finance-related tasks: `voyage-finance-2`
 </Accordion>

 <Accordion title="Which similarity function should I use?">
 You can use Voyage embeddings with either dot-product similarity, cosine similarity, or Euclidean distance. An explanation about embedding similarity can be found [here](https://www.pinecone.io/learn/vector-similarity/).

 Voyage AI embeddings are normalized to length 1, which means that:

 * Cosine similarity is equivalent to dot-product similarity, while the latter can be computed more quickly.
 * Cosine similarity and Euclidean distance will result in the identical rankings.
 </Accordion>

 <Accordion title="What is the relationship between characters, words, and tokens?">
 Please see this [page](https://docs.voyageai.com/docs/tokenization?ref=anthropic).
 </Accordion>

 <Accordion title="When and how should I use the input_type parameter?">
 For all retrieval tasks and use cases (e.g., RAG), we recommend that the `input_type` parameter be used to specify whether the input text is a query or document. Do not omit `input_type` or set `input_type=None`. Specifying whether input text is a query or document can create better dense vector representations for retrieval, which can lead to better retrieval quality.

 When using the `input_type` parameter, special prompts are prepended to the input text prior to embedding. Specifically:

 > 📘 **Prompts associated with `input_type`**
 >
 > * For a query, the prompt is “Represent the query for retrieving supporting documents: “.
 > * For a document, the prompt is “Represent the document for retrieval: “.
 > * Example
 > * When `input_type="query"`, a query like "When is Apple's conference call scheduled?" will become "**Represent the query for retrieving supporting documents:** When is Apple's conference call scheduled?"
 > * When `input_type="document"`, a query like "Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET." will become "**Represent the document for retrieval:** Apple's conference call to discuss fourth fiscal quarter results and business updates is scheduled for Thursday, November 2, 2023 at 2:00 p.m. PT / 5:00 p.m. ET."

 `voyage-large-2-instruct`, as the name suggests, is trained to be responsive to additional instructions that are prepended to the input text. For classification, clustering, or other [MTEB](https://huggingface.co/mteb) subtasks, please use the instructions [here](https://github.com/voyage-ai/voyage-large-2-instruct).
 </Accordion>

 <Accordion title="What quantization options are available?">
 Quantization in embeddings converts high-precision values, like 32-bit single-precision floating-point numbers, to lower-precision formats such as 8-bit integers or 1-bit binary values, reducing storage, memory, and costs by 4x and 32x, respectively. Supported Voyage models enable quantization by specifying the output data type with the `output_dtype` parameter:

 * `float`: Each returned embedding is a list of 32-bit (4-byte) single-precision floating-point numbers. This is the default and provides the highest precision / retrieval accuracy.
 * `int8` and `uint8`: Each returned embedding is a list of 8-bit (1-byte) integers ranging from -128 to 127 and 0 to 255, respectively.
 * `binary` and `ubinary`: Each returned embedding is a list of 8-bit integers that represent bit-packed, quantized single-bit embedding values: `int8` for `binary` and `uint8` for `ubinary`. The length of the returned list of integers is 1/8 of the actual dimension of the embedding. The binary type uses the offset binary method, which you can learn more about in the FAQ below.

 > **Binary quantization example**
 >
 > Consider the following eight embedding values: -0.03955078, 0.006214142, -0.07446289, -0.039001465, 0.0046463013, 0.00030612946, -0.08496094, and 0.03994751. With binary quantization, values less than or equal to zero will be quantized to a binary zero, and positive values to a binary one, resulting in the following binary sequence: 0, 1, 0, 0, 1, 1, 0, 1. These eight bits are then packed into a single 8-bit integer, 01001101 (with the leftmost bit as the most significant bit).
 >
 > * `ubinary`: The binary sequence is directly converted and represented as the unsigned integer (`uint8`) 77.
 > * `binary`: The binary sequence is represented as the signed integer (`int8`) -51, calculated using the offset binary method (77 - 128 = -51).
 </Accordion>

 <Accordion title="How can I truncate Matryoshka embeddings?">
 Matryoshka learning creates embeddings with coarse-to-fine representations within a single vector. Voyage models, such as `voyage-code-3`, that support multiple output dimensions generate such Matryoshka embeddings. You can truncate these vectors by keeping the leading subset of dimensions. For example, the following Python code demonstrates how to truncate 1024-dimensional vectors to 256 dimensions:```python  theme={null}
    import voyageai
    import numpy as np

    def embd_normalize(v: np.ndarray) -> np.ndarray:
        """
        Normalize the rows of a 2D numpy array to unit vectors by dividing each row by its Euclidean
        norm. Raises a ValueError if any row has a norm of zero to prevent division by zero.
        """
        row_norms = np.linalg.norm(v, axis=1, keepdims=True)
        if np.any(row_norms == 0):
            raise ValueError("Cannot normalize rows with a norm of zero.")
        return v / row_norms


    vo = voyageai.Client()

    # Generate voyage-code-3 vectors, which by default are 1024-dimensional floating-point numbers
    embd = vo.embed(['Sample text 1', 'Sample text 2'], model='voyage-code-3').embeddings

    # Set shorter dimension
    short_dim = 256

    # Resize and normalize vectors to shorter dimension
    resized_embd = embd_normalize(np.array(embd)[:, :short_dim]).tolist()
    ```</Accordion>
</AccordionGroup>

## Pricing

Visit Voyage's [pricing page](https://docs.voyageai.com/docs/pricing?ref=anthropic) for the most up to date pricing details.

[END OF DOCUMENT: CLAUDE2I4PXB8T70]
---

[START OF DOCUMENT: CLAUDETE7IYFSAY | Title: Eval-Tool]

# Using the Evaluation Tool

> The [Claude Console](https://console.anthropic.com/dashboard) features an **Evaluation tool** that allows you to test your prompts under various scenarios.

## Accessing the Evaluate Feature

To get started with the Evaluation tool:

1. Open the Claude Console and navigate to the prompt editor.
2. After composing your prompt, look for the 'Evaluate' tab at the top of the screen.

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=663abe685548444261647cce0baefe9c" alt="Accessing Evaluate Feature" data-og-width="1999" width="1999" data-og-height="1061" height="1061" data-path="images/access_evaluate.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=d8b903a38366bd5b123b4d4f2195f850 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=9e0ec4080716507bc02820943b2e48f1 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=01a9e18c4b67a487c0c7485c05a98f07 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=52677a88eedc2403bbb371dc101c81e4 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=bc938506a6d1a1bcb6c110d6b1815db8 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/access_evaluate.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=861bc715fdba424eaeab91190c8f49ed 2500w" />

<Tip>
 Ensure your prompt includes at least 1-2 dynamic variables using the double brace syntax: \{\{variable}}. This is required for creating eval test sets.
</Tip>

## Generating Prompts

The Console offers a built-in [prompt generator](/en/docs/build-with-claude/prompt-engineering/prompt-generator) powered by Claude Opus 4.1:

<Steps>
 <Step title="Click 'Generate Prompt'">
 Clicking the 'Generate Prompt' helper tool will open a modal that allows you to enter your task information.
 </Step>

 <Step title="Describe your task">
 Describe your desired task (e.g., "Triage inbound customer support requests") with as much or as little detail as you desire. The more context you include, the more Claude can tailor its generated prompt to your specific needs.
 </Step>

 <Step title="Generate your prompt">
 Clicking the orange 'Generate Prompt' button at the bottom will have Claude generate a high quality prompt for you. You can then further improve those prompts using the Evaluation screen in the Console.
 </Step>
</Steps>

This feature makes it easier to create prompts with the appropriate variable syntax for evaluation.

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=65c7176d98dac6a07367adb03161b3ae" alt="Prompt Generator" data-og-width="1654" width="1654" data-og-height="904" height="904" data-path="images/promptgenerator.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=ca75eca8b693f7579eb49770059a9e77 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=f0fc84679e844b991128352ac5705b95 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=eca386d7da8ffdcf6ad75371d72390eb 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=ed08774d3d2f7a785713221596575d4a 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=2505fc166935e27e71c4317341527aee 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/promptgenerator.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=612ff9bde77f2c0d6352a8405e7a46cd 2500w" />

## Creating Test Cases

When you access the Evaluation screen, you have several options to create test cases:

1. Click the '+ Add Row' button at the bottom left to manually add a case.
2. Use the 'Generate Test Case' feature to have Claude automatically generate test cases for you.
3. Import test cases from a CSV file.

To use the 'Generate Test Case' feature:

<Steps>
 <Step title="Click on 'Generate Test Case'">
 Claude will generate test cases for you, one row at a time for each time you click the button.
 </Step>

 <Step title="Edit generation logic (optional)">
 You can also edit the test case generation logic by clicking on the arrow dropdown to the right of the 'Generate Test Case' button, then on 'Show generation logic' at the top of the Variables window that pops up. You may have to click \`Generate' on the top right of this window to populate initial generation logic.

 Editing this allows you to customize and fine tune the test cases that Claude generates to greater precision and specificity.
 </Step>
</Steps>

Here's an example of a populated Evaluation screen with several test cases:

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=aa5f378cd1fd8bee94acbb557476f690" alt="Populated Evaluation Screen" data-og-width="1999" width="1999" data-og-height="1061" height="1061" data-path="images/eval_populated.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=da25331850100232004b2b4120acbc92 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=35e3ad0abb9226a616b2f2484237f012 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=42df4fd14669cf2ff21b404b74e16a97 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=c8f77e4d68b0ef75d0d9cb18d27387f7 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=f8b324b1995be107795b39379818c324 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/eval_populated.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=77d58591d4fac46b9de9ceb24568a5fa 2500w" />

<Note>
 If you update your original prompt text, you can re-run the entire eval suite against the new prompt to see how changes affect performance across all test cases.
</Note>

## Tips for Effective Evaluation

<Accordion title="Prompt Structure for Evaluation">
 To make the most of the Evaluation tool, structure your prompts with clear input and output formats. For example:```
  In this task, you will generate a cute one sentence story that incorporates two elements: a color and a sound.
  The color to include in the story is:
  <color>
  {{COLOR}}
  </color>
  The sound to include in the story is:
  <sound>
  {{SOUND}}
  </sound>
  Here are the steps to generate the story:
  1. Think of an object, animal, or scene that is commonly associated with the color provided. For example, if the color is "blue", you might think of the sky, the ocean, or a bluebird.
  2. Imagine a simple action, event or scene involving the colored object/animal/scene you identified and the sound provided. For instance, if the color is "blue" and the sound is "whistle", you might imagine a bluebird whistling a tune.
  3. Describe the action, event or scene you imagined in a single, concise sentence. Focus on making the sentence cute, evocative and imaginative. For example: "A cheerful bluebird whistled a merry melody as it soared through the azure sky."
  Please keep your story to one sentence only. Aim to make that sentence as charming and engaging as possible while naturally incorporating the given color and sound.
  Write your completed one sentence story inside <story> tags.

  ```This structure makes it easy to vary inputs (\{\{COLOR}} and \{\{SOUND}}) and evaluate outputs consistently.
</Accordion>

<Tip>
 Use the 'Generate a prompt' helper tool in the Console to quickly create prompts with the appropriate variable syntax for evaluation.
</Tip>

## Understanding and comparing results

The Evaluation tool offers several features to help you refine your prompts:

1. **Side-by-side comparison**: Compare the outputs of two or more prompts to quickly see the impact of your changes.
2. **Quality grading**: Grade response quality on a 5-point scale to track improvements in response quality per prompt.
3. **Prompt versioning**: Create new versions of your prompt and re-run the test suite to quickly iterate and improve results.

By reviewing results across test cases and comparing different prompt versions, you can spot patterns and make informed adjustments to your prompt more efficiently.

Start evaluating your prompts today to build more robust AI applications with Claude!

[END OF DOCUMENT: CLAUDETE7IYFSAY]
---

[START OF DOCUMENT: CLAUDERXOC43O90 | Title: Extended-Thinking-Tips]

# Extended thinking tips

export const TryInConsoleButton = ({userPrompt, systemPrompt, maxTokens, thinkingBudgetTokens, buttonVariant = "primary", children}) => {
 const url = new URL("https://console.anthropic.com/workbench/new");
 if (userPrompt) {
 url.searchParams.set("user", userPrompt);
 }
 if (systemPrompt) {
 url.searchParams.set("system", systemPrompt);
 }
 if (maxTokens) {
 url.searchParams.set("max_tokens", maxTokens);
 }
 if (thinkingBudgetTokens) {
 url.searchParams.set("thinking.budget_tokens", thinkingBudgetTokens);
 }
 return <div style={{
 width: "100%",
 position: "relative",
 top: "-77px",
 textAlign: "right"
 }}>
 <a href={url.href} className={`btn size-xs ${buttonVariant}`} style={{
 position: "relative",
 right: "20px",
 zIndex: "10"
 }}>
 {children || "Try in Console"}{" "}
 <Icon icon="arrow-up-right" color="currentColor" size={14} />
 </a>
 </div>;
};

This guide provides advanced strategies and techniques for getting the most out of Claude's extended thinking features. Extended thinking allows Claude to work through complex problems step-by-step, improving performance on difficult tasks.

See [Extended thinking models](/en/docs/about-claude/models/extended-thinking-models) for guidance on deciding when to use extended thinking.

## Before diving in

This guide presumes that you have already decided to use extended thinking mode and have reviewed our basic steps on [how to get started with extended thinking](/en/docs/about-claude/models/extended-thinking-models#getting-started-with-extended-thinking-models) as well as our [extended thinking implementation guide](/en/docs/build-with-claude/extended-thinking).

### Technical considerations for extended thinking

* Thinking tokens have a minimum budget of 1024 tokens. We recommend that you start with the minimum thinking budget and incrementally increase to adjust based on your needs and task complexity.
* For workloads where the optimal thinking budget is above 32K, we recommend that you use [batch processing](/en/docs/build-with-claude/batch-processing) to avoid networking issues. Requests pushing the model to think above 32K tokens causes long running requests that might run up against system timeouts and open connection limits.
* Extended thinking performs best in English, though final outputs can be in [any language Claude supports](/en/docs/build-with-claude/multilingual-support).
* If you need thinking below the minimum budget, we recommend using standard mode, with thinking turned off, with traditional chain-of-thought prompting with XML tags (like `<thinking>`). See [chain of thought prompting](/en/docs/build-with-claude/prompt-engineering/chain-of-thought).

## Prompting techniques for extended thinking

### Use general instructions first, then troubleshoot with more step-by-step instructions

Claude often performs better with high level instructions to just think deeply about a task rather than step-by-step prescriptive guidance. The model's creativity in approaching problems may exceed a human's ability to prescribe the optimal thinking process.

For example, instead of:

<CodeGroup>```text User theme={null}
  Think through this math problem step by step: 
  1. First, identify the variables
  2. Then, set up the equation
  3. Next, solve for x
  ...
  ```</CodeGroup>

Consider:

<CodeGroup>```text User theme={null}
  Please think about this math problem thoroughly and in great detail. 
  Consider multiple approaches and show your complete reasoning.
  Try different methods if your first approach doesn't work.
  ```/>
</CodeGroup>

<TryInConsoleButton
 userPrompt={
 `Please think about this math problem thoroughly and in great detail.
Consider multiple approaches and show your complete reasoning.
Try different methods if your first approach doesn't work.`
}
 thinkingBudgetTokens={16000}
>
 Try in Console
</TryInConsoleButton>

That said, Claude can still effectively follow complex structured execution steps when needed. The model can handle even longer lists with more complex instructions than previous versions. We recommend that you start with more generalized instructions, then read Claude's thinking output and iterate to provide more specific instructions to steer its thinking from there.

### Multishot prompting with extended thinking

[Multishot prompting](/en/docs/build-with-claude/prompt-engineering/multishot-prompting) works well with extended thinking. When you provide Claude examples of how to think through problems, it will follow similar reasoning patterns within its extended thinking blocks.

You can include few-shot examples in your prompt in extended thinking scenarios by using XML tags like `<thinking>` or `<scratchpad>` to indicate canonical patterns of extended thinking in those examples.

Claude will generalize the pattern to the formal extended thinking process. However, it's possible you'll get better results by giving Claude free rein to think in the way it deems best.

Example:

<CodeGroup>```text User theme={null}
  I'm going to show you how to solve a math problem, then I want you to solve a similar one.

  Problem 1: What is 15% of 80?

  <thinking>
  To find 15% of 80:
  1. Convert 15% to a decimal: 15% = 0.15
  2. Multiply: 0.15 × 80 = 12
  </thinking>

  The answer is 12.

  Now solve this one:
  Problem 2: What is 35% of 240?
  ```/>
</CodeGroup>

<TryInConsoleButton
 userPrompt={
 `I'm going to show you how to solve a math problem, then I want you to solve a similar one.

Problem 1: What is 15% of 80?

<thinking>
To find 15% of 80:
1. Convert 15% to a decimal: 15% = 0.15
2. Multiply: 0.15 × 80 = 12
</thinking>

The answer is 12.

Now solve this one:
Problem 2: What is 35% of 240?`
}
 thinkingBudgetTokens={16000}
>
 Try in Console
</TryInConsoleButton>

### Maximizing instruction following with extended thinking

Claude shows significantly improved instruction following when extended thinking is enabled. The model typically:

1. Reasons about instructions inside the extended thinking block
2. Executes those instructions in the response

To maximize instruction following:

* Be clear and specific about what you want
* For complex instructions, consider breaking them into numbered steps that Claude should work through methodically
* Allow Claude enough budget to process the instructions fully in its extended thinking

### Using extended thinking to debug and steer Claude's behavior

You can use Claude's thinking output to debug Claude's logic, although this method is not always perfectly reliable.

To make the best use of this methodology, we recommend the following tips:

* We don't recommend passing Claude's extended thinking back in the user text block, as this doesn't improve performance and may actually degrade results.
* Prefilling extended thinking is explicitly not allowed, and manually changing the model's output text that follows its thinking block is likely going to degrade results due to model confusion.

When extended thinking is turned off, standard `assistant` response text [prefill](/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response) is still allowed.

<Note>
 Sometimes Claude may repeat its extended thinking in the assistant output text. If you want a clean response, instruct Claude not to repeat its extended thinking and to only output the answer.
</Note>

### Making the best of long outputs and longform thinking

For dataset generation use cases, try prompts such as "Please create an extremely detailed table of..." for generating comprehensive datasets.

For use cases such as detailed content generation where you may want to generate longer extended thinking blocks and more detailed responses, try these tips:

* Increase both the maximum extended thinking length AND explicitly ask for longer outputs
* For very long outputs (20,000+ words), request a detailed outline with word counts down to the paragraph level. Then ask Claude to index its paragraphs to the outline and maintain the specified word counts

<Warning>
 We do not recommend that you push Claude to output more tokens for outputting tokens' sake. Rather, we encourage you to start with a small thinking budget and increase as needed to find the optimal settings for your use case.
</Warning>

Here are example use cases where Claude excels due to longer extended thinking:

<AccordionGroup>
 <Accordion title="Complex STEM problems">
 Complex STEM problems require Claude to build mental models, apply specialized knowledge, and work through sequential logical steps—processes that benefit from longer reasoning time.

 <Tabs>
 <Tab title="Standard prompt">
 <CodeGroup>```text User theme={null}
          Write a python script for a bouncing yellow ball within a square,
          make sure to handle collision detection properly.
          Make the square slowly rotate.
          ```/>
 </CodeGroup>

 <TryInConsoleButton
 userPrompt={
 `Write a python script for a bouncing yellow ball within a square,
make sure to handle collision detection properly.
Make the square slowly rotate.`
 }
 thinkingBudgetTokens={16000}
 >
 Try in Console
 </TryInConsoleButton>

 <Note>
 This simpler task typically results in only about a few seconds of thinking time.
 </Note>
 </Tab>

 <Tab title="Enhanced prompt">
 <CodeGroup>```text User theme={null}
          Write a Python script for a bouncing yellow ball within a tesseract, 
          making sure to handle collision detection properly. 
          Make the tesseract slowly rotate. 
          Make sure the ball stays within the tesseract.
          ```/>
 </CodeGroup>

 <TryInConsoleButton
 userPrompt={
 `Write a Python script for a bouncing yellow ball within a tesseract,
making sure to handle collision detection properly.
Make the tesseract slowly rotate.
Make sure the ball stays within the tesseract.`
 }
 thinkingBudgetTokens={16000}
 >
 Try in Console
 </TryInConsoleButton>

 <Note>
 This complex 4D visualization challenge makes the best use of long extended thinking time as Claude works through the mathematical and programming complexity.
 </Note>
 </Tab>
 </Tabs>
 </Accordion>

 <Accordion title="Constraint optimization problems">
 Constraint optimization challenges Claude to satisfy multiple competing requirements simultaneously, which is best accomplished when allowing for long extended thinking time so that the model can methodically address each constraint.

 <Tabs>
 <Tab title="Standard prompt">
 <CodeGroup>```text User theme={null}
          Plan a week-long vacation to Japan.
          ```/>
 </CodeGroup>

 <TryInConsoleButton userPrompt="Plan a week-long vacation to Japan." thinkingBudgetTokens={16000}>
 Try in Console
 </TryInConsoleButton>

 <Note>
 This open-ended request typically results in only about a few seconds of thinking time.
 </Note>
 </Tab>

 <Tab title="Enhanced prompt">
 <CodeGroup>```text User theme={null}
          Plan a 7-day trip to Japan with the following constraints:
          - Budget of $2,500
          - Must include Tokyo and Kyoto
          - Need to accommodate a vegetarian diet
          - Preference for cultural experiences over shopping
          - Must include one day of hiking
          - No more than 2 hours of travel between locations per day
          - Need free time each afternoon for calls back home
          - Must avoid crowds where possible
          ```/>
 </CodeGroup>

 <TryInConsoleButton
 userPrompt={
 `Plan a 7-day trip to Japan with the following constraints:
- Budget of $2,500
- Must include Tokyo and Kyoto
- Need to accommodate a vegetarian diet
- Preference for cultural experiences over shopping
- Must include one day of hiking
- No more than 2 hours of travel between locations per day
- Need free time each afternoon for calls back home
- Must avoid crowds where possible`
 }
 thinkingBudgetTokens={16000}
 >
 Try in Console
 </TryInConsoleButton>

 <Note>
 With multiple constraints to balance, Claude will naturally perform best when given more space to think through how to satisfy all requirements optimally.
 </Note>
 </Tab>
 </Tabs>
 </Accordion>

 <Accordion title="Thinking frameworks">
 Structured thinking frameworks give Claude an explicit methodology to follow, which may work best when Claude is given long extended thinking space to follow each step.

 <Tabs>
 <Tab title="Standard prompt">
 <CodeGroup>```text User theme={null}
          Develop a comprehensive strategy for Microsoft 
          entering the personalized medicine market by 2027.
          ```/>
 </CodeGroup>

 <TryInConsoleButton
 userPrompt={
 `Develop a comprehensive strategy for Microsoft
entering the personalized medicine market by 2027.`
 }
 thinkingBudgetTokens={16000}
 >
 Try in Console
 </TryInConsoleButton>

 <Note>
 This broad strategic question typically results in only about a few seconds of thinking time.
 </Note>
 </Tab>

 <Tab title="Enhanced prompt">
 <CodeGroup>```text User theme={null}
          Develop a comprehensive strategy for Microsoft entering 
          the personalized medicine market by 2027.

          Begin with:
          1. A Blue Ocean Strategy canvas
          2. Apply Porter's Five Forces to identify competitive pressures

          Next, conduct a scenario planning exercise with four 
          distinct futures based on regulatory and technological variables.

          For each scenario:
          - Develop strategic responses using the Ansoff Matrix

          Finally, apply the Three Horizons framework to:
          - Map the transition pathway
          - Identify potential disruptive innovations at each stage
          ```/>
 </CodeGroup>

 <TryInConsoleButton
 userPrompt={
 `Develop a comprehensive strategy for Microsoft entering
the personalized medicine market by 2027.

Begin with:
1. A Blue Ocean Strategy canvas
2. Apply Porter's Five Forces to identify competitive pressures

Next, conduct a scenario planning exercise with four
distinct futures based on regulatory and technological variables.

For each scenario:
- Develop strategic responses using the Ansoff Matrix

Finally, apply the Three Horizons framework to:
- Map the transition pathway
- Identify potential disruptive innovations at each stage`
 }
 thinkingBudgetTokens={16000}
 >
 Try in Console
 </TryInConsoleButton>

 <Note>
 By specifying multiple analytical frameworks that must be applied sequentially, thinking time naturally increases as Claude works through each framework methodically.
 </Note>
 </Tab>
 </Tabs>
 </Accordion>
</AccordionGroup>

### Have Claude reflect on and check its work for improved consistency and error handling

You can use simple natural language prompting to improve consistency and reduce errors:

1. Ask Claude to verify its work with a simple test before declaring a task complete
2. Instruct the model to analyze whether its previous step achieved the expected result
3. For coding tasks, ask Claude to run through test cases in its extended thinking

Example:

<CodeGroup>```text User theme={null}
  Write a function to calculate the factorial of a number. 
  Before you finish, please verify your solution with test cases for:
  - n=0
  - n=1
  - n=5
  - n=10
  And fix any issues you find.
  ```/>
</CodeGroup>

<TryInConsoleButton
 userPrompt={
 `Write a function to calculate the factorial of a number.
Before you finish, please verify your solution with test cases for:
- n=0
- n=1
- n=5
- n=10
And fix any issues you find.`
}
 thinkingBudgetTokens={16000}
>
 Try in Console
</TryInConsoleButton>

## Next steps

<CardGroup>
 <Card title="Extended thinking cookbook" icon="book" href="https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking">
 Explore practical examples of extended thinking in our cookbook.
 </Card>

 <Card title="Extended thinking guide" icon="code" href="/en/docs/build-with-claude/extended-thinking">
 See complete technical documentation for implementing extended thinking.
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDERXOC43O90]
---

[START OF DOCUMENT: CLAUDEGK5BSPX3H | Title: Extended-Thinking]

# Building with extended thinking

export const TryInConsoleButton = ({userPrompt, systemPrompt, maxTokens, thinkingBudgetTokens, buttonVariant = "primary", children}) => {
 const url = new URL("https://console.anthropic.com/workbench/new");
 if (userPrompt) {
 url.searchParams.set("user", userPrompt);
 }
 if (systemPrompt) {
 url.searchParams.set("system", systemPrompt);
 }
 if (maxTokens) {
 url.searchParams.set("max_tokens", maxTokens);
 }
 if (thinkingBudgetTokens) {
 url.searchParams.set("thinking.budget_tokens", thinkingBudgetTokens);
 }
 return <div style={{
 width: "100%",
 position: "relative",
 top: "-77px",
 textAlign: "right"
 }}>
 <a href={url.href} className={`btn size-xs ${buttonVariant}`} style={{
 position: "relative",
 right: "20px",
 zIndex: "10"
 }}>
 {children || "Try in Console"}{" "}
 <Icon icon="arrow-up-right" color="currentColor" size={14} />
 </a>
 </div>;
};

Extended thinking gives Claude enhanced reasoning capabilities for complex tasks, while providing varying levels of transparency into its step-by-step thought process before it delivers its final answer.

## Supported models

Extended thinking is supported in the following models:

* Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
* Claude Sonnet 4 (`claude-sonnet-4-20250514`)
* Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([deprecated](/en/docs/about-claude/model-deprecations))
* Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)
* Claude Opus 4.1 (`claude-opus-4-1-20250805`)
* Claude Opus 4 (`claude-opus-4-20250514`)

<Note>
 API behavior differs across Claude Sonnet 3.7 and Claude 4 models, but the API shapes remain exactly the same.

 For more information, see [Differences in thinking across model versions](#differences-in-thinking-across-model-versions).
</Note>

## How extended thinking works

When extended thinking is turned on, Claude creates `thinking` content blocks where it outputs its internal reasoning. Claude incorporates insights from this reasoning before crafting a final response.

The API response will include `thinking` content blocks, followed by `text` content blocks.

Here's an example of the default response format:```json  theme={null}
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```For more information about the response format of extended thinking, see the [Messages API Reference](/en/api/messages).

## How to use extended thinking

Here is an example of using extended thinking in the Messages API:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 16000,
      "thinking": {
          "type": "enabled",
          "budget_tokens": 10000
      },
      "messages": [
          {
              "role": "user",
              "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
          }
      ]
  }'
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  response = client.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=16000,
      thinking={
          "type": "enabled",
          "budget_tokens": 10000
      },
      messages=[{
          "role": "user",
          "content": "Are there an infinite number of prime numbers such that n mod 4 == 3?"
      }]
  )

  # The response will contain summarized thinking blocks and text blocks
  for block in response.content:
      if block.type == "thinking":
          print(f"\nThinking summary: {block.thinking}")
      elif block.type == "text":
          print(f"\nResponse: {block.text}")
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const client = new Anthropic();

  const response = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 16000,
    thinking: {
      type: "enabled",
      budget_tokens: 10000
    },
    messages: [{
      role: "user",
      content: "Are there an infinite number of prime numbers such that n mod 4 == 3?"
    }]
  });

  // The response will contain summarized thinking blocks and text blocks
  for (const block of response.content) {
    if (block.type === "thinking") {
      console.log(`\nThinking summary: ${block.thinking}`);
    } else if (block.type === "text") {
      console.log(`\nResponse: ${block.text}`);
    }
  }
  ``````java Java theme={null}
  import com.anthropic.client.AnthropicClient;
  import com.anthropic.client.okhttp.AnthropicOkHttpClient;
  import com.anthropic.models.beta.messages.*;
  import com.anthropic.models.beta.messages.MessageCreateParams;
  import com.anthropic.models.messages.*;

  public class SimpleThinkingExample {
      public static void main(String[] args) {
          AnthropicClient client = AnthropicOkHttpClient.fromEnv();

          BetaMessage response = client.beta().messages().create(
                  MessageCreateParams.builder()
                          .model(Model.CLAUDE_OPUS_4_0)
                          .maxTokens(16000)
                          .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                          .addUserMessage("Are there an infinite number of prime numbers such that n mod 4 == 3?")
                          .build()
          );

          System.out.println(response);
      }
  }
  ```</CodeGroup>

To turn on extended thinking, add a `thinking` object, with the `type` parameter set to `enabled` and the `budget_tokens` to a specified token budget for extended thinking.

The `budget_tokens` parameter determines the maximum number of tokens Claude is allowed to use for its internal reasoning process. In Claude 4 models, this limit applies to full thinking tokens, and not to [the summarized output](#summarized-thinking). Larger budgets can improve response quality by enabling more thorough analysis for complex problems, although Claude may not use the entire budget allocated, especially at ranges above 32k.

`budget_tokens` must be set to a value less than `max_tokens`. However, when using [interleaved thinking with tools](#interleaved-thinking), you can exceed this limit as the token limit becomes your entire context window (200k tokens).

### Summarized thinking

With extended thinking enabled, the Messages API for Claude 4 models returns a summary of Claude's full thinking process. Summarized thinking provides the full intelligence benefits of extended thinking, while preventing misuse.

Here are some important considerations for summarized thinking:

* You're charged for the full thinking tokens generated by the original request, not the summary tokens.
* The billed output token count will **not match** the count of tokens you see in the response.
* The first few lines of thinking output are more verbose, providing detailed reasoning that's particularly helpful for prompt engineering purposes.
* As Anthropic seeks to improve the extended thinking feature, summarization behavior is subject to change.
* Summarization preserves the key ideas of Claude's thinking process with minimal added latency, enabling a streamable user experience and easy migration from Claude Sonnet 3.7 to Claude 4 models.
* Summarization is processed by a different model than the one you target in your requests. The thinking model does not see the summarized output.

<Note>
 Claude Sonnet 3.7 continues to return full thinking output.

 In rare cases where you need access to full thinking output for Claude 4 models, [contact our sales team](mailto:sales@anthropic.com).
</Note>

### Streaming thinking

You can stream extended thinking responses using [server-sent events (SSE)](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent%5Fevents/Using%5Fserver-sent%5Fevents).

When streaming is enabled for extended thinking, you receive thinking content via `thinking_delta` events.

For more documention on streaming via the Messages API, see [Streaming Messages](/en/docs/build-with-claude/streaming).

Here's how to handle streaming with thinking:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data \
  '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 16000,
      "stream": true,
      "thinking": {
          "type": "enabled",
          "budget_tokens": 10000
      },
      "messages": [
          {
              "role": "user",
              "content": "What is 27 * 453?"
          }
      ]
  }'
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  with client.messages.stream(
      model="claude-sonnet-4-5",
      max_tokens=16000,
      thinking={"type": "enabled", "budget_tokens": 10000},
      messages=[{"role": "user", "content": "What is 27 * 453?"}],
  ) as stream:
      thinking_started = False
      response_started = False

      for event in stream:
          if event.type == "content_block_start":
              print(f"\nStarting {event.content_block.type} block...")
              # Reset flags for each new block
              thinking_started = False
              response_started = False
          elif event.type == "content_block_delta":
              if event.delta.type == "thinking_delta":
                  if not thinking_started:
                      print("Thinking: ", end="", flush=True)
                      thinking_started = True
                  print(event.delta.thinking, end="", flush=True)
              elif event.delta.type == "text_delta":
                  if not response_started:
                      print("Response: ", end="", flush=True)
                      response_started = True
                  print(event.delta.text, end="", flush=True)
          elif event.type == "content_block_stop":
              print("\nBlock complete.")
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const client = new Anthropic();

  const stream = await client.messages.stream({
    model: "claude-sonnet-4-5",
    max_tokens: 16000,
    thinking: {
      type: "enabled",
      budget_tokens: 10000
    },
    messages: [{
      role: "user",
      content: "What is 27 * 453?"
    }]
  });

  let thinkingStarted = false;
  let responseStarted = false;

  for await (const event of stream) {
    if (event.type === 'content_block_start') {
      console.log(`\nStarting ${event.content_block.type} block...`);
      // Reset flags for each new block
      thinkingStarted = false;
      responseStarted = false;
    } else if (event.type === 'content_block_delta') {
      if (event.delta.type === 'thinking_delta') {
        if (!thinkingStarted) {
          process.stdout.write('Thinking: ');
          thinkingStarted = true;
        }
        process.stdout.write(event.delta.thinking);
      } else if (event.delta.type === 'text_delta') {
        if (!responseStarted) {
          process.stdout.write('Response: ');
          responseStarted = true;
        }
        process.stdout.write(event.delta.text);
      }
    } else if (event.type === 'content_block_stop') {
      console.log('\nBlock complete.');
    }
  }
  ``````java Java theme={null}
  import com.anthropic.client.AnthropicClient;
  import com.anthropic.client.okhttp.AnthropicOkHttpClient;
  import com.anthropic.core.http.StreamResponse;
  import com.anthropic.models.beta.messages.MessageCreateParams;
  import com.anthropic.models.beta.messages.BetaRawMessageStreamEvent;
  import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
  import com.anthropic.models.messages.Model;

  public class SimpleThinkingStreamingExample {
      private static boolean thinkingStarted = false;
      private static boolean responseStarted = false;
      
      public static void main(String[] args) {
          AnthropicClient client = AnthropicOkHttpClient.fromEnv();

          MessageCreateParams createParams = MessageCreateParams.builder()
                  .model(Model.CLAUDE_OPUS_4_0)
                  .maxTokens(16000)
                  .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                  .addUserMessage("What is 27 * 453?")
                  .build();

          try (StreamResponse<BetaRawMessageStreamEvent> streamResponse =
                       client.beta().messages().createStreaming(createParams)) {
              streamResponse.stream()
                      .forEach(event -> {
                          if (event.isContentBlockStart()) {
                              System.out.printf("\nStarting %s block...%n",
                                      event.asContentBlockStart()._type());
                              // Reset flags for each new block
                              thinkingStarted = false;
                              responseStarted = false;
                          } else if (event.isContentBlockDelta()) {
                              var delta = event.asContentBlockDelta().delta();
                              if (delta.isBetaThinking()) {
                                  if (!thinkingStarted) {
                                      System.out.print("Thinking: ");
                                      thinkingStarted = true;
                                  }
                                  System.out.print(delta.asBetaThinking().thinking());
                                  System.out.flush();
                              } else if (delta.isBetaText()) {
                                  if (!responseStarted) {
                                      System.out.print("Response: ");
                                      responseStarted = true;
                                  }
                                  System.out.print(delta.asBetaText().text());
                                  System.out.flush();
                              }
                          } else if (event.isContentBlockStop()) {
                              System.out.println("\nBlock complete.");
                          }
                      });
          }
      }
  }
  ```</CodeGroup>

<TryInConsoleButton userPrompt="What is 27 * 453?" thinkingBudgetTokens={16000}>
 Try in Console
</TryInConsoleButton>

Example streaming output:```json  theme={null}
event: message_start
data: {"type": "message_start", "message": {"id": "msg_01...", "type": "message", "role": "assistant", "content": [], "model": "claude-sonnet-4-5", "stop_reason": null, "stop_sequence": null}}

event: content_block_start
data: {"type": "content_block_start", "index": 0, "content_block": {"type": "thinking", "thinking": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "Let me solve this step by step:\n\n1. First break down 27 * 453"}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "thinking_delta", "thinking": "\n2. 453 = 400 + 50 + 3"}}

// Additional thinking deltas...

event: content_block_delta
data: {"type": "content_block_delta", "index": 0, "delta": {"type": "signature_delta", "signature": "EqQBCgIYAhIM1gbcDa9GJwZA2b3hGgxBdjrkzLoky3dl1pkiMOYds..."}}

event: content_block_stop
data: {"type": "content_block_stop", "index": 0}

event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "text", "text": ""}}

event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "text_delta", "text": "27 * 453 = 12,231"}}

// Additional text deltas...

event: content_block_stop
data: {"type": "content_block_stop", "index": 1}

event: message_delta
data: {"type": "message_delta", "delta": {"stop_reason": "end_turn", "stop_sequence": null}}

event: message_stop
data: {"type": "message_stop"}
```<Note>
 When using streaming with thinking enabled, you might notice that text sometimes arrives in larger chunks alternating with smaller, token-by-token delivery. This is expected behavior, especially for thinking content.

 The streaming system needs to process content in batches for optimal performance, which can result in this "chunky" delivery pattern, with possible delays between streaming events. We're continuously working to improve this experience, with future updates focused on making thinking content stream more smoothly.
</Note>

## Extended thinking with tool use

Extended thinking can be used alongside [tool use](/en/docs/agents-and-tools/tool-use/overview), allowing Claude to reason through tool selection and results processing.

When using extended thinking with tool use, be aware of the following limitations:

1. **Tool choice limitation**: Tool use with thinking only supports `tool_choice: {"type": "auto"}` (the default) or `tool_choice: {"type": "none"}`. Using `tool_choice: {"type": "any"}` or `tool_choice: {"type": "tool", "name": "..."}` will result in an error because these options force tool use, which is incompatible with extended thinking.

2. **Preserving thinking blocks**: During tool use, you must pass `thinking` blocks back to the API for the last assistant message. Include the complete unmodified block back to the API to maintain reasoning continuity.

### Toggling thinking modes in conversations

You cannot toggle thinking in the middle of an assistant turn, including during tool use loops. The entire assistant turn must operate in a single thinking mode:

* **If thinking is enabled**, the final assistant turn must start with a thinking block.
* **If thinking is disabled**, the final assistant turn must not contain any thinking blocks

From the model's perspective, **tool use loops are part of the assistant turn**. An assistant turn doesn't complete until Claude finishes its full response, which may include multiple tool calls and results.

For example, this sequence is all part of a **single assistant turn**:```
User: "What's the weather in Paris?"
Assistant: [thinking] + [tool_use: get_weather]
User: [tool_result: "20°C, sunny"]
Assistant: [text: "The weather in Paris is 20°C and sunny"]
```Even though there are multiple API messages, the tool use loop is conceptually part of one continuous assistant response.

#### Common error scenarios

You might encounter this error:```
Expected `thinking` or `redacted_thinking`, but found `tool_use`.
When `thinking` is enabled, a final `assistant` message must start
with a thinking block (preceding the lastmost set of `tool_use` and
`tool_result` blocks).
```This typically occurs when:

1. You had thinking **disabled** during a tool use sequence
2. You want to enable thinking again
3. Your last assistant message contains tool use blocks but no thinking block

#### Practical guidance

**✗ Invalid: Toggling thinking immediately after tool use**```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
// Cannot enable thinking here - still in the same assistant turn
```**✓ Valid: Complete the assistant turn first**```
User: "What's the weather?"
Assistant: [tool_use] (thinking disabled)
User: [tool_result]
Assistant: [text: "It's sunny"] 
User: "What about tomorrow?" (thinking disabled)
Assistant: [thinking] + [text: "..."] (thinking enabled - new turn)
```**Best practice**: Plan your thinking strategy at the start of each turn rather than trying to toggle mid-turn.

<Note>
 Toggling thinking modes also invalidates prompt caching for message history. For more details, see the [Extended thinking with prompt caching](#extended-thinking-with-prompt-caching) section.
</Note>

<AccordionGroup>
 <Accordion title="Example: Passing thinking blocks with tool results">
 Here's a practical example showing how to preserve thinking blocks when providing tool results:

 <CodeGroup>```python Python theme={null}
      weather_tool = {
          "name": "get_weather",
          "description": "Get current weather for a location",
          "input_schema": {
              "type": "object",
              "properties": {
                  "location": {"type": "string"}
              },
              "required": ["location"]
          }
      }

      # First request - Claude responds with thinking and tool request
      response = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          tools=[weather_tool],
          messages=[
              {"role": "user", "content": "What's the weather in Paris?"}
          ]
      )
      ``````typescript TypeScript theme={null}
      const weatherTool = {
        name: "get_weather",
        description: "Get current weather for a location",
        input_schema: {
          type: "object",
          properties: {
            location: { type: "string" }
          },
          required: ["location"]
        }
      };

      // First request - Claude responds with thinking and tool request
      const response = await client.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        tools: [weatherTool],
        messages: [
          { role: "user", content: "What's the weather in Paris?" }
        ]
      });
      ``````java Java theme={null}
      import java.util.List;
      import java.util.Map;

      import com.anthropic.client.AnthropicClient;
      import com.anthropic.client.okhttp.AnthropicOkHttpClient;
      import com.anthropic.core.JsonValue;
      import com.anthropic.models.beta.messages.BetaMessage;
      import com.anthropic.models.beta.messages.MessageCreateParams;
      import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
      import com.anthropic.models.beta.messages.BetaTool;
      import com.anthropic.models.beta.messages.BetaTool.InputSchema;
      import com.anthropic.models.messages.Model;

      public class ThinkingWithToolsExample {
          public static void main(String[] args) {
              AnthropicClient client = AnthropicOkHttpClient.fromEnv();

              InputSchema schema = InputSchema.builder()
                      .properties(JsonValue.from(Map.of(
                              "location", Map.of("type", "string")
                      )))
                      .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                      .build();

              BetaTool weatherTool = BetaTool.builder()
                      .name("get_weather")
                      .description("Get current weather for a location")
                      .inputSchema(schema)
                      .build();

              BetaMessage response = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                              .addTool(weatherTool)
                              .addUserMessage("What's the weather in Paris?")
                              .build()
              );

              System.out.println(response);
          }
      }
      ```</CodeGroup>

 The API response will include thinking, text, and tool\_use blocks:```json  theme={null}
    {
        "content": [
            {
                "type": "thinking",
                "thinking": "The user wants to know the current weather in Paris. I have access to a function `get_weather`...",
                "signature": "BDaL4VrbR2Oj0hO4XpJxT28J5TILnCrrUXoKiiNBZW9P+nr8XSj1zuZzAl4egiCCpQNvfyUuFFJP5CncdYZEQPPmLxYsNrcs...."
            },
            {
                "type": "text",
                "text": "I can help you get the current weather information for Paris. Let me check that for you"
            },
            {
                "type": "tool_use",
                "id": "toolu_01CswdEQBMshySk6Y9DFKrfq",
                "name": "get_weather",
                "input": {
                    "location": "Paris"
                }
            }
        ]
    }
    ```Now let's continue the conversation and use the tool

 <CodeGroup>```python Python theme={null}
      # Extract thinking block and tool use block
      thinking_block = next((block for block in response.content
                            if block.type == 'thinking'), None)
      tool_use_block = next((block for block in response.content
                            if block.type == 'tool_use'), None)

      # Call your actual weather API, here is where your actual API call would go
      # let's pretend this is what we get back
      weather_data = {"temperature": 88}

      # Second request - Include thinking block and tool result
      # No new thinking blocks will be generated in the response
      continuation = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          tools=[weather_tool],
          messages=[
              {"role": "user", "content": "What's the weather in Paris?"},
              # notice that the thinking_block is passed in as well as the tool_use_block
              # if this is not passed in, an error is raised
              {"role": "assistant", "content": [thinking_block, tool_use_block]},
              {"role": "user", "content": [{
                  "type": "tool_result",
                  "tool_use_id": tool_use_block.id,
                  "content": f"Current temperature: {weather_data['temperature']}°F"
              }]}
          ]
      )
      ``````typescript TypeScript theme={null}
      // Extract thinking block and tool use block
      const thinkingBlock = response.content.find(block =>
        block.type === 'thinking');
      const toolUseBlock = response.content.find(block =>
        block.type === 'tool_use');

      // Call your actual weather API, here is where your actual API call would go
      // let's pretend this is what we get back
      const weatherData = { temperature: 88 };

      // Second request - Include thinking block and tool result
      // No new thinking blocks will be generated in the response
      const continuation = await client.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        tools: [weatherTool],
        messages: [
          { role: "user", content: "What's the weather in Paris?" },
          // notice that the thinkingBlock is passed in as well as the toolUseBlock
          // if this is not passed in, an error is raised
          { role: "assistant", content: [thinkingBlock, toolUseBlock] },
          { role: "user", content: [{
            type: "tool_result",
            tool_use_id: toolUseBlock.id,
            content: `Current temperature: ${weatherData.temperature}°F`
          }]}
        ]
      });
      ``````java Java theme={null}
      import java.util.List;
      import java.util.Map;
      import java.util.Optional;

      import com.anthropic.client.AnthropicClient;
      import com.anthropic.client.okhttp.AnthropicOkHttpClient;
      import com.anthropic.core.JsonValue;
      import com.anthropic.models.beta.messages.*;
      import com.anthropic.models.beta.messages.BetaTool.InputSchema;
      import com.anthropic.models.messages.Model;

      public class ThinkingToolsResultExample {
          public static void main(String[] args) {
              AnthropicClient client = AnthropicOkHttpClient.fromEnv();

              InputSchema schema = InputSchema.builder()
                      .properties(JsonValue.from(Map.of(
                              "location", Map.of("type", "string")
                      )))
                      .putAdditionalProperty("required", JsonValue.from(List.of("location")))
                      .build();

              BetaTool weatherTool = BetaTool.builder()
                      .name("get_weather")
                      .description("Get current weather for a location")
                      .inputSchema(schema)
                      .build();

              BetaMessage response = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                              .addTool(weatherTool)
                              .addUserMessage("What's the weather in Paris?")
                              .build()
              );

              // Extract thinking block and tool use block
              Optional<BetaThinkingBlock> thinkingBlockOpt = response.content().stream()
                      .filter(BetaContentBlock::isThinking)
                      .map(BetaContentBlock::asThinking)
                      .findFirst();

              Optional<BetaToolUseBlock> toolUseBlockOpt = response.content().stream()
                      .filter(BetaContentBlock::isToolUse)
                      .map(BetaContentBlock::asToolUse)
                      .findFirst();

              if (thinkingBlockOpt.isPresent() && toolUseBlockOpt.isPresent()) {
                  BetaThinkingBlock thinkingBlock = thinkingBlockOpt.get();
                  BetaToolUseBlock toolUseBlock = toolUseBlockOpt.get();

                  // Call your actual weather API, here is where your actual API call would go
                  // let's pretend this is what we get back
                  Map<String, Object> weatherData = Map.of("temperature", 88);

                  // Second request - Include thinking block and tool result
                  // No new thinking blocks will be generated in the response
                  BetaMessage continuation = client.beta().messages().create(
                          MessageCreateParams.builder()
                                  .model(Model.CLAUDE_OPUS_4_0)
                                  .maxTokens(16000)
                                  .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                                  .addTool(weatherTool)
                                  .addUserMessage("What's the weather in Paris?")
                                  .addAssistantMessageOfBetaContentBlockParams(
                                          // notice that the thinkingBlock is passed in as well as the toolUseBlock
                                          // if this is not passed in, an error is raised
                                          List.of(
                                                  BetaContentBlockParam.ofThinking(thinkingBlock.toParam()),
                                                  BetaContentBlockParam.ofToolUse(toolUseBlock.toParam())
                                          )
                                  )
                                  .addUserMessageOfBetaContentBlockParams(List.of(
                                          BetaContentBlockParam.ofToolResult(
                                                  BetaToolResultBlockParam.builder()
                                                          .toolUseId(toolUseBlock.id())
                                                          .content(String.format("Current temperature: %d°F", (Integer)weatherData.get("temperature")))
                                                          .build()
                                          )
                                  ))
                                  .build()
                  );

                  System.out.println(continuation);
              }
          }
      }
      ```</CodeGroup>

 The API response will now **only** include text```json  theme={null}
    {
        "content": [
            {
                "type": "text",
                "text": "Currently in Paris, the temperature is 88°F (31°C)"
            }
        ]
    }
    ```</Accordion>
</AccordionGroup>

### Preserving thinking blocks

During tool use, you must pass `thinking` blocks back to the API, and you must include the complete unmodified block back to the API. This is critical for maintaining the model's reasoning flow and conversation integrity.

<Tip>
 While you can omit `thinking` blocks from prior `assistant` role turns, we suggest always passing back all thinking blocks to the API for any multi-turn conversation. The API will:

 * Automatically filter the provided thinking blocks
 * Use the relevant thinking blocks necessary to preserve the model's reasoning
 * Only bill for the input tokens for the blocks shown to Claude
</Tip>

<Note>
 When toggling thinking modes during a conversation, remember that the entire assistant turn (including tool use loops) must operate in a single thinking mode. For more details, see [Toggling thinking modes in conversations](#toggling-thinking-modes-in-conversations).
</Note>

When Claude invokes tools, it is pausing its construction of a response to await external information. When tool results are returned, Claude will continue building that existing response. This necessitates preserving thinking blocks during tool use, for a couple of reasons:

1. **Reasoning continuity**: The thinking blocks capture Claude's step-by-step reasoning that led to tool requests. When you post tool results, including the original thinking ensures Claude can continue its reasoning from where it left off.

2. **Context maintenance**: While tool results appear as user messages in the API structure, they're part of a continuous reasoning flow. Preserving thinking blocks maintains this conceptual flow across multiple API calls. For more information on context management, see our [guide on context windows](/en/docs/build-with-claude/context-windows).

**Important**: When providing `thinking` blocks, the entire sequence of consecutive `thinking` blocks must match the outputs generated by the model during the original request; you cannot rearrange or modify the sequence of these blocks.

### Interleaved thinking

Extended thinking with tool use in Claude 4 models supports interleaved thinking, which enables Claude to think between tool calls and make more sophisticated reasoning after receiving tool results.

With interleaved thinking, Claude can:

* Reason about the results of a tool call before deciding what to do next
* Chain multiple tool calls with reasoning steps in between
* Make more nuanced decisions based on intermediate results

To enable interleaved thinking, add [the beta header](/en/api/beta-headers) `interleaved-thinking-2025-05-14` to your API request.

Here are some important considerations for interleaved thinking:

* With interleaved thinking, the `budget_tokens` can exceed the `max_tokens` parameter, as it represents the total budget across all thinking blocks within one assistant turn.
* Interleaved thinking is only supported for [tools used via the Messages API](/en/docs/agents-and-tools/tool-use/overview).
* Interleaved thinking is supported for Claude 4 models only, with the beta header `interleaved-thinking-2025-05-14`.
* Direct calls to the Claude API allow you to pass `interleaved-thinking-2025-05-14` in requests to any model, with no effect.
* On 3rd-party platforms (e.g., [Amazon Bedrock](/en/docs/build-with-claude/claude-on-amazon-bedrock) and [Vertex AI](/en/docs/build-with-claude/claude-on-vertex-ai)), if you pass `interleaved-thinking-2025-05-14` to any model aside from Claude Opus 4.1, Opus 4, or Sonnet 4, your request will fail.

<AccordionGroup>
 <Accordion title="Tool use without interleaved thinking">
 <CodeGroup>```python Python theme={null}
      import anthropic

      client = anthropic.Anthropic()

      # Define tools
      calculator_tool = {
          "name": "calculator",
          "description": "Perform mathematical calculations",
          "input_schema": {
              "type": "object",
              "properties": {
                  "expression": {
                      "type": "string",
                      "description": "Mathematical expression to evaluate"
                  }
              },
              "required": ["expression"]
          }
      }

      database_tool = {
          "name": "database_query",
          "description": "Query product database",
          "input_schema": {
              "type": "object",
              "properties": {
                  "query": {
                      "type": "string",
                      "description": "SQL query to execute"
                  }
              },
              "required": ["query"]
          }
      }

      # First request - Claude thinks once before all tool calls
      response = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          tools=[calculator_tool, database_tool],
          messages=[{
              "role": "user",
              "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
          }]
      )

      # Response includes thinking followed by tool uses
      # Note: Claude thinks once at the beginning, then makes all tool decisions
      print("First response:")
      for block in response.content:
          if block.type == "thinking":
              print(f"Thinking (summarized): {block.thinking}")
          elif block.type == "tool_use":
              print(f"Tool use: {block.name} with input {block.input}")
          elif block.type == "text":
              print(f"Text: {block.text}")

      # You would execute the tools and return results...
      # After getting both tool results back, Claude directly responds without additional thinking
      ``````typescript TypeScript theme={null}
      import Anthropic from '@anthropic-ai/sdk';

      const client = new Anthropic();

      // Define tools
      const calculatorTool = {
        name: "calculator",
        description: "Perform mathematical calculations",
        input_schema: {
          type: "object",
          properties: {
            expression: {
              type: "string",
              description: "Mathematical expression to evaluate"
            }
          },
          required: ["expression"]
        }
      };

      const databaseTool = {
        name: "database_query",
        description: "Query product database",
        input_schema: {
          type: "object",
          properties: {
            query: {
              type: "string",
              description: "SQL query to execute"
            }
          },
          required: ["query"]
        }
      };

      // First request - Claude thinks once before all tool calls
      const response = await client.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        tools: [calculatorTool, databaseTool],
        messages: [{
          role: "user",
          content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        }]
      });

      // Response includes thinking followed by tool uses
      // Note: Claude thinks once at the beginning, then makes all tool decisions
      console.log("First response:");
      for (const block of response.content) {
        if (block.type === "thinking") {
          console.log(`Thinking (summarized): ${block.thinking}`);
        } else if (block.type === "tool_use") {
          console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
        } else if (block.type === "text") {
          console.log(`Text: ${block.text}`);
        }
      }

      // You would execute the tools and return results...
      // After getting both tool results back, Claude directly responds without additional thinking
      ``````java Java theme={null}
      import com.anthropic.client.AnthropicClient;
      import com.anthropic.client.okhttp.AnthropicOkHttpClient;
      import com.anthropic.core.JsonValue;
      import com.anthropic.models.beta.messages.*;
      import com.anthropic.models.messages.Model;
      import java.util.List;
      import java.util.Map;

      public class NonInterleavedThinkingExample {
          public static void main(String[] args) {
              AnthropicClient client = AnthropicOkHttpClient.fromEnv();

              // Define calculator tool
              BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                      .properties(JsonValue.from(Map.of(
                              "expression", Map.of(
                                      "type", "string",
                                      "description", "Mathematical expression to evaluate"
                              )
                      )))
                      .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                      .build();

              BetaTool calculatorTool = BetaTool.builder()
                      .name("calculator")
                      .description("Perform mathematical calculations")
                      .inputSchema(calculatorSchema)
                      .build();

              // Define database tool
              BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                      .properties(JsonValue.from(Map.of(
                              "query", Map.of(
                                      "type", "string",
                                      "description", "SQL query to execute"
                              )
                      )))
                      .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                      .build();

              BetaTool databaseTool = BetaTool.builder()
                      .name("database_query")
                      .description("Query product database")
                      .inputSchema(databaseSchema)
                      .build();

              // First request - Claude thinks once before all tool calls
              BetaMessage response = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder()
                                      .budgetTokens(10000)
                                      .build())
                              .addTool(calculatorTool)
                              .addTool(databaseTool)
                              .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                              .build()
              );

              // Response includes thinking followed by tool uses
              // Note: Claude thinks once at the beginning, then makes all tool decisions
              System.out.println("First response:");
              for (BetaContentBlock block : response.content()) {
                  if (block.isThinking()) {
                      System.out.println("Thinking (summarized): " + block.asThinking().thinking());
                  } else if (block.isToolUse()) {
                      BetaToolUseBlock toolUse = block.asToolUse();
                      System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
                  } else if (block.isText()) {
                      System.out.println("Text: " + block.asText().text());
                  }
              }

              // You would execute the tools and return results...
              // After getting both tool results back, Claude directly responds without additional thinking
          }
      }
      ```</CodeGroup>

 In this example without interleaved thinking:

 1. Claude thinks once at the beginning to understand the task
 2. Makes all tool use decisions upfront
 3. When tool results are returned, Claude immediately provides a response without additional thinking
 </Accordion>

 <Accordion title="Tool use with interleaved thinking">
 <CodeGroup>```python Python theme={null}
      import anthropic

      client = anthropic.Anthropic()

      # Same tool definitions as before
      calculator_tool = {
          "name": "calculator",
          "description": "Perform mathematical calculations",
          "input_schema": {
              "type": "object",
              "properties": {
                  "expression": {
                      "type": "string",
                      "description": "Mathematical expression to evaluate"
                  }
              },
              "required": ["expression"]
          }
      }

      database_tool = {
          "name": "database_query",
          "description": "Query product database",
          "input_schema": {
              "type": "object",
              "properties": {
                  "query": {
                      "type": "string",
                      "description": "SQL query to execute"
                  }
              },
              "required": ["query"]
          }
      }

      # First request with interleaved thinking enabled
      response = client.beta.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          tools=[calculator_tool, database_tool],
          betas=["interleaved-thinking-2025-05-14"],
          messages=[{
              "role": "user",
              "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
          }]
      )

      print("Initial response:")
      thinking_blocks = []
      tool_use_blocks = []

      for block in response.content:
          if block.type == "thinking":
              thinking_blocks.append(block)
              print(f"Thinking: {block.thinking}")
          elif block.type == "tool_use":
              tool_use_blocks.append(block)
              print(f"Tool use: {block.name} with input {block.input}")
          elif block.type == "text":
              print(f"Text: {block.text}")

      # First tool result (calculator)
      calculator_result = "7500"  # 150 * 50

      # Continue with first tool result
      response2 = client.beta.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          tools=[calculator_tool, database_tool],
          betas=["interleaved-thinking-2025-05-14"],
          messages=[
              {
                  "role": "user",
                  "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
              },
              {
                  "role": "assistant",
                  "content": [thinking_blocks[0], tool_use_blocks[0]]
              },
              {
                  "role": "user",
                  "content": [{
                      "type": "tool_result",
                      "tool_use_id": tool_use_blocks[0].id,
                      "content": calculator_result
                  }]
              }
          ]
      )

      print("\nAfter calculator result:")
      # With interleaved thinking, Claude can think about the calculator result
      # before deciding to query the database
      for block in response2.content:
          if block.type == "thinking":
              thinking_blocks.append(block)
              print(f"Interleaved thinking: {block.thinking}")
          elif block.type == "tool_use":
              tool_use_blocks.append(block)
              print(f"Tool use: {block.name} with input {block.input}")

      # Second tool result (database)
      database_result = "5200"  # Example average monthly revenue

      # Continue with second tool result
      response3 = client.beta.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          tools=[calculator_tool, database_tool],
          betas=["interleaved-thinking-2025-05-14"],
          messages=[
              {
                  "role": "user",
                  "content": "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
              },
              {
                  "role": "assistant",
                  "content": [thinking_blocks[0], tool_use_blocks[0]]
              },
              {
                  "role": "user",
                  "content": [{
                      "type": "tool_result",
                      "tool_use_id": tool_use_blocks[0].id,
                      "content": calculator_result
                  }]
              },
              {
                  "role": "assistant",
                  "content": thinking_blocks[1:] + tool_use_blocks[1:]
              },
              {
                  "role": "user",
                  "content": [{
                      "type": "tool_result",
                      "tool_use_id": tool_use_blocks[1].id,
                      "content": database_result
                  }]
              }
          ]
      )

      print("\nAfter database result:")
      # With interleaved thinking, Claude can think about both results
      # before formulating the final response
      for block in response3.content:
          if block.type == "thinking":
              print(f"Final thinking: {block.thinking}")
          elif block.type == "text":
              print(f"Final response: {block.text}")
      ``````typescript TypeScript theme={null}
      import Anthropic from '@anthropic-ai/sdk';

      const client = new Anthropic();

      // Same tool definitions as before
      const calculatorTool = {
        name: "calculator",
        description: "Perform mathematical calculations",
        input_schema: {
          type: "object",
          properties: {
            expression: {
              type: "string",
              description: "Mathematical expression to evaluate"
            }
          },
          required: ["expression"]
        }
      };

      const databaseTool = {
        name: "database_query",
        description: "Query product database",
        input_schema: {
          type: "object",
          properties: {
            query: {
              type: "string",
              description: "SQL query to execute"
            }
          },
          required: ["query"]
        }
      };

      // First request with interleaved thinking enabled
      const response = await client.beta.messages.create({
        // Enable interleaved thinking
        betas: ["interleaved-thinking-2025-05-14"],
        model: "claude-sonnet-4-5",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        tools: [calculatorTool, databaseTool],
        messages: [{
          role: "user",
          content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
        }]
      });

      console.log("Initial response:");
      const thinkingBlocks = [];
      const toolUseBlocks = [];

      for (const block of response.content) {
        if (block.type === "thinking") {
          thinkingBlocks.push(block);
          console.log(`Thinking: ${block.thinking}`);
        } else if (block.type === "tool_use") {
          toolUseBlocks.push(block);
          console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
        } else if (block.type === "text") {
          console.log(`Text: ${block.text}`);
        }
      }

      // First tool result (calculator)
      const calculatorResult = "7500"; // 150 * 50

      // Continue with first tool result
      const response2 = await client.beta.messages.create({
        betas: ["interleaved-thinking-2025-05-14"],
        model: "claude-sonnet-4-5",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        tools: [calculatorTool, databaseTool],
        messages: [
          {
            role: "user",
            content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
          },
          {
            role: "assistant",
            content: [thinkingBlocks[0], toolUseBlocks[0]]
          },
          {
            role: "user",
            content: [{
              type: "tool_result",
              tool_use_id: toolUseBlocks[0].id,
              content: calculatorResult
            }]
          }
        ]
      });

      console.log("\nAfter calculator result:");
      // With interleaved thinking, Claude can think about the calculator result
      // before deciding to query the database
      for (const block of response2.content) {
        if (block.type === "thinking") {
          thinkingBlocks.push(block);
          console.log(`Interleaved thinking: ${block.thinking}`);
        } else if (block.type === "tool_use") {
          toolUseBlocks.push(block);
          console.log(`Tool use: ${block.name} with input ${JSON.stringify(block.input)}`);
        }
      }

      // Second tool result (database)
      const databaseResult = "5200"; // Example average monthly revenue

      // Continue with second tool result
      const response3 = await client.beta.messages.create({
        betas: ["interleaved-thinking-2025-05-14"],
        model: "claude-sonnet-4-5",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        tools: [calculatorTool, databaseTool],
        messages: [
          {
            role: "user",
            content: "What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?"
          },
          {
            role: "assistant",
            content: [thinkingBlocks[0], toolUseBlocks[0]]
          },
          {
            role: "user",
            content: [{
              type: "tool_result",
              tool_use_id: toolUseBlocks[0].id,
              content: calculatorResult
            }]
          },
          {
            role: "assistant",
            content: thinkingBlocks.slice(1).concat(toolUseBlocks.slice(1))
          },
          {
            role: "user",
            content: [{
              type: "tool_result",
              tool_use_id: toolUseBlocks[1].id,
              content: databaseResult
            }]
          }
        ]
      });

      console.log("\nAfter database result:");
      // With interleaved thinking, Claude can think about both results
      // before formulating the final response
      for (const block of response3.content) {
        if (block.type === "thinking") {
          console.log(`Final thinking: ${block.thinking}`);
        } else if (block.type === "text") {
          console.log(`Final response: ${block.text}`);
        }
      }
      ``````java Java theme={null}
      import com.anthropic.client.AnthropicClient;
      import com.anthropic.client.okhttp.AnthropicOkHttpClient;
      import com.anthropic.core.JsonValue;
      import com.anthropic.models.beta.messages.*;
      import com.anthropic.models.messages.Model;
      import java.util.*;
      import static java.util.stream.Collectors.toList;

      public class InterleavedThinkingExample {
          public static void main(String[] args) {
              AnthropicClient client = AnthropicOkHttpClient.fromEnv();

              // Define calculator tool
              BetaTool.InputSchema calculatorSchema = BetaTool.InputSchema.builder()
                      .properties(JsonValue.from(Map.of(
                              "expression", Map.of(
                                      "type", "string",
                                      "description", "Mathematical expression to evaluate"
                              )
                      )))
                      .putAdditionalProperty("required", JsonValue.from(List.of("expression")))
                      .build();

              BetaTool calculatorTool = BetaTool.builder()
                      .name("calculator")
                      .description("Perform mathematical calculations")
                      .inputSchema(calculatorSchema)
                      .build();

              // Define database tool
              BetaTool.InputSchema databaseSchema = BetaTool.InputSchema.builder()
                      .properties(JsonValue.from(Map.of(
                              "query", Map.of(
                                      "type", "string",
                                      "description", "SQL query to execute"
                              )
                      )))
                      .putAdditionalProperty("required", JsonValue.from(List.of("query")))
                      .build();

              BetaTool databaseTool = BetaTool.builder()
                      .name("database_query")
                      .description("Query product database")
                      .inputSchema(databaseSchema)
                      .build();

              // First request with interleaved thinking enabled
              BetaMessage response = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder()
                                      .budgetTokens(10000)
                                      .build())
                              .addTool(calculatorTool)
                              .addTool(databaseTool)
                              // Enable interleaved thinking with beta header
                              .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                              .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                              .build()
              );

              System.out.println("Initial response:");
              List<BetaThinkingBlock> thinkingBlocks = new ArrayList<>();
              List<BetaToolUseBlock> toolUseBlocks = new ArrayList<>();

              for (BetaContentBlock block : response.content()) {
                  if (block.isThinking()) {
                      BetaThinkingBlock thinking = block.asThinking();
                      thinkingBlocks.add(thinking);
                      System.out.println("Thinking: " + thinking.thinking());
                  } else if (block.isToolUse()) {
                      BetaToolUseBlock toolUse = block.asToolUse();
                      toolUseBlocks.add(toolUse);
                      System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
                  } else if (block.isText()) {
                      System.out.println("Text: " + block.asText().text());
                  }
              }

              // First tool result (calculator)
              String calculatorResult = "7500"; // 150 * 50

              // Continue with first tool result
              BetaMessage response2 = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder()
                                      .budgetTokens(10000)
                                      .build())
                              .addTool(calculatorTool)
                              .addTool(databaseTool)
                              .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                              .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                              .addAssistantMessageOfBetaContentBlockParams(List.of(
                                      BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                      BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                              ))
                              .addUserMessageOfBetaContentBlockParams(List.of(
                                      BetaContentBlockParam.ofToolResult(
                                              BetaToolResultBlockParam.builder()
                                                      .toolUseId(toolUseBlocks.get(0).id())
                                                      .content(calculatorResult)
                                                      .build()
                                      )
                              ))
                              .build()
              );

              System.out.println("\nAfter calculator result:");
              // With interleaved thinking, Claude can think about the calculator result
              // before deciding to query the database
              for (BetaContentBlock block : response2.content()) {
                  if (block.isThinking()) {
                      BetaThinkingBlock thinking = block.asThinking();
                      thinkingBlocks.add(thinking);
                      System.out.println("Interleaved thinking: " + thinking.thinking());
                  } else if (block.isToolUse()) {
                      BetaToolUseBlock toolUse = block.asToolUse();
                      toolUseBlocks.add(toolUse);
                      System.out.println("Tool use: " + toolUse.name() + " with input " + toolUse.input());
                  }
              }

              // Second tool result (database)
              String databaseResult = "5200"; // Example average monthly revenue

              // Prepare combined content for assistant message
              List<BetaContentBlockParam> combinedContent = new ArrayList<>();
              for (int i = 1; i < thinkingBlocks.size(); i++) {
                  combinedContent.add(BetaContentBlockParam.ofThinking(thinkingBlocks.get(i).toParam()));
              }
              for (int i = 1; i < toolUseBlocks.size(); i++) {
                  combinedContent.add(BetaContentBlockParam.ofToolUse(toolUseBlocks.get(i).toParam()));
              }

              // Continue with second tool result
              BetaMessage response3 = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder()
                                      .budgetTokens(10000)
                                      .build())
                              .addTool(calculatorTool)
                              .addTool(databaseTool)
                              .putAdditionalHeader("anthropic-beta", "interleaved-thinking-2025-05-14")
                              .addUserMessage("What's the total revenue if we sold 150 units of product A at $50 each, and how does this compare to our average monthly revenue from the database?")
                              .addAssistantMessageOfBetaContentBlockParams(List.of(
                                      BetaContentBlockParam.ofThinking(thinkingBlocks.get(0).toParam()),
                                      BetaContentBlockParam.ofToolUse(toolUseBlocks.get(0).toParam())
                              ))
                              .addUserMessageOfBetaContentBlockParams(List.of(
                                      BetaContentBlockParam.ofToolResult(
                                              BetaToolResultBlockParam.builder()
                                                      .toolUseId(toolUseBlocks.get(0).id())
                                                      .content(calculatorResult)
                                                      .build()
                                      )
                              ))
                              .addAssistantMessageOfBetaContentBlockParams(combinedContent)
                              .addUserMessageOfBetaContentBlockParams(List.of(
                                      BetaContentBlockParam.ofToolResult(
                                              BetaToolResultBlockParam.builder()
                                                      .toolUseId(toolUseBlocks.get(1).id())
                                                      .content(databaseResult)
                                                      .build()
                                      )
                              ))
                              .build()
              );

              System.out.println("\nAfter database result:");
              // With interleaved thinking, Claude can think about both results
              // before formulating the final response
              for (BetaContentBlock block : response3.content()) {
                  if (block.isThinking()) {
                      System.out.println("Final thinking: " + block.asThinking().thinking());
                  } else if (block.isText()) {
                      System.out.println("Final response: " + block.asText().text());
                  }
              }
          }
      }
      ```</CodeGroup>

 In this example with interleaved thinking:

 1. Claude thinks about the task initially
 2. After receiving the calculator result, Claude can think again about what that result means
 3. Claude then decides how to query the database based on the first result
 4. After receiving the database result, Claude thinks once more about both results before formulating a final response
 5. The thinking budget is distributed across all thinking blocks within the turn

 This pattern allows for more sophisticated reasoning chains where each tool's output informs the next decision.
 </Accordion>
</AccordionGroup>

## Extended thinking with prompt caching

[Prompt caching](/en/docs/build-with-claude/prompt-caching) with thinking has several important considerations:

<Tip>
 Extended thinking tasks often take longer than 5 minutes to complete. Consider using the [1-hour cache duration](/en/docs/build-with-claude/prompt-caching#1-hour-cache-duration) to maintain cache hits across longer thinking sessions and multi-step workflows.
</Tip>

**Thinking block context removal**

* Thinking blocks from previous turns are removed from context, which can affect cache breakpoints
* When continuing conversations with tool use, thinking blocks are cached and count as input tokens when read from cache
* This creates a tradeoff: while thinking blocks don't consume context window space visually, they still count toward your input token usage when cached
* If thinking becomes disabled, requests will fail if you pass thinking content in the current tool use turn. In other contexts, thinking content passed to the API is simply ignored

**Cache invalidation patterns**

* Changes to thinking parameters (enabled/disabled or budget allocation) invalidate message cache breakpoints
* [Interleaved thinking](#interleaved-thinking) amplifies cache invalidation, as thinking blocks can occur between multiple [tool calls](#extended-thinking-with-tool-use)
* System prompts and tools remain cached despite thinking parameter changes or block removal

<Note>
 While thinking blocks are removed for caching and context calculations, they must be preserved when continuing conversations with [tool use](#extended-thinking-with-tool-use), especially with [interleaved thinking](#interleaved-thinking).
</Note>

### Understanding thinking block caching behavior

When using extended thinking with tool use, thinking blocks exhibit specific caching behavior that affects token counting:

**How it works:**

1. Caching only occurs when you make a subsequent request that includes tool results
2. When the subsequent request is made, the previous conversation history (including thinking blocks) can be cached
3. These cached thinking blocks count as input tokens in your usage metrics when read from the cache
4. When a non-tool-result user block is included, all previous thinking blocks are ignored and stripped from context

**Detailed example flow:**

**Request 1:**```
User: "What's the weather in Paris?"
```**Response 1:**```
[thinking_block_1] + [tool_use block 1]
```**Request 2:**```
User: ["What's the weather in Paris?"], 
Assistant: [thinking_block_1] + [tool_use block 1], 
User: [tool_result_1, cache=True]
```**Response 2:**```
[thinking_block_2] + [text block 2]
```Request 2 writes a cache of the request content (not the response). The cache includes the original user message, the first thinking block, tool use block, and the tool result.

**Request 3:**```
User: ["What's the weather in Paris?"], 
Assistant: [thinking_block_1] + [tool_use block 1], 
User: [tool_result_1, cache=True], 
Assistant: [thinking_block_2] + [text block 2], 
User: [Text response, cache=True]
```Because a non-tool-result user block was included, all previous thinking blocks are ignored. This request will be processed the same as:```
User: ["What's the weather in Paris?"], 
Assistant: [tool_use block 1], 
User: [tool_result_1, cache=True], 
Assistant: [text block 2], 
User: [Text response, cache=True]
```**Key points:**

* This caching behavior happens automatically, even without explicit `cache_control` markers
* This behavior is consistent whether using regular thinking or interleaved thinking

<AccordionGroup>
 <Accordion title="System prompt caching (preserved when thinking changes)">
 <CodeGroup>```python Python theme={null}
      from anthropic import Anthropic
      import requests
      from bs4 import BeautifulSoup

      client = Anthropic()

      def fetch_article_content(url):
          response = requests.get(url)
          soup = BeautifulSoup(response.content, 'html.parser')

          # Remove script and style elements
          for script in soup(["script", "style"]):
              script.decompose()

          # Get text
          text = soup.get_text()

          # Break into lines and remove leading and trailing space on each
          lines = (line.strip() for line in text.splitlines())
          # Break multi-headlines into a line each
          chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
          # Drop blank lines
          text = '\n'.join(chunk for chunk in chunks if chunk)

          return text

      # Fetch the content of the article
      book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
      book_content = fetch_article_content(book_url)
      # Use just enough text for caching (first few chapters)
      LARGE_TEXT = book_content[:5000]

      SYSTEM_PROMPT=[
          {
              "type": "text",
              "text": "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
          },
          {
              "type": "text",
              "text": LARGE_TEXT,
              "cache_control": {"type": "ephemeral"}
          }
      ]

      MESSAGES = [
          {
              "role": "user",
              "content": "Analyze the tone of this passage."
          }
      ]

      # First request - establish cache
      print("First request - establishing cache")
      response1 = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=20000,
          thinking={
              "type": "enabled",
              "budget_tokens": 4000
          },
          system=SYSTEM_PROMPT,
          messages=MESSAGES
      )

      print(f"First response usage: {response1.usage}")

      MESSAGES.append({
          "role": "assistant",
          "content": response1.content
      })
      MESSAGES.append({
          "role": "user",
          "content": "Analyze the characters in this passage."
      })
      # Second request - same thinking parameters (cache hit expected)
      print("\nSecond request - same thinking parameters (cache hit expected)")
      response2 = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=20000,
          thinking={
              "type": "enabled",
              "budget_tokens": 4000
          },
          system=SYSTEM_PROMPT,
          messages=MESSAGES
      )

      print(f"Second response usage: {response2.usage}")

      # Third request - different thinking parameters (cache miss for messages)
      print("\nThird request - different thinking parameters (cache miss for messages)")
      response3 = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=20000,
          thinking={
              "type": "enabled",
              "budget_tokens": 8000  # Changed thinking budget
          },
          system=SYSTEM_PROMPT,  # System prompt remains cached
          messages=MESSAGES  # Messages cache is invalidated
      )

      print(f"Third response usage: {response3.usage}")
      ``````typescript TypeScript theme={null}
      import Anthropic from '@anthropic-ai/sdk';
      import axios from 'axios';
      import * as cheerio from 'cheerio';

      const client = new Anthropic();

      async function fetchArticleContent(url: string): Promise<string> {
        const response = await axios.get(url);
        const $ = cheerio.load(response.data);
        
        // Remove script and style elements
        $('script, style').remove();
        
        // Get text
        let text = $.text();
        
        // Break into lines and remove leading and trailing space on each
        const lines = text.split('\n').map(line => line.trim());
        // Drop blank lines
        text = lines.filter(line => line.length > 0).join('\n');
        
        return text;
      }

      // Fetch the content of the article
      const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
      const bookContent = await fetchArticleContent(bookUrl);
      // Use just enough text for caching (first few chapters)
      const LARGE_TEXT = bookContent.slice(0, 5000);

      const SYSTEM_PROMPT = [
        {
          type: "text",
          text: "You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.",
        },
        {
          type: "text",
          text: LARGE_TEXT,
          cache_control: { type: "ephemeral" }
        }
      ];

      const MESSAGES = [
        {
          role: "user",
          content: "Analyze the tone of this passage."
        }
      ];

      // First request - establish cache
      console.log("First request - establishing cache");
      const response1 = await client.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 20000,
        thinking: {
          type: "enabled",
          budget_tokens: 4000
        },
        system: SYSTEM_PROMPT,
        messages: MESSAGES
      });

      console.log(`First response usage: ${response1.usage}`);

      MESSAGES.push({
        role: "assistant",
        content: response1.content
      });
      MESSAGES.push({
        role: "user",
        content: "Analyze the characters in this passage."
      });

      // Second request - same thinking parameters (cache hit expected)
      console.log("\nSecond request - same thinking parameters (cache hit expected)");
      const response2 = await client.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 20000,
        thinking: {
          type: "enabled",
          budget_tokens: 4000
        },
        system: SYSTEM_PROMPT,
        messages: MESSAGES
      });

      console.log(`Second response usage: ${response2.usage}`);

      // Third request - different thinking parameters (cache miss for messages)
      console.log("\nThird request - different thinking parameters (cache miss for messages)");
      const response3 = await client.messages.create({
        model: "claude-sonnet-4-5",
        max_tokens: 20000,
        thinking: {
          type: "enabled",
          budget_tokens: 8000  // Changed thinking budget
        },
        system: SYSTEM_PROMPT,  // System prompt remains cached
        messages: MESSAGES  // Messages cache is invalidated
      });

      console.log(`Third response usage: ${response3.usage}`);
      ```</CodeGroup>
 </Accordion>

 <Accordion title="Messages caching (invalidated when thinking changes)">
 <CodeGroup>```python Python theme={null}
      from anthropic import Anthropic
      import requests
      from bs4 import BeautifulSoup

      client = Anthropic()

      def fetch_article_content(url):
          response = requests.get(url)
          soup = BeautifulSoup(response.content, 'html.parser')

          # Remove script and style elements
          for script in soup(["script", "style"]):
              script.decompose()

          # Get text
          text = soup.get_text()

          # Break into lines and remove leading and trailing space on each
          lines = (line.strip() for line in text.splitlines())
          # Break multi-headlines into a line each
          chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
          # Drop blank lines
          text = '\n'.join(chunk for chunk in chunks if chunk)

          return text

      # Fetch the content of the article
      book_url = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt"
      book_content = fetch_article_content(book_url)
      # Use just enough text for caching (first few chapters)
      LARGE_TEXT = book_content[:5000]

      # No system prompt - caching in messages instead
      MESSAGES = [
          {
              "role": "user",
              "content": [
                  {
                      "type": "text",
                      "text": LARGE_TEXT,
                      "cache_control": {"type": "ephemeral"},
                  },
                  {
                      "type": "text",
                      "text": "Analyze the tone of this passage."
                  }
              ]
          }
      ]

      # First request - establish cache
      print("First request - establishing cache")
      response1 = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=20000,
          thinking={
              "type": "enabled",
              "budget_tokens": 4000
          },
          messages=MESSAGES
      )

      print(f"First response usage: {response1.usage}")

      MESSAGES.append({
          "role": "assistant",
          "content": response1.content
      })
      MESSAGES.append({
          "role": "user",
          "content": "Analyze the characters in this passage."
      })
      # Second request - same thinking parameters (cache hit expected)
      print("\nSecond request - same thinking parameters (cache hit expected)")
      response2 = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=20000,
          thinking={
              "type": "enabled",
              "budget_tokens": 4000  # Same thinking budget
          },
          messages=MESSAGES
      )

      print(f"Second response usage: {response2.usage}")

      MESSAGES.append({
          "role": "assistant",
          "content": response2.content
      })
      MESSAGES.append({
          "role": "user",
          "content": "Analyze the setting in this passage."
      })

      # Third request - different thinking budget (cache miss expected)
      print("\nThird request - different thinking budget (cache miss expected)")
      response3 = client.messages.create(
          model="claude-sonnet-4-5",
          max_tokens=20000,
          thinking={
              "type": "enabled",
              "budget_tokens": 8000  # Different thinking budget breaks cache
          },
          messages=MESSAGES
      )

      print(f"Third response usage: {response3.usage}")
      ``````typescript TypeScript theme={null}
      import Anthropic from '@anthropic-ai/sdk';
      import axios from 'axios';
      import * as cheerio from 'cheerio';

      const client = new Anthropic();

      async function fetchArticleContent(url: string): Promise<string> {
        const response = await axios.get(url);
        const $ = cheerio.load(response.data);

        // Remove script and style elements
        $('script, style').remove();

        // Get text
        let text = $.text();

        // Clean up text (break into lines, remove whitespace)
        const lines = text.split('\n').map(line => line.trim());
        const chunks = lines.flatMap(line => line.split('  ').map(phrase => phrase.trim()));
        text = chunks.filter(chunk => chunk).join('\n');

        return text;
      }

      async function main() {
        // Fetch the content of the article
        const bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
        const bookContent = await fetchArticleContent(bookUrl);
        // Use just enough text for caching (first few chapters)
        const LARGE_TEXT = bookContent.substring(0, 5000);

        // No system prompt - caching in messages instead
        let MESSAGES = [
          {
            role: "user",
            content: [
              {
                type: "text",
                text: LARGE_TEXT,
                cache_control: {type: "ephemeral"},
              },
              {
                type: "text",
                text: "Analyze the tone of this passage."
              }
            ]
          }
        ];

        // First request - establish cache
        console.log("First request - establishing cache");
        const response1 = await client.messages.create({
          model: "claude-sonnet-4-5",
          max_tokens: 20000,
          thinking: {
            type: "enabled",
            budget_tokens: 4000
          },
          messages: MESSAGES
        });

        console.log(`First response usage: `, response1.usage);

        MESSAGES = [
          ...MESSAGES,
          {
            role: "assistant",
            content: response1.content
          },
          {
            role: "user",
            content: "Analyze the characters in this passage."
          }
        ];

        // Second request - same thinking parameters (cache hit expected)
        console.log("\nSecond request - same thinking parameters (cache hit expected)");
        const response2 = await client.messages.create({
          model: "claude-sonnet-4-5",
          max_tokens: 20000,
          thinking: {
            type: "enabled",
            budget_tokens: 4000  // Same thinking budget
          },
          messages: MESSAGES
        });

        console.log(`Second response usage: `, response2.usage);

        MESSAGES = [
          ...MESSAGES,
          {
            role: "assistant",
            content: response2.content
          },
          {
            role: "user",
            content: "Analyze the setting in this passage."
          }
        ];

        // Third request - different thinking budget (cache miss expected)
        console.log("\nThird request - different thinking budget (cache miss expected)");
        const response3 = await client.messages.create({
          model: "claude-sonnet-4-5",
          max_tokens: 20000,
          thinking: {
            type: "enabled",
            budget_tokens: 8000  // Different thinking budget breaks cache
          },
          messages: MESSAGES
        });

        console.log(`Third response usage: `, response3.usage);
      }

      main().catch(console.error);
      ``````java Java theme={null}
      import java.io.IOException;
      import java.io.InputStream;
      import java.util.ArrayList;
      import java.util.List;
      import java.io.BufferedReader;
      import java.io.InputStreamReader;
      import java.net.URL;
      import java.util.Arrays;
      import java.util.regex.Pattern;

      import com.anthropic.client.AnthropicClient;
      import com.anthropic.client.okhttp.AnthropicOkHttpClient;
      import com.anthropic.models.beta.messages.*;
      import com.anthropic.models.beta.messages.MessageCreateParams;
      import com.anthropic.models.messages.Model;

      import static java.util.stream.Collectors.joining;
      import static java.util.stream.Collectors.toList;

      public class ThinkingCacheExample {
          public static void main(String[] args) throws IOException {
              AnthropicClient client = AnthropicOkHttpClient.fromEnv();

              // Fetch the content of the article
              String bookUrl = "https://www.gutenberg.org/cache/epub/1342/pg1342.txt";
              String bookContent = fetchArticleContent(bookUrl);
              // Use just enough text for caching (first few chapters)
              String largeText = bookContent.substring(0, 5000);

              List<BetaTextBlockParam> systemPrompt = List.of(
                      BetaTextBlockParam.builder()
                              .text("You are an AI assistant that is tasked with literary analysis. Analyze the following text carefully.")
                              .build(),
                      BetaTextBlockParam.builder()
                              .text(largeText)
                              .cacheControl(BetaCacheControlEphemeral.builder().build())
                              .build()
              );

              List<BetaMessageParam> messages = new ArrayList<>();
              messages.add(BetaMessageParam.builder()
                      .role(BetaMessageParam.Role.USER)
                      .content("Analyze the tone of this passage.")
                      .build());

              // First request - establish cache
              System.out.println("First request - establishing cache");
              BetaMessage response1 = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(20000)
                              .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                              .systemOfBetaTextBlockParams(systemPrompt)
                              .messages(messages)
                              .build()
              );

              System.out.println("First response usage: " + response1.usage());

              // Second request - same thinking parameters (cache hit expected)
              System.out.println("\nSecond request - same thinking parameters (cache hit expected)");
              BetaMessage response2 = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(20000)
                              .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(4000).build())
                              .systemOfBetaTextBlockParams(systemPrompt)
                              .addMessage(response1)
                              .addUserMessage("Analyze the characters in this passage.")
                              .messages(messages)
                              .build()
              );

              System.out.println("Second response usage: " + response2.usage());

              // Third request - different thinking budget (cache hit expected because system prompt caching)
              System.out.println("\nThird request - different thinking budget (cache hit expected)");
              BetaMessage response3 = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_OPUS_4_0)
                              .maxTokens(20000)
                              .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(8000).build())
                              .systemOfBetaTextBlockParams(systemPrompt)
                              .addMessage(response1)
                              .addUserMessage("Analyze the characters in this passage.")
                              .addMessage(response2)
                              .addUserMessage("Analyze the setting in this passage.")
                              .build()
              );

              System.out.println("Third response usage: " + response3.usage());
          }

          private static String fetchArticleContent(String url) throws IOException {
              // Fetch HTML content
              String htmlContent = fetchHtml(url);

              // Remove script and style elements
              String noScriptStyle = removeElements(htmlContent, "script", "style");

              // Extract text (simple approach - remove HTML tags)
              String text = removeHtmlTags(noScriptStyle);

              // Clean up text (break into lines, remove whitespace)
              List<String> lines = Arrays.asList(text.split("\n"));
              List<String> trimmedLines = lines.stream()
                      .map(String::trim)
                      .collect(toList());

              // Split on double spaces and flatten
              List<String> chunks = trimmedLines.stream()
                      .flatMap(line -> Arrays.stream(line.split("  "))
                              .map(String::trim))
                      .collect(toList());

              // Filter empty chunks and join with newlines
              return chunks.stream()
                      .filter(chunk -> !chunk.isEmpty())
                      .collect(joining("\n"));
          }

          /**
           * Fetches HTML content from a URL
           */
          private static String fetchHtml(String urlString) throws IOException {
              try (InputStream inputStream = new URL(urlString).openStream()) {
                  StringBuilder content = new StringBuilder();
                  try (BufferedReader reader = new BufferedReader(
                          new InputStreamReader(inputStream))) {
                      String line;
                      while ((line = reader.readLine()) != null) {
                          content.append(line).append("\n");
                      }
                  }
                  return content.toString();
              }
          }

          /**
           * Removes specified HTML elements and their content
           */
          private static String removeElements(String html, String... elementNames) {
              String result = html;
              for (String element : elementNames) {
                  // Pattern to match <element>...</element> and self-closing tags
                  String pattern = "<" + element + "\\s*[^>]*>.*?</" + element + ">|<" + element + "\\s*[^>]*/?>";
                  result = Pattern.compile(pattern, Pattern.DOTALL).matcher(result).replaceAll("");
              }
              return result;
          }

          /**
           * Removes all HTML tags from content
           */
          private static String removeHtmlTags(String html) {
              // Replace <br> and <p> tags with newlines for better text formatting
              String withLineBreaks = html.replaceAll("<br\\s*/?\\s*>|</?p\\s*[^>]*>", "\n");

              // Remove remaining HTML tags
              String noTags = withLineBreaks.replaceAll("<[^>]*>", "");

              // Decode HTML entities (simplified for common entities)
              return decodeHtmlEntities(noTags);
          }

          /**
           * Simple HTML entity decoder for common entities
           */
          private static String decodeHtmlEntities(String text) {
              return text
                      .replaceAll("&nbsp;", " ")
                      .replaceAll("&amp;", "&")
                      .replaceAll("&lt;", "<")
                      .replaceAll("&gt;", ">")
                      .replaceAll("&quot;", "\"")
                      .replaceAll("&#39;", "'")
                      .replaceAll("&hellip;", "...")
                      .replaceAll("&mdash;", "—");
          }

      }
      ```</CodeGroup>

 Here is the output of the script (you may see slightly different numbers)```
    First request - establishing cache
    First response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 17, output_tokens: 700 }

    Second request - same thinking parameters (cache hit expected)

    Second response usage: { cache_creation_input_tokens: 0, cache_read_input_tokens: 1370, input_tokens: 303, output_tokens: 874 }

    Third request - different thinking budget (cache miss expected)
    Third response usage: { cache_creation_input_tokens: 1370, cache_read_input_tokens: 0, input_tokens: 747, output_tokens: 619 }
    ```This example demonstrates that when caching is set up in the messages array, changing the thinking parameters (budget\_tokens increased from 4000 to 8000) **invalidates the cache**. The third request shows no cache hit with `cache_creation_input_tokens=1370` and `cache_read_input_tokens=0`, proving that message-based caching is invalidated when thinking parameters change.
 </Accordion>
</AccordionGroup>

## Max tokens and context window size with extended thinking

In older Claude models (prior to Claude Sonnet 3.7), if the sum of prompt tokens and `max_tokens` exceeded the model's context window, the system would automatically adjust `max_tokens` to fit within the context limit. This meant you could set a large `max_tokens` value and the system would silently reduce it as needed.

With Claude 3.7 and 4 models, `max_tokens` (which includes your thinking budget when thinking is enabled) is enforced as a strict limit. The system will now return a validation error if prompt tokens + `max_tokens` exceeds the context window size.

<Note>
 You can read through our [guide on context windows](/en/docs/build-with-claude/context-windows) for a more thorough deep dive.
</Note>

### The context window with extended thinking

When calculating context window usage with thinking enabled, there are some considerations to be aware of:

* Thinking blocks from previous turns are stripped and not counted towards your context window
* Current turn thinking counts towards your `max_tokens` limit for that turn

The diagram below demonstrates the specialized token management when extended thinking is enabled:

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=3ad289c01610a87c8ec1214faa09578d" alt="Context window diagram with extended thinking" data-og-width="960" width="960" data-og-height="540" height="540" data-path="images/context-window-thinking.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=a8e00261582812914cb53efe0a936e7a 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=4c0f43f4dd4e6f67c32820de8a7eba04 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=c265f69b5e1dac0f8f000d9f33253093 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=6bfb8b7c35aaf9ad13283a1108b7ec0b 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=de6d6671e549c0a407b5cc1d3cb9b078 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking.svg?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=e41fb48241b7526f815f05125d349f07 2500w" />

The effective context window is calculated as:```
context window =
  (current input tokens - previous thinking tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```We recommend using the [token counting API](/en/docs/build-with-claude/token-counting) to get accurate token counts for your specific use case, especially when working with multi-turn conversations that include thinking.

### The context window with extended thinking and tool use

When using extended thinking with tool use, thinking blocks must be explicitly preserved and returned with the tool results.

The effective context window calculation for extended thinking with tool use becomes:```
context window =
  (current input tokens + previous thinking tokens + tool use tokens) +
  (thinking tokens + encrypted thinking tokens + text output tokens)
```The diagram below illustrates token management for extended thinking with tool use:

<img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=557310c0bf57d88b7a6e550abd35bc75" alt="Context window diagram with extended thinking and tool use" data-og-width="960" width="960" data-og-height="540" height="540" data-path="images/context-window-thinking-tools.svg" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=a086ebd51148723ed500a924fb6c62a6 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=9d97e97afa9bc41f92232cd60044f716 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=65ed1d60de36587470712b2ca5ab4f45 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=174dc8d916dfdf284c86fddb4c9175f3 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=881aaf0622f43065cc942538f88562af 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/context-window-thinking-tools.svg?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=fd086e656df4fd7bf8cc2e6584689d58 2500w" />

### Managing tokens with extended thinking

Given the context window and `max_tokens` behavior with extended thinking Claude 3.7 and 4 models, you may need to:

* More actively monitor and manage your token usage
* Adjust `max_tokens` values as your prompt length changes
* Potentially use the [token counting endpoints](/en/docs/build-with-claude/token-counting) more frequently
* Be aware that previous thinking blocks don't accumulate in your context window

This change has been made to provide more predictable and transparent behavior, especially as maximum token limits have increased significantly.

## Thinking encryption

Full thinking content is encrypted and returned in the `signature` field. This field is used to verify that thinking blocks were generated by Claude when passed back to the API.

<Note>
 It is only strictly necessary to send back thinking blocks when using [tools with extended thinking](#extended-thinking-with-tool-use). Otherwise you can omit thinking blocks from previous turns, or let the API strip them for you if you pass them back.

 If sending back thinking blocks, we recommend passing everything back as you received it for consistency and to avoid potential issues.
</Note>

Here are some important considerations on thinking encryption:

* When [streaming responses](#streaming-thinking), the signature is added via a `signature_delta` inside a `content_block_delta` event just before the `content_block_stop` event.
* `signature` values are significantly longer in Claude 4 models than in previous models.
* The `signature` field is an opaque field and should not be interpreted or parsed - it exists solely for verification purposes.
* `signature` values are compatible across platforms (Claude APIs, [Amazon Bedrock](/en/docs/build-with-claude/claude-on-amazon-bedrock), and [Vertex AI](/en/docs/build-with-claude/claude-on-vertex-ai)). Values generated on one platform will be compatible with another.

### Thinking redaction

Occasionally Claude's internal reasoning will be flagged by our safety systems. When this occurs, we encrypt some or all of the `thinking` block and return it to you as a `redacted_thinking` block. `redacted_thinking` blocks are decrypted when passed back to the API, allowing Claude to continue its response without losing context.

When building customer-facing applications that use extended thinking:

* Be aware that redacted thinking blocks contain encrypted content that isn't human-readable
* Consider providing a simple explanation like: "Some of Claude's internal reasoning has been automatically encrypted for safety reasons. This doesn't affect the quality of responses."
* If showing thinking blocks to users, you can filter out redacted blocks while preserving normal thinking blocks
* Be transparent that using extended thinking features may occasionally result in some reasoning being encrypted
* Implement appropriate error handling to gracefully manage redacted thinking without breaking your UI

Here's an example showing both normal and redacted thinking blocks:```json  theme={null}
{
  "content": [
    {
      "type": "thinking",
      "thinking": "Let me analyze this step by step...",
      "signature": "WaUjzkypQ2mUEVM36O2TxuC06KN8xyfbJwyem2dw3URve/op91XWHOEBLLqIOMfFG/UvLEczmEsUjavL...."
    },
    {
      "type": "redacted_thinking",
      "data": "EmwKAhgBEgy3va3pzix/LafPsn4aDFIT2Xlxh0L5L8rLVyIwxtE3rAFBa8cr3qpPkNRj2YfWXGmKDxH4mPnZ5sQ7vB9URj2pLmN3kF8/dW5hR7xJ0aP1oLs9yTcMnKVf2wRpEGjH9XZaBt4UvDcPrQ..."
    },
    {
      "type": "text",
      "text": "Based on my analysis..."
    }
  ]
}
```<Note>
 Seeing redacted thinking blocks in your output is expected behavior. The model can still use this redacted reasoning to inform its responses while maintaining safety guardrails.

 If you need to test redacted thinking handling in your application, you can use this special test string as your prompt: `ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB`
</Note>

When passing `thinking` and `redacted_thinking` blocks back to the API in a multi-turn conversation, you must include the complete unmodified block back to the API for the last assistant turn. This is critical for maintaining the model's reasoning flow. We suggest always passing back all thinking blocks to the API. For more details, see the [Preserving thinking blocks](#preserving-thinking-blocks) section above.

<AccordionGroup>
 <Accordion title="Example: Working with redacted thinking blocks">
 This example demonstrates how to handle `redacted_thinking` blocks that may appear in responses when Claude's internal reasoning contains content flagged by safety systems:

 <CodeGroup>```python Python theme={null}
      import anthropic

      client = anthropic.Anthropic()

      # Using a special prompt that triggers redacted thinking (for demonstration purposes only)
      response = client.messages.create(
          model="claude-sonnet-4-5-20250929",
          max_tokens=16000,
          thinking={
              "type": "enabled",
              "budget_tokens": 10000
          },
          messages=[{
              "role": "user",
              "content": "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
          }]
      )

      # Identify redacted thinking blocks
      has_redacted_thinking = any(
          block.type == "redacted_thinking" for block in response.content
      )

      if has_redacted_thinking:
          print("Response contains redacted thinking blocks")
          # These blocks are still usable in subsequent requests

          # Extract all blocks (both redacted and non-redacted)
          all_thinking_blocks = [
              block for block in response.content
              if block.type in ["thinking", "redacted_thinking"]
          ]

          # When passing to subsequent requests, include all blocks without modification
          # This preserves the integrity of Claude's reasoning

          print(f"Found {len(all_thinking_blocks)} thinking blocks total")
          print(f"These blocks are still billable as output tokens")
      ``````typescript TypeScript theme={null}
      import Anthropic from '@anthropic-ai/sdk';

      const client = new Anthropic();

      // Using a special prompt that triggers redacted thinking (for demonstration purposes only)
      const response = await client.messages.create({
        model: "claude-sonnet-4-5-20250929",
        max_tokens: 16000,
        thinking: {
          type: "enabled",
          budget_tokens: 10000
        },
        messages: [{
          role: "user",
          content: "ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB"
        }]
      });

      // Identify redacted thinking blocks
      const hasRedactedThinking = response.content.some(
        block => block.type === "redacted_thinking"
      );

      if (hasRedactedThinking) {
        console.log("Response contains redacted thinking blocks");
        // These blocks are still usable in subsequent requests

        // Extract all blocks (both redacted and non-redacted)
        const allThinkingBlocks = response.content.filter(
          block => block.type === "thinking" || block.type === "redacted_thinking"
        );

        // When passing to subsequent requests, include all blocks without modification
        // This preserves the integrity of Claude's reasoning

        console.log(`Found ${allThinkingBlocks.length} thinking blocks total`);
        console.log(`These blocks are still billable as output tokens`);
      }
      ``````java Java theme={null}
      import java.util.List;

      import static java.util.stream.Collectors.toList;

      import com.anthropic.client.AnthropicClient;
      import com.anthropic.client.okhttp.AnthropicOkHttpClient;
      import com.anthropic.models.beta.messages.BetaContentBlock;
      import com.anthropic.models.beta.messages.BetaMessage;
      import com.anthropic.models.beta.messages.MessageCreateParams;
      import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
      import com.anthropic.models.messages.Model;

      public class RedactedThinkingExample {
          public static void main(String[] args) {
              AnthropicClient client = AnthropicOkHttpClient.fromEnv();

              // Using a special prompt that triggers redacted thinking (for demonstration purposes only)
              BetaMessage response = client.beta().messages().create(
                      MessageCreateParams.builder()
                              .model(Model.CLAUDE_SONNET_4_5)
                              .maxTokens(16000)
                              .thinking(BetaThinkingConfigEnabled.builder().budgetTokens(10000).build())
                              .addUserMessage("ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB")
                              .build()
              );

              // Identify redacted thinking blocks
              boolean hasRedactedThinking = response.content().stream()
                      .anyMatch(BetaContentBlock::isRedactedThinking);

              if (hasRedactedThinking) {
                  System.out.println("Response contains redacted thinking blocks");
                  // These blocks are still usable in subsequent requests
                  // Extract all blocks (both redacted and non-redacted)
                  List<BetaContentBlock> allThinkingBlocks = response.content().stream()
                          .filter(block -> block.isThinking() ||
                                  block.isRedactedThinking())
                          .collect(toList());

                  // When passing to subsequent requests, include all blocks without modification
                  // This preserves the integrity of Claude's reasoning
                  System.out.println("Found " + allThinkingBlocks.size() + " thinking blocks total");
                  System.out.println("These blocks are still billable as output tokens");
              }
          }
      }
      ```</CodeGroup>

 <TryInConsoleButton userPrompt="ANTHROPIC_MAGIC_STRING_TRIGGER_REDACTED_THINKING_46C9A13E193C177646C7398A98432ECCCE4C1253D5E2D82641AC0E52CC2876CB" thinkingBudgetTokens={16000}>
 Try in Console
 </TryInConsoleButton>
 </Accordion>
</AccordionGroup>

## Differences in thinking across model versions

The Messages API handles thinking differently across Claude Sonnet 3.7 and Claude 4 models, primarily in redaction and summarization behavior.

See the table below for a condensed comparison:

| Feature | Claude Sonnet 3.7 | Claude 4 Models |
| ------------------------ | ---------------------------- | ------------------------------------------------------------ |
| **Thinking Output** | Returns full thinking output | Returns summarized thinking |
| **Interleaved Thinking** | Not supported | Supported with `interleaved-thinking-2025-05-14` beta header |

## Pricing

Extended thinking uses the standard token pricing scheme:

| Model | Base Input Tokens | Cache Writes | Cache Hits | Output Tokens |
| ----------------- | ----------------- | -------------- | ------------- | ------------- |
| Claude Opus 4.1 | \$15 / MTok | \$18.75 / MTok | \$1.50 / MTok | \$75 / MTok |
| Claude Opus 4 | \$15 / MTok | \$18.75 / MTok | \$1.50 / MTok | \$75 / MTok |
| Claude Sonnet 4.5 | \$3 / MTok | \$3.75 / MTok | \$0.30 / MTok | \$15 / MTok |
| Claude Sonnet 4 | \$3 / MTok | \$3.75 / MTok | \$0.30 / MTok | \$15 / MTok |
| Claude Sonnet 3.7 | \$3 / MTok | \$3.75 / MTok | \$0.30 / MTok | \$15 / MTok |

The thinking process incurs charges for:

* Tokens used during thinking (output tokens)
* Thinking blocks from the last assistant turn included in subsequent requests (input tokens)
* Standard text output tokens

<Note>
 When extended thinking is enabled, a specialized system prompt is automatically included to support this feature.
</Note>

When using summarized thinking:

* **Input tokens**: Tokens in your original request (excludes thinking tokens from previous turns)
* **Output tokens (billed)**: The original thinking tokens that Claude generated internally
* **Output tokens (visible)**: The summarized thinking tokens you see in the response
* **No charge**: Tokens used to generate the summary

<Warning>
 The billed output token count will **not** match the visible token count in the response. You are billed for the full thinking process, not the summary you see.
</Warning>

## Best practices and considerations for extended thinking

### Working with thinking budgets

* **Budget optimization:** The minimum budget is 1,024 tokens. We suggest starting at the minimum and increasing the thinking budget incrementally to find the optimal range for your use case. Higher token counts enable more comprehensive reasoning but with diminishing returns depending on the task. Increasing the budget can improve response quality at the tradeoff of increased latency. For critical tasks, test different settings to find the optimal balance. Note that the thinking budget is a target rather than a strict limit—actual token usage may vary based on the task.
* **Starting points:** Start with larger thinking budgets (16k+ tokens) for complex tasks and adjust based on your needs.
* **Large budgets:** For thinking budgets above 32k, we recommend using [batch processing](/en/docs/build-with-claude/batch-processing) to avoid networking issues. Requests pushing the model to think above 32k tokens causes long running requests that might run up against system timeouts and open connection limits.
* **Token usage tracking:** Monitor thinking token usage to optimize costs and performance.

### Performance considerations

* **Response times:** Be prepared for potentially longer response times due to the additional processing required for the reasoning process. Factor in that generating thinking blocks may increase overall response time.
* **Streaming requirements:** Streaming is required when `max_tokens` is greater than 21,333. When streaming, be prepared to handle both thinking and text content blocks as they arrive.

### Feature compatibility

* Thinking isn't compatible with `temperature` or `top_k` modifications as well as [forced tool use](/en/docs/agents-and-tools/tool-use/implement-tool-use#forcing-tool-use).
* When thinking is enabled, you can set `top_p` to values between 1 and 0.95.
* You cannot pre-fill responses when thinking is enabled.
* Changes to the thinking budget invalidate cached prompt prefixes that include messages. However, cached system prompts and tool definitions will continue to work when thinking parameters change.

### Usage guidelines

* **Task selection:** Use extended thinking for particularly complex tasks that benefit from step-by-step reasoning like math, coding, and analysis.
* **Context handling:** You do not need to remove previous thinking blocks yourself. The Claude API automatically ignores thinking blocks from previous turns and they are not included when calculating context usage.
* **Prompt engineering:** Review our [extended thinking prompting tips](/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips) if you want to maximize Claude's thinking capabilities.

## Next steps

<CardGroup>
 <Card title="Try the extended thinking cookbook" icon="book" href="https://github.com/anthropics/anthropic-cookbook/tree/main/extended_thinking">
 Explore practical examples of thinking in our cookbook.
 </Card>

 <Card title="Extended thinking prompting tips" icon="code" href="/en/docs/build-with-claude/prompt-engineering/extended-thinking-tips">
 Learn prompt engineering best practices for extended thinking.
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDEGK5BSPX3H]
---

[START OF DOCUMENT: CLAUDECYU6EPKUE | Title: Files]

# Files API

The Files API lets you upload and manage files to use with the Claude API without re-uploading content with each request. This is particularly useful when using the [code execution tool](/en/docs/agents-and-tools/tool-use/code-execution-tool) to provide inputs (e.g. datasets and documents) and then download outputs (e.g. charts). You can also use the Files API to prevent having to continually re-upload frequently used documents and images across multiple API calls. You can [explore the API reference directly](/en/api/files-create), in addition to this guide.

<Note>
 The Files API is currently in beta. Please reach out through our [feedback form](https://forms.gle/tisHyierGwgN4DUE9) to share your experience with the Files API.
</Note>

## Supported models

Referencing a `file_id` in a Messages request is supported in all models that support the given file type. For example, [images](/en/docs/build-with-claude/vision) are supported in all Claude 3+ models, [PDFs](/en/docs/build-with-claude/pdf-support) in all Claude 3.5+ models, and [various other file types](/en/docs/agents-and-tools/tool-use/code-execution-tool#supported-file-types) for the code execution tool in Claude 3.5 Haiku plus all Claude 3.7+ models.

The Files API is currently not supported on Amazon Bedrock or Google Vertex AI.

## How the Files API works

The Files API provides a simple create-once, use-many-times approach for working with files:

* **Upload files** to our secure storage and receive a unique `file_id`
* **Download files** that are created from skills or the code execution tool
* **Reference files** in [Messages](/en/api/messages) requests using the `file_id` instead of re-uploading content
* **Manage your files** with list, retrieve, and delete operations

## How to use the Files API

<Note>
 To use the Files API, you'll need to include the beta feature header: `anthropic-beta: files-api-2025-04-14`.
</Note>

### Uploading a file

Upload a file to be referenced in future API calls:

<CodeGroup>```bash Shell theme={null}
  curl -X POST https://api.anthropic.com/v1/files \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: files-api-2025-04-14" \
    -F "file=@/path/to/document.pdf"
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()
  client.beta.files.upload(
    file=("document.pdf", open("/path/to/document.pdf", "rb"), "application/pdf"),
  )
  ``````typescript TypeScript theme={null}
  import Anthropic, { toFile } from '@anthropic-ai/sdk';
  import fs from "fs";

  const anthropic = new Anthropic();

  await anthropic.beta.files.upload({
    file: await toFile(fs.createReadStream('/path/to/document.pdf'), undefined, { type: 'application/pdf' })
  }, {
    betas: ['files-api-2025-04-14']
  });
  ```</CodeGroup>

The response from uploading a file will include:```json  theme={null}
{
  "id": "file_011CNha8iCJcU1wXNR6q4V8w",
  "type": "file",
  "filename": "document.pdf",
  "mime_type": "application/pdf",
  "size_bytes": 1024000,
  "created_at": "2025-01-01T00:00:00Z",
  "downloadable": false
}
```### Using a file in messages

Once uploaded, reference the file using its `file_id`:

<CodeGroup>```bash Shell theme={null}
  curl -X POST https://api.anthropic.com/v1/messages \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: files-api-2025-04-14" \
    -H "content-type: application/json" \
    -d '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
        {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": "Please summarize this document for me."          
            },
            {
              "type": "document",
              "source": {
                "type": "file",
                "file_id": "file_011CNha8iCJcU1wXNR6q4V8w"
              }
            }
          ]
        }
      ]
    }'
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "text",
                      "text": "Please summarize this document for me."
                  },
                  {
                      "type": "document",
                      "source": {
                          "type": "file",
                          "file_id": "file_011CNha8iCJcU1wXNR6q4V8w"
                      }
                  }
              ]
          }
      ],
      betas=["files-api-2025-04-14"],
  )
  print(response)
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 1024,
    messages: [
      {
        role: "user",
        content: [
          {
            type: "text",
            text: "Please summarize this document for me."
          },
          {
            type: "document",
            source: {
              type: "file",
              file_id: "file_011CNha8iCJcU1wXNR6q4V8w"
            }
          }
        ]
      }
    ],
    betas: ["files-api-2025-04-14"],
  });

  console.log(response);
  ```</CodeGroup>

### File types and content blocks

The Files API supports different file types that correspond to different content block types:

| File Type | MIME Type | Content Block Type | Use Case |
| :---------------------------------------------------------------------------------------------- | :--------------------------------------------------- | :----------------- | :---------------------------------- |
| PDF | `application/pdf` | `document` | Text analysis, document processing |
| Plain text | `text/plain` | `document` | Text analysis, processing |
| Images | `image/jpeg`, `image/png`, `image/gif`, `image/webp` | `image` | Image analysis, visual tasks |
| [Datasets, others](/en/docs/agents-and-tools/tool-use/code-execution-tool#supported-file-types) | Varies | `container_upload` | Analyze data, create visualizations |

### Working with other file formats

For file types that are not supported as `document` blocks (.csv, .txt, .md, .docx, .xlsx), convert the files to plain text, and include the content directly in your message:

<CodeGroup>```bash Shell theme={null}
  # Example: Reading a text file and sending it as plain text
  # Note: For files with special characters, consider base64 encoding
  TEXT_CONTENT=$(cat document.txt | jq -Rs .)

  curl https://api.anthropic.com/v1/messages \
    -H "content-type: application/json" \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -d @- <<EOF
  {
    "model": "claude-sonnet-4-5",
    "max_tokens": 1024,
    "messages": [
      {
        "role": "user",
        "content": [
          {
            "type": "text",
            "text": "Here's the document content:\n\n${TEXT_CONTENT}\n\nPlease summarize this document."
          }
        ]
      }
    ]
  }
  EOF
  ``````python Python theme={null}
  import pandas as pd
  import anthropic

  client = anthropic.Anthropic()

  # Example: Reading a CSV file
  df = pd.read_csv('data.csv')
  csv_content = df.to_string()

  # Send as plain text in the message
  response = client.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "text",
                      "text": f"Here's the CSV data:\n\n{csv_content}\n\nPlease analyze this data."
                  }
              ]
          }
      ]
  )

  print(response.content[0].text)
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';
  import fs from 'fs';

  const anthropic = new Anthropic();

  async function analyzeDocument() {
    // Example: Reading a text file
    const textContent = fs.readFileSync('document.txt', 'utf-8');

    // Send as plain text in the message
    const response = await anthropic.messages.create({
      model: 'claude-sonnet-4-5',
      max_tokens: 1024,
      messages: [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: `Here's the document content:\n\n${textContent}\n\nPlease summarize this document.`
            }
          ]
        }
      ]
    });

    console.log(response.content[0].text);
  }

  analyzeDocument();
  ```</CodeGroup>

<Note>
 For .docx files containing images, convert them to PDF format first, then use [PDF support](/en/docs/build-with-claude/pdf-support) to take advantage of the built-in image parsing. This allows using citations from the PDF document.
</Note>

#### Document blocks

For PDFs and text files, use the `document` content block:```json  theme={null}
{
  "type": "document",
  "source": {
    "type": "file",
    "file_id": "file_011CNha8iCJcU1wXNR6q4V8w"
  },
  "title": "Document Title", // Optional
  "context": "Context about the document", // Optional  
  "citations": {"enabled": true} // Optional, enables citations
}
```#### Image blocks

For images, use the `image` content block:```json  theme={null}
{
  "type": "image",
  "source": {
    "type": "file",
    "file_id": "file_011CPMxVD3fHLUhvTqtsQA5w"
  }
}
```### Managing files

#### List files

Retrieve a list of your uploaded files:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/files \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: files-api-2025-04-14"
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()
  files = client.beta.files.list()
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();
  const files = await anthropic.beta.files.list({
    betas: ['files-api-2025-04-14'],
  });
  ```</CodeGroup>

#### Get file metadata

Retrieve information about a specific file:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/files/file_011CNha8iCJcU1wXNR6q4V8w \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: files-api-2025-04-14"
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()
  file = client.beta.files.retrieve_metadata("file_011CNha8iCJcU1wXNR6q4V8w")
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();
  const file = await anthropic.beta.files.retrieveMetadata(
    "file_011CNha8iCJcU1wXNR6q4V8w",
    { betas: ['files-api-2025-04-14'] },
  );
  ```</CodeGroup>

#### Delete a file

Remove a file from your workspace:

<CodeGroup>```bash Shell theme={null}
  curl -X DELETE https://api.anthropic.com/v1/files/file_011CNha8iCJcU1wXNR6q4V8w \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: files-api-2025-04-14"
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()
  result = client.beta.files.delete("file_011CNha8iCJcU1wXNR6q4V8w")
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();
  const result = await anthropic.beta.files.delete(
    "file_011CNha8iCJcU1wXNR6q4V8w",
    { betas: ['files-api-2025-04-14'] },
  );
  ```</CodeGroup>

### Downloading a file

Download files that have been created by skills or the code execution tool:

<CodeGroup>```bash Shell theme={null}
  curl -X GET "https://api.anthropic.com/v1/files/file_011CNha8iCJcU1wXNR6q4V8w/content" \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: files-api-2025-04-14" \
    --output downloaded_file.txt
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()
  file_content = client.beta.files.download("file_011CNha8iCJcU1wXNR6q4V8w")

  # Save to file
  with open("downloaded_file.txt", "w") as f:
      f.write(file_content.decode('utf-8'))
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';
  import fs from 'fs';

  const anthropic = new Anthropic();

  const fileContent = await anthropic.beta.files.download(
    "file_011CNha8iCJcU1wXNR6q4V8w",
    { betas: ['files-api-2025-04-14'] },
  );

  // Save to file
  fs.writeFileSync("downloaded_file.txt", fileContent);
  ```</CodeGroup>

<Note>
 You can only download files that were created by [skills](/en/docs/build-with-claude/skills-guide) or the [code execution tool](/en/docs/agents-and-tools/tool-use/code-execution-tool). Files that you uploaded cannot be downloaded.
</Note>

***

## File storage and limits

### Storage limits

* **Maximum file size:** 500 MB per file
* **Total storage:** 100 GB per organization

### File lifecycle

* Files are scoped to the workspace of the API key. Other API keys can use files created by any other API key associated with the same workspace
* Files persist until you delete them
* Deleted files cannot be recovered
* Files are inaccessible via the API shortly after deletion, but they may persist in active `Messages` API calls and associated tool uses
* Files that users delete will be deleted in accordance with our [data retention policy](https://privacy.claude.com/en/articles/7996866-how-long-do-you-store-my-organization-s-data).

***

## Error handling

Common errors when using the Files API include:

* **File not found (404):** The specified `file_id` doesn't exist or you don't have access to it
* **Invalid file type (400):** The file type doesn't match the content block type (e.g., using an image file in a document block)
* **Exceeds context window size (400):** The file is larger than the context window size (e.g. using a 500 MB plaintext file in a `/v1/messages` request)
* **Invalid filename (400):** Filename doesn't meet the length requirements (1-255 characters) or contains forbidden characters (`<`, `>`, `:`, `"`, `|`, `?`, `*`, `\`, `/`, or unicode characters 0-31)
* **File too large (413):** File exceeds the 500 MB limit
* **Storage limit exceeded (403):** Your organization has reached the 100 GB storage limit```json  theme={null}
{
  "type": "error",
  "error": {
    "type": "invalid_request_error",
    "message": "File not found: file_011CNha8iCJcU1wXNR6q4V8w"
  }
}
```## Usage and billing

File API operations are **free**:

* Uploading files
* Downloading files
* Listing files
* Getting file metadata
* Deleting files

File content used in `Messages` requests are priced as input tokens. You can only download files created by [skills](/en/docs/build-with-claude/skills-guide) or the [code execution tool](/en/docs/agents-and-tools/tool-use/code-execution-tool).

### Rate limits

During the beta period:

* File-related API calls are limited to approximately 100 requests per minute
* [Contact us](mailto:sales@anthropic.com) if you need higher limits for your use case

[END OF DOCUMENT: CLAUDECYU6EPKUE]
---

