
[SYSTEM INSTRUCTION]
This is a structured knowledge file. Interpret it according to these rules:
1.  **File Structure:** Begins with a Table of Contents (TOC).
2.  **Document ID (DocID):** Each document has a short, unique `DocID` for citation.
3.  **Content Hash:** A full SHA256 hash is provided for data integrity.
4.  **Markers:** Content is encapsulated by `[START/END OF DOCUMENT]` markers.
5.  **Usage:** Use the content to answer queries, citing the `DocID` and Title.
[/SYSTEM INSTRUCTION]
---

--- TABLE OF CONTENTS ---
[DocID: CLAUDE16HPC8GBGL (sha256-6d06552d2605c1ebc994253d99a2bddf3fbd5bf1d48444ff9134510a78174d4f) | Title: Choosing-A-Model]
[DocID: CLAUDEHH6LZTUWG (sha256-2cd7d1d358b0a43fce2b9dcc3e156c1a0f3b953f32643f5ace0add2fd0bf46ca) | Title: Citations]
[DocID: CLAUDE2YTGIWDLN (sha256-079cf639eedb5a1ab1241e17c7f44c3ab139d5126b0803c93214a9b7e8099253) | Title: Claude-4-Best-Practices]
[DocID: CLAUDE43GD9X3TT (sha256-0a8265684ea10677e1c01a976055243d1c357b9d828a5f7d53649f01f602df98) | Title: Claude-Code-Analytics-Api]
[DocID: CLAUDE25RNR5IJPB (sha256-c7896a04d53fabab310438a264a8f26fca9a97de4cfea6809e290a381a6a3de7) | Title: Claude-For-Sheets]
[DocID: CLAUDEHHXD8BZJA (sha256-2ce5616434a6f334f3a4b18b5b03d95d32592f9e16226ac1f401ce0bfd75baf8) | Title: Claude-On-Amazon-Bedrock]
[DocID: CLAUDESW5UK5909 (sha256-4a225dc2d8d96bdf78bed373d72d1988734162c507a2314a40b9c8279c507fb7) | Title: Claude-On-Vertex-Ai]
[DocID: CLAUDE1F74X8D129 (sha256-835d18c872a1a2d5187b41929ea6c7a3fe6605218e974e6cdef3c7d616bf6def) | Title: Code-Execution-Tool]
[DocID: CLAUDE9QDUKWN9W (sha256-18f8f8c659d4196f187b2d16f5c8711f272b794c554e9c129ba26e4b9f2cb687) | Title: Computer-Use-Tool]
[DocID: CLAUDE1WX3OH4GDJ (sha256-b0d5255dace7a37d1b43e0c13a716978e5ccb2d244395828833a90e927695742) | Title: Context-Editing]
--- END OF TOC ---

[START OF DOCUMENT: CLAUDE16HPC8GBGL | Title: Choosing-A-Model]

# Choosing the right model

> Selecting the optimal Claude model for your application involves balancing three key considerations: capabilities, speed, and cost. This guide helps you make an informed decision based on your specific requirements.

## Establish key criteria

When choosing a Claude model, we recommend first evaluating these factors:

* **Capabilities:** What specific features or capabilities will you need the model to have in order to meet your needs?
* **Speed:** How quickly does the model need to respond in your application?
* **Cost:** What's your budget for both development and production usage?

Knowing these answers in advance will make narrowing down and deciding which model to use much easier.

***

## Choose the best model to start with

There are two general approaches you can use to start testing which Claude model best works for your needs.

### Option 1: Start with a fast, cost-effective model

For many applications, starting with a faster, more cost-effective model like Claude Haiku 4.5 can be the optimal approach:

1. Begin implementation with Claude Haiku 4.5
2. Test your use case thoroughly
3. Evaluate if performance meets your requirements
4. Upgrade only if necessary for specific capability gaps

This approach allows for quick iteration, lower development costs, and is often sufficient for many common applications. This approach is best for:

* Initial prototyping and development
* Applications with tight latency requirements
* Cost-sensitive implementations
* High-volume, straightforward tasks

### Option 2: Start with the most capable model

For complex tasks where intelligence and advanced capabilities are paramount, you may want to start with the most capable model and then consider optimizing to more efficient models down the line:

1. Implement with Claude Sonnet 4.5
2. Optimize your prompts for these models
3. Evaluate if performance meets your requirements
4. Consider increasing efficiency by downgrading intelligence over time with greater workflow optimization

This approach is best for:

* Complex reasoning tasks
* Scientific or mathematical applications
* Tasks requiring nuanced understanding
* Applications where accuracy outweighs cost considerations
* Advanced coding

## Model selection matrix

| When you need... | We recommend starting with... | Example use cases |
| ----------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------- |
| Best model for complex agents and coding, highest intelligence across most tasks, superior tool orchestration for long-running autonomous tasks | Claude Sonnet 4.5 | Autonomous coding agents, cybersecurity automation, complex financial analysis, multi-hour research tasks, multi agent frameworks |
| Exceptional intelligence and reasoning for specialized complex tasks | Claude Opus 4.1 | Highly complex codebase refactoring, nuanced creative writing, specialized scientific analysis |
| Near-frontier performance with lightning-fast speed and extended thinking - our fastest and most intelligent Haiku model at the most economical price point | Claude Haiku 4.5 | Real-time applications, high-volume intelligent processing, cost-sensitive deployments needing strong reasoning, sub-agent tasks |

***

## Decide whether to upgrade or change models

To determine if you need to upgrade or change models, you should:

1. [Create benchmark tests](/en/docs/test-and-evaluate/develop-tests) specific to your use case - having a good evaluation set is the most important step in the process
2. Test with your actual prompts and data
3. Compare performance across models for:
 * Accuracy of responses
 * Response quality
 * Handling of edge cases
4. Weigh performance and cost tradeoffs

## Next steps

<CardGroup cols={3}>
 <Card title="Model comparison chart" icon="head-side-gear" href="/en/docs/about-claude/models/overview">
 See detailed specifications and pricing for the latest Claude models
 </Card>

 <Card title="What's new in Claude 4.5" icon="sparkles" href="/en/docs/about-claude/models/whats-new-claude-4-5">
 Explore the latest improvements in Claude 4.5 models
 </Card>

 <Card title="Start building" icon="code" href="/en/docs/get-started">
 Get started with your first API call
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDE16HPC8GBGL]
---

[START OF DOCUMENT: CLAUDEHH6LZTUWG | Title: Citations]

# Citations

Claude is capable of providing detailed citations when answering questions about documents, helping you track and verify information sources in responses.

All [active models](/en/docs/about-claude/models/overview) support citations, with the exception of Haiku 3.

<Warning>
 *Citations with Claude Sonnet 3.7*

 Claude Sonnet 3.7 may be less likely to make citations compared to other Claude models without more explicit instructions from the user. When using citations with Claude Sonnet 3.7, we recommend including additional instructions in the `user` turn, like `"Use citations to back up your answer."` for example.

 We've also observed that when the model is asked to structure its response, it is unlikely to use citations unless explicitly told to use citations within that format. For example, if the model is asked to use `<result>` tags in its response, you should add something like `"Always use citations in your answer, even within <result> tags."`
</Warning>

<Tip>
 Please share your feedback and suggestions about the citations feature using this [form](https://forms.gle/9n9hSrKnKe3rpowH9).
</Tip>

Here's an example of how to use citations with the Messages API:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
    -H "content-type: application/json" \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -d '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
        {
          "role": "user",
          "content": [
            {
              "type": "document",
              "source": {
                "type": "text",
                "media_type": "text/plain",
                "data": "The grass is green. The sky is blue."
              },
              "title": "My Document",
              "context": "This is a trustworthy document.",
              "citations": {"enabled": true}
            },
            {
              "type": "text",
              "text": "What color is the grass and sky?"
            }
          ]
        }
      ]
    }'
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  response = client.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "document",
                      "source": {
                          "type": "text",
                          "media_type": "text/plain",
                          "data": "The grass is green. The sky is blue."
                      },
                      "title": "My Document",
                      "context": "This is a trustworthy document.",
                      "citations": {"enabled": True}
                  },
                  {
                      "type": "text",
                      "text": "What color is the grass and sky?"
                  }
              ]
          }
      ]
  )
  print(response)
  ``````java Java theme={null}
  import java.util.List;

  import com.anthropic.client.AnthropicClient;
  import com.anthropic.client.okhttp.AnthropicOkHttpClient;
  import com.anthropic.models.messages.*;

  public class DocumentExample {

      public static void main(String[] args) {
          AnthropicClient client = AnthropicOkHttpClient.fromEnv();

          PlainTextSource source = PlainTextSource.builder()
                  .data("The grass is green. The sky is blue.")
                  .build();

          DocumentBlockParam documentParam = DocumentBlockParam.builder()
                  .source(source)
                  .title("My Document")
                  .context("This is a trustworthy document.")
                  .citations(CitationsConfigParam.builder().enabled(true).build())
                  .build();
          
          TextBlockParam textBlockParam = TextBlockParam.builder()
                  .text("What color is the grass and sky?")
                  .build();

          MessageCreateParams params = MessageCreateParams.builder()
                  .model(Model.CLAUDE_SONNET_4_20250514)
                  .maxTokens(1024)
                  .addUserMessageOfBlockParams(List.of(ContentBlockParam.ofDocument(documentParam), ContentBlockParam.ofText(textBlockParam)))
                  .build();

          Message message = client.messages().create(params);
          System.out.println(message);
      }
  }
  ```</CodeGroup>

<Tip>
 **Comparison with prompt-based approaches**

 In comparison with prompt-based citations solutions, the citations feature has the following advantages:

 * **Cost savings:** If your prompt-based approach asks Claude to output direct quotes, you may see cost savings due to the fact that `cited_text` does not count towards your output tokens.
 * **Better citation reliability:** Because we parse citations into the respective response formats mentioned above and extract `cited_text`, citations are guaranteed to contain valid pointers to the provided documents.
 * **Improved citation quality:** In our evals, we found the citations feature to be significantly more likely to cite the most relevant quotes from documents as compared to purely prompt-based approaches.
</Tip>

***

## How citations work

Integrate citations with Claude in these steps:

<Steps>
 <Step title="Provide document(s) and enable citations">
 * Include documents in any of the supported formats: [PDFs](#pdf-documents), [plain text](#plain-text-documents), or [custom content](#custom-content-documents) documents
 * Set `citations.enabled=true` on each of your documents. Currently, citations must be enabled on all or none of the documents within a request.
 * Note that only text citations are currently supported and image citations are not yet possible.
 </Step>

 <Step title="Documents get processed">
 * Document contents are "chunked" in order to define the minimum granularity of possible citations. For example, sentence chunking would allow Claude to cite a single sentence or chain together multiple consecutive sentences to cite a paragraph (or longer)!
 * **For PDFs:** Text is extracted as described in [PDF Support](/en/docs/build-with-claude/pdf-support) and content is chunked into sentences. Citing images from PDFs is not currently supported.
 * **For plain text documents:** Content is chunked into sentences that can be cited from.
 * **For custom content documents:** Your provided content blocks are used as-is and no further chunking is done.
 </Step>

 <Step title="Claude provides cited response">
 * Responses may now include multiple text blocks where each text block can contain a claim that Claude is making and a list of citations that support the claim.
 * Citations reference specific locations in source documents. The format of these citations are dependent on the type of document being cited from.
 * **For PDFs:** citations will include the page number range (1-indexed).
 * **For plain text documents:** Citations will include the character index range (0-indexed).
 * **For custom content documents:** Citations will include the content block index range (0-indexed) corresponding to the original content list provided.
 * Document indices are provided to indicate the reference source and are 0-indexed according to the list of all documents in your original request.
 </Step>
</Steps>

<Tip>
 **Automatic chunking vs custom content**

 By default, plain text and PDF documents are automatically chunked into sentences. If you need more control over citation granularity (e.g., for bullet points or transcripts), use custom content documents instead. See [Document Types](#document-types) for more details.

 For example, if you want Claude to be able to cite specific sentences from your RAG chunks, you should put each RAG chunk into a plain text document. Otherwise, if you do not want any further chunking to be done, or if you want to customize any additional chunking, you can put RAG chunks into custom content document(s).
</Tip>

### Citable vs non-citable content

* Text found within a document's `source` content can be cited from.
* `title` and `context` are optional fields that will be passed to the model but not used towards cited content.
* `title` is limited in length so you may find the `context` field to be useful in storing any document metadata as text or stringified json.

### Citation indices

* Document indices are 0-indexed from the list of all document content blocks in the request (spanning across all messages).
* Character indices are 0-indexed with exclusive end indices.
* Page numbers are 1-indexed with exclusive end page numbers.
* Content block indices are 0-indexed with exclusive end indices from the `content` list provided in the custom content document.

### Token costs

* Enabling citations incurs a slight increase in input tokens due to system prompt additions and document chunking.
* However, the citations feature is very efficient with output tokens. Under the hood, the model is outputting citations in a standardized format that are then parsed into cited text and document location indices. The `cited_text` field is provided for convenience and does not count towards output tokens.
* When passed back in subsequent conversation turns, `cited_text` is also not counted towards input tokens.

### Feature compatibility

Citations works in conjunction with other API features including [prompt caching](/en/docs/build-with-claude/prompt-caching), [token counting](/en/docs/build-with-claude/token-counting) and [batch processing](/en/docs/build-with-claude/batch-processing).

<Warning>
 **Citations and Structured Outputs are incompatible**

 Citations cannot be used together with [Structured Outputs](/en/docs/build-with-claude/structured-outputs). If you enable citations on any user-provided document (Document blocks or RequestSearchResultBlock) and also include the `output_format` parameter, the API will return a 400 error.

 This is because citations require interleaving citation blocks with text output, which is incompatible with the strict JSON schema constraints of structured outputs.
</Warning>

#### Using Prompt Caching with Citations

Citations and prompt caching can be used together effectively.

The citation blocks generated in responses cannot be cached directly, but the source documents they reference can be cached. To optimize performance, apply `cache_control` to your top-level document content blocks.

<CodeGroup>```python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  # Long document content (e.g., technical documentation)
  long_document = "This is a very long document with thousands of words..." + " ... " * 1000  # Minimum cacheable length

  response = client.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      messages=[
          {
              "role": "user",
              "content": [
                  {
                      "type": "document",
                      "source": {
                          "type": "text",
                          "media_type": "text/plain",
                          "data": long_document
                      },
                      "citations": {"enabled": True},
                      "cache_control": {"type": "ephemeral"}  # Cache the document content
                  },
                  {
                      "type": "text",
                      "text": "What does this document say about API features?"
                  }
              ]
          }
      ]
  )
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const client = new Anthropic();

  // Long document content (e.g., technical documentation)
  const longDocument = "This is a very long document with thousands of words..." + " ... ".repeat(1000);  // Minimum cacheable length

  const response = await client.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 1024,
    messages: [
      {
        role: "user",
        content: [
          {
            type: "document",
            source: {
              type: "text",
              media_type: "text/plain",
              data: longDocument
            },
            citations: { enabled: true },
            cache_control: { type: "ephemeral" }  // Cache the document content
          },
          {
            type: "text",
            text: "What does this document say about API features?"
          }
        ]
      }
    ]
  });
  ``````bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
       --header "x-api-key: $ANTHROPIC_API_KEY" \
       --header "anthropic-version: 2023-06-01" \
       --header "content-type: application/json" \
       --data '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "messages": [
          {
              "role": "user",
              "content": [
                  {
                      "type": "document",
                      "source": {
                          "type": "text",
                          "media_type": "text/plain",
                          "data": "This is a very long document with thousands of words..."
                      },
                      "citations": {"enabled": true},
                      "cache_control": {"type": "ephemeral"}
                  },
                  {
                      "type": "text",
                      "text": "What does this document say about API features?"
                  }
              ]
          }
      ]
  }'
  ```</CodeGroup>

In this example:

* The document content is cached using `cache_control` on the document block
* Citations are enabled on the document
* Claude can generate responses with citations while benefiting from cached document content
* Subsequent requests using the same document will benefit from the cached content

## Document Types

### Choosing a document type

We support three document types for citations. Documents can be provided directly in the message (base64, text, or URL) or uploaded via the [Files API](/en/docs/build-with-claude/files) and referenced by `file_id`:

| Type | Best for | Chunking | Citation format |
| :------------- | :-------------------------------------------------------------- | :--------------------- | :---------------------------- |
| Plain text | Simple text documents, prose | Sentence | Character indices (0-indexed) |
| PDF | PDF files with text content | Sentence | Page numbers (1-indexed) |
| Custom content | Lists, transcripts, special formatting, more granular citations | No additional chunking | Block indices (0-indexed) |

<Note>
 .csv, .xlsx, .docx, .md, and .txt files are not supported as document blocks. Convert these to plain text and include directly in message content. See [Working with other file formats](/en/docs/build-with-claude/files#working-with-other-file-formats).
</Note>

### Plain text documents

Plain text documents are automatically chunked into sentences. You can provide them inline or by reference with their `file_id`:

<Tabs>
 <Tab title="Inline text">```python  theme={null}
    {
        "type": "document",
        "source": {
            "type": "text",
            "media_type": "text/plain",
            "data": "Plain text content..."
        },
        "title": "Document Title", # optional
        "context": "Context about the document that will not be cited from", # optional
        "citations": {"enabled": True}
    }
    ```</Tab>

 <Tab title="Files API">```python  theme={null}
    {
        "type": "document",
        "source": {
            "type": "file",
            "file_id": "file_011CNvxoj286tYUAZFiZMf1U"
        },
        "title": "Document Title", # optional
        "context": "Context about the document that will not be cited from", # optional
        "citations": {"enabled": True}
    }
    ```</Tab>
</Tabs>

<Accordion title="Example plain text citation">```python  theme={null}
  {
      "type": "char_location",
      "cited_text": "The exact text being cited", # not counted towards output tokens
      "document_index": 0,
      "document_title": "Document Title",
      "start_char_index": 0,    # 0-indexed
      "end_char_index": 50      # exclusive
  }
  ```</Accordion>

### PDF documents

PDF documents can be provided as base64-encoded data or by `file_id`. PDF text is extracted and chunked into sentences. As image citations are not yet supported, PDFs that are scans of documents and do not contain extractable text will not be citable.

<Tabs>
 <Tab title="Base64">```python  theme={null}
    {
        "type": "document",
        "source": {
            "type": "base64",
            "media_type": "application/pdf",
            "data": base64_encoded_pdf_data
        },
        "title": "Document Title", # optional
        "context": "Context about the document that will not be cited from", # optional
        "citations": {"enabled": True}
    }
    ```</Tab>

 <Tab title="Files API">```python  theme={null}
    {
        "type": "document",
        "source": {
            "type": "file",
            "file_id": "file_011CNvxoj286tYUAZFiZMf1U"
        },
        "title": "Document Title", # optional
        "context": "Context about the document that will not be cited from", # optional
        "citations": {"enabled": True}
    }
    ```</Tab>
</Tabs>

<Accordion title="Example PDF citation">```python  theme={null}
  {
      "type": "page_location",
      "cited_text": "The exact text being cited", # not counted towards output tokens
      "document_index": 0,     
      "document_title": "Document Title", 
      "start_page_number": 1,  # 1-indexed
      "end_page_number": 2     # exclusive
  }
  ```</Accordion>

### Custom content documents

Custom content documents give you control over citation granularity. No additional chunking is done and chunks are provided to the model according to the content blocks provided.```python  theme={null}
{
    "type": "document",
    "source": {
        "type": "content",
        "content": [
            {"type": "text", "text": "First chunk"},
            {"type": "text", "text": "Second chunk"}
        ]
    },
    "title": "Document Title", # optional
    "context": "Context about the document that will not be cited from", # optional
    "citations": {"enabled": True}
}
```<Accordion title="Example citation">```python  theme={null}
  {
      "type": "content_block_location",
      "cited_text": "The exact text being cited", # not counted towards output tokens
      "document_index": 0,
      "document_title": "Document Title",
      "start_block_index": 0,   # 0-indexed
      "end_block_index": 1      # exclusive
  }
  ```</Accordion>

***

## Response Structure

When citations are enabled, responses include multiple text blocks with citations:```python  theme={null}
{
    "content": [
        {
            "type": "text",
            "text": "According to the document, "
        },
        {
            "type": "text",
            "text": "the grass is green",
            "citations": [{
                "type": "char_location",
                "cited_text": "The grass is green.",
                "document_index": 0,
                "document_title": "Example Document",
                "start_char_index": 0,
                "end_char_index": 20
            }]
        },
        {
            "type": "text",
            "text": " and "
        },
        {
            "type": "text",
            "text": "the sky is blue",
            "citations": [{
                "type": "char_location",
                "cited_text": "The sky is blue.",
                "document_index": 0,
                "document_title": "Example Document",
                "start_char_index": 20,
                "end_char_index": 36
            }]
        },
        {
            "type": "text",
            "text": ". Information from page 5 states that ",
        },
        {
            "type": "text",
            "text": "water is essential",
            "citations": [{
                "type": "page_location",
                "cited_text": "Water is essential for life.",
                "document_index": 1,
                "document_title": "PDF Document",
                "start_page_number": 5,
                "end_page_number": 6
            }]
        },
        {
            "type": "text",
            "text": ". The custom document mentions ",
        },
        {
            "type": "text",
            "text": "important findings",
            "citations": [{
                "type": "content_block_location",
                "cited_text": "These are important findings.",
                "document_index": 2,
                "document_title": "Custom Content Document",
                "start_block_index": 0,
                "end_block_index": 1
            }]
        }
    ]
}
```### Streaming Support

For streaming responses, we've added a `citations_delta` type that contains a single citation to be added to the `citations` list on the current `text` content block.

<AccordionGroup>
 <Accordion title="Example streaming events">```python  theme={null}
    event: message_start
    data: {"type": "message_start", ...}

    event: content_block_start
    data: {"type": "content_block_start", "index": 0, ...}

    event: content_block_delta
    data: {"type": "content_block_delta", "index": 0, 
           "delta": {"type": "text_delta", "text": "According to..."}}

    event: content_block_delta
    data: {"type": "content_block_delta", "index": 0,
           "delta": {"type": "citations_delta", 
                     "citation": {
                         "type": "char_location",
                         "cited_text": "...",
                         "document_index": 0,
                         ...
                     }}}

    event: content_block_stop
    data: {"type": "content_block_stop", "index": 0}

    event: message_stop
    data: {"type": "message_stop"}
    ```</Accordion>
</AccordionGroup>

[END OF DOCUMENT: CLAUDEHH6LZTUWG]
---

[START OF DOCUMENT: CLAUDE2YTGIWDLN | Title: Claude-4-Best-Practices]

# Prompting best practices

This guide provides specific prompt engineering techniques for Claude 4.x models, with specific guidance for Sonnet 4.5 and Haiku 4.5. These models have been trained for more precise instruction following than previous generations of Claude models.

<Tip>
 For an overview of Claude 4.5's new capabilities, see [What's new in Claude 4.5](/en/docs/about-claude/models/whats-new-claude-4-5). For migration guidance from previous models, see [Migrating to Claude 4.5](/en/docs/about-claude/models/migrating-to-claude-4).
</Tip>

## General principles

### Be explicit with your instructions

Claude 4.x models respond well to clear, explicit instructions. Being specific about your desired output can help enhance results. Customers who desire the "above and beyond" behavior from previous Claude models might need to more explicitly request these behaviors with newer models.

<Accordion title="Example: Creating an analytics dashboard">
 **Less effective:**```text  theme={null}
  Create an analytics dashboard
  ```**More effective:**```text  theme={null}
  Create an analytics dashboard. Include as many relevant features and interactions as possible. Go beyond the basics to create a fully-featured implementation.
  ```</Accordion>

### Add context to improve performance

Providing context or motivation behind your instructions, such as explaining to Claude why such behavior is important, can help Claude 4.x models better understand your goals and deliver more targeted responses.

<Accordion title="Example: Formatting preferences">
 **Less effective:**```text  theme={null}
  NEVER use ellipses
  ```**More effective:**```text  theme={null}
  Your response will be read aloud by a text-to-speech engine, so never use ellipses since the text-to-speech engine will not know how to pronounce them.
  ```</Accordion>

Claude is smart enough to generalize from the explanation.

### Be vigilant with examples & details

Claude 4.x models pay close attention to details and examples as part of their precise instruction following capabilities. Ensure that your examples align with the behaviors you want to encourage and minimize behaviors you want to avoid.

### Long-horizon reasoning and state tracking

Claude 4.5 models excel at long-horizon reasoning tasks with exceptional state tracking capabilities. It maintains orientation across extended sessions by focusing on incremental progress—making steady advances on a few things at a time rather than attempting everything at once. This capability especially emerges over multiple context windows or task iterations, where Claude can work on a complex task, save the state, and continue with a fresh context window.

#### Context awareness and multi-window workflows

Claude 4.5 models feature [context awareness](/en/docs/build-with-claude/context-windows#context-awareness-in-claude-sonnet-4-5), enabling the model to track its remaining context window (i.e. "token budget") throughout a conversation. This enables Claude to execute tasks and manage context more effectively by understanding how much space it has to work.

**Managing context limits:**

If you are using Claude in an agent harness that compacts context or allows saving context to external files (like in Claude Code), we suggest adding this information to your prompt so Claude can behave accordingly. Otherwise, Claude may sometimes naturally try to wrap up work as it approaches the context limit. Below is an example prompt:```text Sample prompt theme={null}
Your context window will be automatically compacted as it approaches its limit, allowing you to continue working indefinitely from where you left off. Therefore, do not stop tasks early due to token budget concerns. As you approach your token budget limit, save your current progress and state to memory before the context window refreshes. Always be as persistent and autonomous as possible and complete tasks fully, even if the end of your budget is approaching. Never artificially stop any task early regardless of the context remaining.
```The [memory tool](/en/docs/agents-and-tools/tool-use/memory-tool) pairs naturally with context awareness for seamless context transitions.

#### Multi-context window workflows

For tasks spanning multiple context windows:

1. **Use a different prompt for the very first context window**: Use the first context window to set up a framework (write tests, create setup scripts), then use future context windows to iterate on a todo-list.

2. **Have the model write tests in a structured format**: Ask Claude to create tests before starting work and keep track of them in a structured format (e.g., `tests.json`). This leads to better long-term ability to iterate. Remind Claude of the importance of tests: "It is unacceptable to remove or edit tests because this could lead to missing or buggy functionality."

3. **Set up quality of life tools**: Encourage Claude to create setup scripts (e.g., `init.sh`) to gracefully start servers, run test suites, and linters. This prevents repeated work when continuing from a fresh context window.

4. **Starting fresh vs compacting**: When a context window is cleared, consider starting with a brand new context window rather than using compaction. Claude 4.5 models are extremely effective at discovering state from the local filesystem. In some cases, you may want to take advantage of this over compaction. Be prescriptive about how it should start:
 * "Call pwd; you can only read and write files in this directory."
 * "Review progress.txt, tests.json, and the git logs."
 * "Manually run through a fundamental integration test before moving on to implementing new features."

5. **Provide verification tools**: As the length of autonomous tasks grows, Claude needs to verify correctness without continuous human feedback. Tools like Playwright MCP server or computer use capabilities for testing UIs are helpful.

6. **Encourage complete usage of context**: Prompt Claude to efficiently complete components before moving on:```text Sample prompt theme={null}
This is a very long task, so it may be beneficial to plan out your work clearly. It's encouraged to spend your entire output context working on the task - just make sure you don't run out of context with significant uncommitted work. Continue working systematically until you have completed this task.
```#### State management best practices

* **Use structured formats for state data**: When tracking structured information (like test results or task status), use JSON or other structured formats to help Claude understand schema requirements
* **Use unstructured text for progress notes**: Freeform progress notes work well for tracking general progress and context
* **Use git for state tracking**: Git provides a log of what's been done and checkpoints that can be restored. Claude 4.5 models perform especially well in using git to track state across multiple sessions.
* **Emphasize incremental progress**: Explicitly ask Claude to keep track of its progress and focus on incremental work

<Accordion title="Example: State tracking">```json  theme={null}
  // Structured state file (tests.json)
  {
    "tests": [
      {"id": 1, "name": "authentication_flow", "status": "passing"},
      {"id": 2, "name": "user_management", "status": "failing"},
      {"id": 3, "name": "api_endpoints", "status": "not_started"}
    ],
    "total": 200,
    "passing": 150,
    "failing": 25,
    "not_started": 25
  }
  ``````text  theme={null}
  // Progress notes (progress.txt)
  Session 3 progress:
  - Fixed authentication token validation
  - Updated user model to handle edge cases
  - Next: investigate user_management test failures (test #2)
  - Note: Do not remove tests as this could lead to missing functionality
  ```</Accordion>

### Communication style

Claude 4.5 models have a more concise and natural communication style compared to previous models:

* **More direct and grounded**: Provides fact-based progress reports rather than self-celebratory updates
* **More conversational**: Slightly more fluent and colloquial, less machine-like
* **Less verbose**: May skip detailed summaries for efficiency unless prompted otherwise

This communication style accurately reflects what has been accomplished without unnecessary elaboration.

## Guidance for specific situations

### Balance verbosity

Claude 4.5 models tend toward efficiency and may skip verbal summaries after tool calls, jumping directly to the next action. While this creates a streamlined workflow, you may prefer more visibility into its reasoning process.

If you want Claude to provide updates as it works:```text Sample prompt theme={null}
After completing a task that involves tool use, provide a quick summary of the work you've done.
```### Tool usage patterns

Claude 4.5 models are trained for precise instruction following and benefits from explicit direction to use specific tools. If you say "can you suggest some changes," it will sometimes provide suggestions rather than implementing them—even if making changes might be what you intended.

For Claude to take action, be more explicit:

<Accordion title="Example: Explicit instructions">
 **Less effective (Claude will only suggest):**```text  theme={null}
  Can you suggest some changes to improve this function?
  ```**More effective (Claude will make the changes):**```text  theme={null}
  Change this function to improve its performance.
  ```Or:```text  theme={null}
  Make these edits to the authentication flow.
  ```</Accordion>

To make Claude more proactive about taking action by default, you can add this to your system prompt:```text Sample prompt for proactive action theme={null}
<default_to_action>
By default, implement changes rather than only suggesting them. If the user's intent is unclear, infer the most useful likely action and proceed, using tools to discover any missing details instead of guessing. Try to infer the user's intent about whether a tool call (e.g., file edit or read) is intended or not, and act accordingly.
</default_to_action>
```On the other hand, if you want the model to be more hesitant by default, less prone to jumping straight into implementations, and only take action if requested, you can steer this behavior with a prompt like the below:```text Sample prompt for conservative action theme={null}
<do_not_act_before_instructions>
Do not jump into implementatation or changes files unless clearly instructed to make changes. When the user's intent is ambiguous, default to providing information, doing research, and providing recommendations rather than taking action. Only proceed with edits, modifications, or implementations when the user explicitly requests them.
</do_not_act_before_instructions>
```### Control the format of responses

There are a few ways that we have found to be particularly effective in steering output formatting in Claude 4.x models:

1. **Tell Claude what to do instead of what not to do**

 * Instead of: "Do not use markdown in your response"
 * Try: "Your response should be composed of smoothly flowing prose paragraphs."

2. **Use XML format indicators**

 * Try: "Write the prose sections of your response in \<smoothly\_flowing\_prose\_paragraphs> tags."

3. **Match your prompt style to the desired output**

 The formatting style used in your prompt may influence Claude's response style. If you are still experiencing steerability issues with output formatting, we recommend as best as you can matching your prompt style to your desired output style. For example, removing markdown from your prompt can reduce the volume of markdown in the output.

4. **Use detailed prompts for specific formatting preferences**

 For more control over markdown and formatting usage, provide explicit guidance:````text Sample prompt to minimize markdown theme={null}
<avoid_excessive_markdown_and_bullet_points>
When writing reports, documents, technical explanations, analyses, or any long-form content, write in clear, flowing prose using complete paragraphs and sentences. Use standard paragraph breaks for organization and reserve markdown primarily for `inline code`, code blocks (```...```), and simple headings (###, and ###). Avoid using **bold** and *italics*.

DO NOT use ordered lists (1. ...) or unordered lists (*) unless : a) you're presenting truly discrete items where a list format is the best option, or b) the user explicitly requests a list or ranking

Instead of listing items with bullets or numbers, incorporate them naturally into sentences. This guidance applies especially to technical writing. Using prose instead of excessive formatting will improve user satisfaction. NEVER output a series of overly short bullet points.

Your goal is readable, flowing text that guides the reader naturally through ideas rather than fragmenting information into isolated points.
</avoid_excessive_markdown_and_bullet_points>
````

### Research and information gathering

Claude 4.5 models demonstrate exceptional agentic search capabilities and can find and synthesize information from multiple sources effectively. For optimal research results:

1. **Provide clear success criteria**: Define what constitutes a successful answer to your research question

2. **Encourage source verification**: Ask Claude to verify information across multiple sources

3. **For complex research tasks, use a structured approach**:```text Sample prompt for complex research theme={null}
Search for this information in a structured way. As you gather data, develop several competing hypotheses. Track your confidence levels in your progress notes to improve calibration. Regularly self-critique your approach and plan. Update a hypothesis tree or research notes file to persist information and provide transparency. Break down this complex research task systematically.
```This structured approach allows Claude to find and synthesize virtually any piece of information and iteratively critique its findings, no matter the size of the corpus.

### Subagent orchestration

Claude 4.5 models demonstrate significantly improved native subagent orchestration capabilities. These models can recognize when tasks would benefit from delegating work to specialized subagents and do so proactively without requiring explicit instruction.

To take advantage of this behavior:

1. **Ensure well-defined subagent tools**: Have subagent tools available and described in tool definitions
2. **Let Claude orchestrate naturally**: Claude will delegate appropriately without explicit instruction
3. **Adjust conservativeness if needed**:```text Sample prompt for conservative subagent usage theme={null}
Only delegate to subagents when the task clearly benefits from a separate agent with a new context window.
```### Model self-knowledge

If you would like Claude to identify itself correctly in your application or use specific API strings:```text Sample prompt for model identity theme={null}
The assistant is Claude, created by Anthropic. The current model is Claude Sonnet 4.5.
```For LLM-powered apps that need to specify model strings:```text Sample prompt for model string theme={null}
When an LLM is needed, please default to Claude Sonnet 4.5 unless the user requests otherwise. The exact model string for Claude Sonnet 4.5 is claude-sonnet-4-5-20250929.
```### Leverage thinking & interleaved thinking capabilities

Claude 4.x models offer thinking capabilities that can be especially helpful for tasks involving reflection after tool use or complex multi-step reasoning. You can guide its initial or interleaved thinking for better results.```text Example prompt theme={null}
After receiving tool results, carefully reflect on their quality and determine optimal next steps before proceeding. Use your thinking to plan and iterate based on this new information, and then take the best next action.
```<Info>
 For more information on thinking capabilities, see [Extended thinking](/en/docs/build-with-claude/extended-thinking).
</Info>

### Document creation

Claude 4.5 models excel at creating presentations, animations, and visual documents. These models match or exceed Claude Opus 4.1 in this domain, with impressive creative flair and stronger instruction following. The models produce polished, usable output on the first try in most cases.

For best results with document creation:```text Sample prompt theme={null}
Create a professional presentation on [topic]. Include thoughtful design elements, visual hierarchy, and engaging animations where appropriate.
```### Optimize parallel tool calling

Claude 4.x models excel at parallel tool execution, with Sonnet 4.5 being particularly aggressive in firing off multiple operations simultaneously. Claude 4.x models will:

* Run multiple speculative searches during research
* Read several files at once to build context faster
* Execute bash commands in parallel (which can even bottleneck system performance)

This behavior is easily steerable. While the model has a high success rate in parallel tool calling without prompting, you can boost this to \~100% or adjust the aggression level:```text Sample prompt for maximum parallel efficiency theme={null}
<use_parallel_tool_calls>
If you intend to call multiple tools and there are no dependencies between the tool calls, make all of the independent tool calls in parallel. Prioritize calling tools simultaneously whenever the actions can be done in parallel rather than sequentially. For example, when reading 3 files, run 3 tool calls in parallel to read all 3 files into context at the same time. Maximize use of parallel tool calls where possible to increase speed and efficiency. However, if some tool calls depend on previous calls to inform dependent values like the parameters, do NOT call these tools in parallel and instead call them sequentially. Never use placeholders or guess missing parameters in tool calls.
</use_parallel_tool_calls>
``````text Sample prompt to reduce parallel execution theme={null}
Execute operations sequentially with brief pauses between each step to ensure stability.
```### Reduce file creation in agentic coding

Claude 4.x models may sometimes create new files for testing and iteration purposes, particularly when working with code. This approach allows Claude to use files, especially python scripts, as a 'temporary scratchpad' before saving its final output. Using temporary files can improve outcomes particularly for agentic coding use cases.

If you'd prefer to minimize net new file creation, you can instruct Claude to clean up after itself:```text Sample prompt theme={null}
If you create any temporary new files, scripts, or helper files for iteration, clean up these files by removing them at the end of the task.
```### Enhance visual and frontend code generation

Claude 4.x models can generate high-quality, visually distinctive, functional user interfaces. However, without guidance, frontend code can default to generic patterns that lack visual interest. To elicit exceptional UI results:

1. **Provide explicit encouragement for creativity:**```text Sample prompt theme={null}
Don't hold back. Give it your all. Create an impressive demonstration showcasing web development capabilities.
```2. **Specify aesthetic direction and design constraints:**```text Sample prompt theme={null}
Create a professional dashboard using a dark blue and cyan color palette, modern sans-serif typography (e.g., Inter for headings, system fonts for body), and card-based layouts with subtle shadows. Include thoughtful details like hover states, transitions, and micro-interactions. Apply design principles: hierarchy, contrast, balance, and movement.
```3. **Encourage design diversity and fusion aesthetics:**```text Sample prompt theme={null}
Provide multiple design options. Create fusion aesthetics by combining elements from different sources—one color scheme, different typography, another layout principle. Avoid generic centered layouts, simplistic gradients, and uniform styling.
```4. **Request specific features explicitly:**

* "Include as many relevant features and interactions as possible"
* "Add animations and interactive elements"
* "Create a fully-featured implementation beyond the basics"

### Avoid focusing on passing tests and hard-coding

Claude 4.x models can sometimes focus too heavily on making tests pass at the expense of more general solutions, or may use workarounds like helper scripts for complex refactoring instead of using standard tools directly. To prevent this behavior and ensure robust, generalizable solutions:```text Sample prompt theme={null}
Please write a high-quality, general-purpose solution using the standard tools available. Do not create helper scripts or workarounds to accomplish the task more efficiently. Implement a solution that works correctly for all valid inputs, not just the test cases. Do not hard-code values or create solutions that only work for specific test inputs. Instead, implement the actual logic that solves the problem generally.

Focus on understanding the problem requirements and implementing the correct algorithm. Tests are there to verify correctness, not to define the solution. Provide a principled implementation that follows best practices and software design principles.

If the task is unreasonable or infeasible, or if any of the tests are incorrect, please inform me rather than working around them. The solution should be robust, maintainable, and extendable.
```### Minimizing hallucinations in agentic coding

Claude 4.x models are less prone to hallucinations and give more accurate, grounded, intelligent answers based on the code. To encourage this behavior even more and minimize hallucinations:```text Sample prompt theme={null}
<investigate_before_answering>
Never speculate about code you have not opened. If the user references a specific file, you MUST read the file before answering. Make sure to investigate and read relevant files BEFORE answering questions about the codebase. Never make any claims about code before investigating unless you are certain of the correct answer - give grounded and hallucination-free answers.
</investigate_before_answering>
```## Migration considerations

When migrating to Claude 4.5 models:

1. **Be specific about desired behavior**: Consider describing exactly what you'd like to see in the output.

2. **Frame your instructions with modifiers**: Adding modifiers that encourage Claude to increase the quality and detail of its output can help better shape Claude's performance. For example, instead of "Create an analytics dashboard", use "Create an analytics dashboard. Include as many relevant features and interactions as possible. Go beyond the basics to create a fully-featured implementation."

3. **Request specific features explicitly**: Animations and interactive elements should be requested explicitly when desired.

[END OF DOCUMENT: CLAUDE2YTGIWDLN]
---

[START OF DOCUMENT: CLAUDE43GD9X3TT | Title: Claude-Code-Analytics-Api]

# Claude Code Analytics API

> Programmatically access your organization's Claude Code usage analytics and productivity metrics with the Claude Code Analytics Admin API.

<Tip>
 **The Admin API is unavailable for individual accounts.** To collaborate with teammates and add members, set up your organization in **Console → Settings → Organization**.
</Tip>

The Claude Code Analytics Admin API provides programmatic access to daily aggregated usage metrics for Claude Code users, enabling organizations to analyze developer productivity and build custom dashboards. This API bridges the gap between our basic [Analytics dashboard](https://console.anthropic.com/claude-code) and the complex OpenTelemetry integration.

This API enables you to better monitor, analyze, and optimize your Claude Code adoption:

* **Developer Productivity Analysis:** Track sessions, lines of code added/removed, commits, and pull requests created using Claude Code
* **Tool Usage Metrics:** Monitor acceptance and rejection rates for different Claude Code tools (Edit, Write, NotebookEdit)
* **Cost Analysis:** View estimated costs and token usage broken down by Claude model
* **Custom Reporting:** Export data to build executive dashboards and reports for management teams
* **Usage Justification:** Provide metrics to justify and expand Claude Code adoption internally

<Check>
 **Admin API key required**

 This API is part of the [Admin API](/en/docs/build-with-claude/administration-api). These endpoints require an Admin API key (starting with `sk-ant-admin...`) that differs from standard API keys. Only organization members with the admin role can provision Admin API keys through the [Claude Console](https://console.anthropic.com/settings/admin-keys).
</Check>

## Quick start

Get your organization's Claude Code analytics for a specific day:```bash  theme={null}
curl "https://api.anthropic.com/v1/organizations/usage_report/claude_code?\
starting_at=2025-09-08&\
limit=20" \
  --header "anthropic-version: 2023-06-01" \
  --header "x-api-key: $ADMIN_API_KEY"
```<Tip>
 **Set a User-Agent header for integrations**

 If you're building an integration, set your User-Agent header to help us understand usage patterns:```
  User-Agent: YourApp/1.0.0 (https://yourapp.com)
  ```</Tip>

## Claude Code Analytics API

Track Claude Code usage, productivity metrics, and developer activity across your organization with the `/v1/organizations/usage_report/claude_code` endpoint.

### Key concepts

* **Daily aggregation**: Returns metrics for a single day specified by the `starting_at` parameter
* **User-level data**: Each record represents one user's activity for the specified day
* **Productivity metrics**: Track sessions, lines of code, commits, pull requests, and tool usage
* **Token and cost data**: Monitor usage and estimated costs broken down by Claude model
* **Cursor-based pagination**: Handle large datasets with stable pagination using opaque cursors
* **Data freshness**: Metrics are available with up to 1-hour delay for consistency

For complete parameter details and response schemas, see the [Claude Code Analytics API reference](/en/api/admin-api/claude-code/get-claude-code-usage-report).

### Basic examples

#### Get analytics for a specific day```bash  theme={null}
curl "https://api.anthropic.com/v1/organizations/usage_report/claude_code?\
starting_at=2025-09-08" \
  --header "anthropic-version: 2023-06-01" \
  --header "x-api-key: $ADMIN_API_KEY"
```#### Get analytics with pagination```bash  theme={null}
# First request
curl "https://api.anthropic.com/v1/organizations/usage_report/claude_code?\
starting_at=2025-09-08&\
limit=20" \
  --header "anthropic-version: 2023-06-01" \
  --header "x-api-key: $ADMIN_API_KEY"

# Subsequent request using cursor from response
curl "https://api.anthropic.com/v1/organizations/usage_report/claude_code?\
starting_at=2025-09-08&\
page=page_MjAyNS0wNS0xNFQwMDowMDowMFo=" \
  --header "anthropic-version: 2023-06-01" \
  --header "x-api-key: $ADMIN_API_KEY"
```### Request parameters

| Parameter | Type | Required | Description |
| ------------- | ------- | -------- | ----------------------------------------------------------------------- |
| `starting_at` | string | Yes | UTC date in YYYY-MM-DD format. Returns metrics for this single day only |
| `limit` | integer | No | Number of records per page (default: 20, max: 1000) |
| `page` | string | No | Opaque cursor token from previous response's `next_page` field |

### Available metrics

Each response record contains the following metrics for a single user on a single day:

#### Dimensions

* **date**: Date in RFC 3339 format (UTC timestamp)
* **actor**: The user or API key that performed the Claude Code actions (either `user_actor` with `email_address` or `api_actor` with `api_key_name`)
* **organization\_id**: Organization UUID
* **customer\_type**: Type of customer account (`api` for API customers, `subscription` for Pro/Team customers)
* **terminal\_type**: Type of terminal or environment where Claude Code was used (e.g., `vscode`, `iTerm.app`, `tmux`)

#### Core metrics

* **num\_sessions**: Number of distinct Claude Code sessions initiated by this actor
* **lines\_of\_code.added**: Total number of lines of code added across all files by Claude Code
* **lines\_of\_code.removed**: Total number of lines of code removed across all files by Claude Code
* **commits\_by\_claude\_code**: Number of git commits created through Claude Code's commit functionality
* **pull\_requests\_by\_claude\_code**: Number of pull requests created through Claude Code's PR functionality

#### Tool action metrics

Breakdown of tool action acceptance and rejection rates by tool type:

* **edit\_tool.accepted/rejected**: Number of Edit tool proposals that the user accepted/rejected
* **write\_tool.accepted/rejected**: Number of Write tool proposals that the user accepted/rejected
* **notebook\_edit\_tool.accepted/rejected**: Number of NotebookEdit tool proposals that the user accepted/rejected

#### Model breakdown

For each Claude model used:

* **model**: Claude model identifier (e.g., `claude-sonnet-4-5-20250929`)
* **tokens.input/output**: Input and output token counts for this model
* **tokens.cache\_read/cache\_creation**: Cache-related token usage for this model
* **estimated\_cost.amount**: Estimated cost in cents USD for this model
* **estimated\_cost.currency**: Currency code for the cost amount (currently always `USD`)

### Response structure

The API returns data in the following format:```json  theme={null}
{
  "data": [
    {
      "date": "2025-09-01T00:00:00Z",
      "actor": {
        "type": "user_actor",
        "email_address": "developer@company.com"
      },
      "organization_id": "dc9f6c26-b22c-4831-8d01-0446bada88f1",
      "customer_type": "api",
      "terminal_type": "vscode",
      "core_metrics": {
        "num_sessions": 5,
        "lines_of_code": {
          "added": 1543,
          "removed": 892
        },
        "commits_by_claude_code": 12,
        "pull_requests_by_claude_code": 2
      },
      "tool_actions": {
        "edit_tool": {
          "accepted": 45,
          "rejected": 5
        },
        "multi_edit_tool": {
          "accepted": 12,
          "rejected": 2
        },
        "write_tool": {
          "accepted": 8,
          "rejected": 1
        },
        "notebook_edit_tool": {
          "accepted": 3,
          "rejected": 0
        }
      },
      "model_breakdown": [
        {
          "model": "claude-sonnet-4-5-20250929",
          "tokens": {
            "input": 100000,
            "output": 35000,
            "cache_read": 10000,
            "cache_creation": 5000
          },
          "estimated_cost": {
            "currency": "USD",
            "amount": 1025
          }
        }
      ]
    }
  ],
  "has_more": false,
  "next_page": null
}
```## Pagination

The API supports cursor-based pagination for organizations with large numbers of users:

1. Make your initial request with optional `limit` parameter
2. If `has_more` is `true` in the response, use the `next_page` value in your next request
3. Continue until `has_more` is `false`

The cursor encodes the position of the last record and ensures stable pagination even as new data arrives. Each pagination session maintains a consistent data boundary to ensure you don't miss or duplicate records.

## Common use cases

* **Executive dashboards**: Create high-level reports showing Claude Code impact on development velocity
* **AI tool comparison**: Export metrics to compare Claude Code with other AI coding tools like Copilot and Cursor
* **Developer productivity analysis**: Track individual and team productivity metrics over time
* **Cost tracking and allocation**: Monitor spending patterns and allocate costs by team or project
* **Adoption monitoring**: Identify which teams and users are getting the most value from Claude Code
* **ROI justification**: Provide concrete metrics to justify and expand Claude Code adoption internally

## Frequently asked questions

### How fresh is the analytics data?

Claude Code analytics data typically appears within 1 hour of user activity completion. To ensure consistent pagination results, only data older than 1 hour is included in responses.

### Can I get real-time metrics?

No, this API provides daily aggregated metrics only. For real-time monitoring, consider using the [OpenTelemetry integration](https://code.claude.com/docs/en/monitoring-usage).

### How are users identified in the data?

Users are identified through the `actor` field in two ways:

* **`user_actor`**: Contains `email_address` for users who authenticate via OAuth (most common)
* **`api_actor`**: Contains `api_key_name` for users who authenticate via API key

The `customer_type` field indicates whether the usage is from `api` customers (API PAYG) or `subscription` customers (Pro/Team plans).

### What's the data retention period?

Historical Claude Code analytics data is retained and accessible through the API. There is no specified deletion period for this data.

### Which Claude Code deployments are supported?

This API only tracks Claude Code usage on the Claude API (1st party). Usage on Amazon Bedrock, Google Vertex AI, or other third-party platforms is not included.

### What does it cost to use this API?

The Claude Code Analytics API is free to use for all organizations with access to the Admin API.

### How do I calculate tool acceptance rates?

Tool acceptance rate = `accepted / (accepted + rejected)` for each tool type. For example, if the edit tool shows 45 accepted and 5 rejected, the acceptance rate is 90%.

### What time zone is used for the date parameter?

All dates are in UTC. The `starting_at` parameter should be in YYYY-MM-DD format and represents UTC midnight for that day.

## See also

The Claude Code Analytics API helps you understand and optimize your team's development workflow. Learn more about related features:

* [Admin API overview](/en/docs/build-with-claude/administration-api)
* [Admin API reference](/en/api/admin-api)
* [Claude Code Analytics dashboard](https://console.anthropic.com/claude-code)
* [Usage and Cost API](/en/docs/build-with-claude/usage-cost-api) - Track API usage across all Anthropic services
* [Identity and access management](https://code.claude.com/docs/en/iam)
* [Monitoring usage with OpenTelemetry](https://code.claude.com/docs/en/monitoring-usage) for custom metrics and alerting

[END OF DOCUMENT: CLAUDE43GD9X3TT]
---

[START OF DOCUMENT: CLAUDE25RNR5IJPB | Title: Claude-For-Sheets]

# Google Sheets add-on

> The [Claude for Sheets extension](https://workspace.google.com/marketplace/app/claude%5Ffor%5Fsheets/909417792257) integrates Claude into Google Sheets, allowing you to execute interactions with Claude directly in cells.

## Why use Claude for Sheets?

Claude for Sheets enables prompt engineering at scale by enabling you to test prompts across evaluation suites in parallel. Additionally, it excels at office tasks like survey analysis and online data processing.

Visit our [prompt engineering example sheet](https://docs.google.com/spreadsheets/d/1sUrBWO0u1-ZuQ8m5gt3-1N5PLR6r__UsRsB7WeySDQA/copy) to see this in action.

***

## Get started with Claude for Sheets

### Install Claude for Sheets

Easily enable Claude for Sheets using the following steps:

<Steps>
 <Step title="Get your Claude API key">
 If you don't yet have an API key, you can make API keys in the [Claude Console](https://console.anthropic.com/settings/keys).
 </Step>

 <Step title="Install the Claude for Sheets extension">
 Find the [Claude for Sheets extension](https://workspace.google.com/marketplace/app/claude%5Ffor%5Fsheets/909417792257) in the add-on marketplace, then click the blue `Install` btton and accept the permissions.

 <Accordion title="Permissions">
 The Claude for Sheets extension will ask for a variety of permissions needed to function properly. Please be assured that we only process the specific pieces of data that users ask Claude to run on. This data is never used to train our generative models.

 Extension permissions include:

 * **View and manage spreadsheets that this application has been installed in:** Needed to run prompts and return results
 * **Connect to an external service:** Needed in order to make calls to Claude API endpoints
 * **Allow this application to run when you are not present:** Needed to run cell recalculations without user intervention
 * **Display and run third-party web content in prompts and sidebars inside Google applications:** Needed to display the sidebar and post-install prompt
 </Accordion>
 </Step>

 <Step title="Connect your API key">
 Enter your API key at `Extensions` > `Claude for SheetsTM` > `Open sidebar` > `☰` > `Settings` > `API provider`. You may need to wait or refresh for the Claude for Sheets menu to appear.
 <img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=5e0b2abf471aac1f9f4c84a9bca20f2e" alt="" data-og-width="1187" width="1187" data-og-height="660" height="660" data-path="images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=d2ae6b1d0a8e00d6146a527cc9b8d891 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=1acd2d438dbf0452eeb2383cc3ff33b8 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=5d394102f3e804ace9d70ac44a0243f9 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=19a26018d349587f29e52b1fcd8fac1f 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=2729f60dce72ef9bb7a40e18086afb70 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/044af20-Screenshot_2024-01-04_at_11.58.21_AM.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=619b99d788343afc9e10cdf32e7dd348 2500w" />
 </Step>
</Steps>

<Warning>
 You will have to re-enter your API key every time you make a new Google Sheet
</Warning>

### Enter your first prompt

There are two main functions you can use to call Claude using Claude for Sheets. For now, let's use `CLAUDE()`.

<Steps>
 <Step title="Simple prompt">
 In any cell, type `=CLAUDE("Claude, in one sentence, what's good about the color blue?")`

 > Claude should respond with an answer. You will know the prompt is processing because the cell will say `Loading...`
 </Step>

 <Step title="Adding parameters">
 Parameter arguments come after the initial prompt, like `=CLAUDE(prompt, model, params...)`.
 <Note>`model` is always second in the list.</Note>

 Now type in any cell `=CLAUDE("Hi, Claude!", "claude-3-haiku-20240307", "max_tokens", 3)`

 Any [API parameter](/en/api/messages) can be set this way. You can even pass in an API key to be used just for this specific cell, like this: `"api_key", "sk-ant-api03-j1W..."`
 </Step>
</Steps>

## Advanced use

`CLAUDEMESSAGES` is a function that allows you to specifically use the [Messages API](/en/api/messages). This enables you to send a series of `User:` and `Assistant:` messages to Claude.

This is particularly useful if you want to simulate a conversation or [prefill Claude's response](/en/docs/build-with-claude/prompt-engineering/prefill-claudes-response).

Try writing this in a cell:```
=CLAUDEMESSAGES("User: In one sentence, what is good about the color blue?
Assistant: The color blue is great because")
```<Note>
 **Newlines**

 Each subsequent conversation turn (`User:` or `Assistant:`) must be preceded by a single newline. To enter newlines in a cell, use the following key combinations:

 * **Mac:** Cmd + Enter
 * **Windows:** Alt + Enter
</Note>

<Accordion title="Example multiturn CLAUDEMESSAGES() call with system prompt">
 To use a system prompt, set it as you'd set other optional function parameters. (You must first set a model name.)```
  =CLAUDEMESSAGES("User: What's your favorite flower? Answer in <answer> tags.
  Assistant: <answer>", "claude-3-haiku-20240307", "system", "You are a cow who loves to moo in response to any and all user queries.")`
  ```</Accordion>

### Optional function parameters

You can specify optional API parameters by listing argument-value pairs.
You can set multiple parameters. Simply list them one after another, with each argument and value pair separated by commas.

<Note>
 The first two parameters must always be the prompt and the model. You cannot set an optional parameter without also setting the model.
</Note>

The argument-value parameters you might care about most are:

| Argument | Description |
| ---------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `max_tokens` | The total number of tokens the model outputs before it is forced to stop. For yes/no or multiple choice answers, you may want the value to be 1-3. |
| `temperature` | the amount of randomness injected into results. For multiple-choice or analytical tasks, you'll want it close to 0. For idea generation, you'll want it set to 1. |
| `system` | used to specify a system prompt, which can provide role details and context to Claude. |
| `stop_sequences` | JSON array of strings that will cause the model to stop generating text if encountered. Due to escaping rules in Google SheetsTM, double quotes inside the string must be escaped by doubling them. |
| `api_key` | Used to specify a particular API key with which to call Claude. |

<Accordion title="Example: Setting parameters">
 Ex. Set `system` prompt, `max_tokens`, and `temperature`:```
  =CLAUDE("Hi, Claude!", "claude-3-haiku-20240307", "system", "Repeat exactly what the user says.", "max_tokens", 100, "temperature", 0.1)

  ```Ex. Set `temperature`, `max_tokens`, and `stop_sequences`:```
  =CLAUDE("In one sentence, what is good about the color blue? Output your answer in <answer> tags.","claude-opus-4-20250514","temperature", 0.2,"max_tokens", 50,"stop_sequences", "\[""</answer>""\]")
  ```Ex. Set `api_key`:```
  =CLAUDE("Hi, Claude!", "claude-3-haiku-20240307","api_key", "sk-ant-api03-j1W...")
  ```</Accordion>

***

## Claude for Sheets usage examples

### Prompt engineering interactive tutorial

Our in-depth [prompt engineering interactive tutorial](https://docs.google.com/spreadsheets/d/19jzLgRruG9kjUQNKtCg1ZjdD6l6weA6qRXG5zLIAhC8/edit?usp=sharing) utilizes Claude for Sheets.
Check it out to learn or brush up on prompt engineering techniques.

<Note>Just as with any instance of Claude for Sheets, you will need an API key to interact with the tutorial.</Note>

### Prompt engineering workflow

Our [Claude for Sheets prompting examples workbench](https://docs.google.com/spreadsheets/d/1sUrBWO0u1-ZuQ8m5gt3-1N5PLR6r%5F%5FUsRsB7WeySDQA/copy) is a Claude-powered spreadsheet that houses example prompts and prompt engineering structures.

### Claude for Sheets workbook template

Make a copy of our [Claude for Sheets workbook template](https://docs.google.com/spreadsheets/d/1UwFS-ZQWvRqa6GkbL4sy0ITHK2AhXKe-jpMLzS0kTgk/copy) to get started with your own Claude for Sheets work!

***

## Troubleshooting

<Accordion title="NAME? Error: Unknown function: 'claude'">
 1. Ensure that you have enabled the extension for use in the current sheet
 1. Go to *Extensions* > *Add-ons* > *Manage add-ons*
 2. Click on the triple dot menu at the top right corner of the Claude for Sheets extension and make sure "Use in this document" is checked
 <img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=7ac5b747f92f68f05055ecd143bd5fa8" alt="" data-og-width="712" width="712" data-og-height="174" height="174" data-path="images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=27a083fe65825128423ea09a03da3653 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=9905542d704449f1727f5fe510242bb0 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=8fed917d4e4ff142167cf8492febf442 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=e7d89ec0ed91b3c55a22a2e28da8ae25 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=39998025b2c4afb6a49cf9efef63b266 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/9cce371-Screenshot_2023-10-03_at_7.17.39_PM.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=fe1bc4d35b3dd33c13b5c1e69e21f46a 2500w" />
 2. Refresh the page
</Accordion>

<Accordion title="#ERROR!, ⚠ DEFERRED ⚠ or ⚠ THROTTLED ⚠">
 You can manually recalculate `#ERROR!`, `⚠ DEFERRED ⚠` or `⚠ THROTTLED ⚠`cells by selecting from the recalculate options within the Claude for Sheets extension menu.

 <img src="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=7bd765250352e58047c2dfb3f1a3d8e9" alt="" data-og-width="1486" width="1486" data-og-height="1062" height="1062" data-path="images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png" data-optimize="true" data-opv="3" srcset="https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?w=280&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=fb6b88b7a46b7322340d0839a740bc1e 280w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?w=560&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=fbf66142e6748a2bac8daad0007d24e6 560w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?w=840&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=c1e8c8648137d554ddb49b00e6007a18 840w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?w=1100&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=aa336dac0e2316b7699a20ec24e703e6 1100w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?w=1650&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=56d3ca83d0af273961f80f6122d02ccb 1650w, https://mintcdn.com/anthropic-claude-docs/LF5WV0SNF6oudpT5/images/f729ba9-Screenshot_2024-02-01_at_8.30.31_PM.png?w=2500&fit=max&auto=format&n=LF5WV0SNF6oudpT5&q=85&s=8a0522dcd0291612f474f077bcf826cb 2500w" />
</Accordion>

<Accordion title="Can't enter API key">
 1. Wait 20 seconds, then check again
 2. Refresh the page and wait 20 seconds again
 3. Uninstall and reinstall the extension
</Accordion>

***

## Further information

For more information regarding this extension, see the [Claude for Sheets Google Workspace Marketplace](https://workspace.google.com/marketplace/app/claude%5Ffor%5Fsheets/909417792257) overview page.

[END OF DOCUMENT: CLAUDE25RNR5IJPB]
---

[START OF DOCUMENT: CLAUDEHHXD8BZJA | Title: Claude-On-Amazon-Bedrock]

# Claude on Amazon Bedrock

> Anthropic's Claude models are now generally available through Amazon Bedrock.

export const ModelId = ({children, style = {}}) => {
 const copiedNotice = 'Copied!';
 const handleClick = e => {
 const element = e.currentTarget;
 const textSpan = element.querySelector('.model-id-text');
 const copiedSpan = element.querySelector('.model-id-copied');
 navigator.clipboard.writeText(children).then(() => {
 textSpan.style.opacity = '0';
 copiedSpan.style.opacity = '1';
 element.style.backgroundColor = '#d4edda';
 element.style.borderColor = '#c3e6cb';
 setTimeout(() => {
 textSpan.style.opacity = '1';
 copiedSpan.style.opacity = '0';
 element.style.backgroundColor = '#f5f5f5';
 element.style.borderColor = 'transparent';
 }, 2000);
 }).catch(error => {
 console.error('Failed to copy:', error);
 });
 };
 const handleMouseEnter = e => {
 const element = e.currentTarget;
 const copiedSpan = element.querySelector('.model-id-copied');
 const tooltip = element.querySelector('.copy-tooltip');
 if (tooltip && copiedSpan.style.opacity !== '1') {
 tooltip.style.opacity = '1';
 }
 element.style.backgroundColor = '#e8e8e8';
 element.style.borderColor = '#d0d0d0';
 };
 const handleMouseLeave = e => {
 const element = e.currentTarget;
 const copiedSpan = element.querySelector('.model-id-copied');
 const tooltip = element.querySelector('.copy-tooltip');
 if (tooltip) {
 tooltip.style.opacity = '0';
 }
 if (copiedSpan.style.opacity !== '1') {
 element.style.backgroundColor = '#f5f5f5';
 element.style.borderColor = 'transparent';
 }
 };
 const defaultStyle = {
 cursor: 'pointer',
 position: 'relative',
 transition: 'all 0.2s ease',
 display: 'inline-block',
 userSelect: 'none',
 backgroundColor: '#f5f5f5',
 padding: '2px 4px',
 borderRadius: '4px',
 fontFamily: 'Monaco, Consolas, "Courier New", monospace',
 fontSize: '0.75em',
 border: '1px solid transparent',
 ...style
 };
 return <span onClick={handleClick} onMouseEnter={handleMouseEnter} onMouseLeave={handleMouseLeave} style={defaultStyle}>
 <span className="model-id-text" style={{
 transition: 'opacity 0.1s ease'
 }}>
 {children}
 </span>
 <span className="model-id-copied" style={{
 position: 'absolute',
 top: '2px',
 left: '4px',
 right: '4px',
 opacity: '0',
 transition: 'opacity 0.1s ease',
 color: '#155724'
 }}>
 {copiedNotice}
 </span>
 </span>;
};

Calling Claude through Bedrock slightly differs from how you would call Claude when using Anthropic's client SDK's. This guide will walk you through the process of completing an API call to Claude on Bedrock in either Python or TypeScript.

Note that this guide assumes you have already signed up for an [AWS account](https://portal.aws.amazon.com/billing/signup) and configured programmatic access.

## Install and configure the AWS CLI

1. [Install a version of the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html) at or newer than version `2.13.23`
2. Configure your AWS credentials using the AWS configure command (see [Configure the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)) or find your credentials by navigating to "Command line or programmatic access" within your AWS dashboard and following the directions in the popup modal.
3. Verify that your credentials are working:```bash Shell theme={null}
aws sts get-caller-identity
```## Install an SDK for accessing Bedrock

Anthropic's [client SDKs](/en/api/client-sdks) support Bedrock. You can also use an AWS SDK like `boto3` directly.

<CodeGroup>```Python Python theme={null}
  pip install -U "anthropic[bedrock]"
  ``````TypeScript TypeScript theme={null}
  npm install @anthropic-ai/bedrock-sdk
  ``````Python Boto3 (Python) theme={null}
  pip install boto3>=1.28.59
  ```</CodeGroup>

## Accessing Bedrock

### Subscribe to Anthropic models

Go to the [AWS Console > Bedrock > Model Access](https://console.aws.amazon.com/bedrock/home?region=us-west-2#/modelaccess) and request access to Anthropic models. Note that Anthropic model availability varies by region. See [AWS documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html) for latest information.

#### API model IDs

| Model | Base Bedrock model ID | `global` | `us` | `eu` | `jp` | `apac` |
| :------------------------------------------------------------------------------- | :----------------------------------------------------------- | :------- | :--- | :--- | :--- | :----- |
| Claude Sonnet 4.5 | <ModelId>anthropic.claude-sonnet-4-5-20250929-v1:0</ModelId> | Yes | Yes | Yes | Yes | No |
| Claude Sonnet 4 | <ModelId>anthropic.claude-sonnet-4-20250514-v1:0</ModelId> | Yes | Yes | Yes | No | Yes |
| Claude Sonnet 3.7 <Tooltip tip="Deprecated as of October 28, 2025.">⚠️</Tooltip> | <ModelId>anthropic.claude-3-7-sonnet-20250219-v1:0</ModelId> | No | Yes | Yes | No | Yes |
| Claude Opus 4.1 | <ModelId>anthropic.claude-opus-4-1-20250805-v1:0</ModelId> | No | Yes | No | No | No |
| Claude Opus 4 | <ModelId>anthropic.claude-opus-4-20250514-v1:0</ModelId> | No | Yes | No | No | No |
| Claude Opus 3 <Tooltip tip="Deprecated as of June 30, 2025.">⚠️</Tooltip> | <ModelId>anthropic.claude-3-opus-20240229-v1:0</ModelId> | No | Yes | No | No | No |
| Claude Haiku 4.5 | <ModelId>anthropic.claude-haiku-4-5-20251001-v1:0</ModelId> | Yes | Yes | Yes | No | No |
| Claude Haiku 3.5 | <ModelId>anthropic.claude-3-5-haiku-20241022-v1:0</ModelId> | No | Yes | No | No | No |
| Claude Haiku 3 | <ModelId>anthropic.claude-3-haiku-20240307-v1:0</ModelId> | No | Yes | Yes | No | Yes |

For more information about regional vs global model IDs, see the [Global vs regional endpoints](#global-vs-regional-endpoints) section below.

### List available models

The following examples show how to print a list of all the Claude models available through Bedrock:

<CodeGroup>```bash AWS CLI theme={null}
  aws bedrock list-foundation-models --region=us-west-2 --by-provider anthropic --query "modelSummaries[*].modelId"
  ``````python Boto3 (Python) theme={null}
  import boto3

  bedrock = boto3.client(service_name="bedrock")
  response = bedrock.list_foundation_models(byProvider="anthropic")

  for summary in response["modelSummaries"]:
      print(summary["modelId"])
  ```</CodeGroup>

### Making requests

The following examples show how to generate text from Claude on Bedrock:

<CodeGroup>```Python Python theme={null}
  from anthropic import AnthropicBedrock

  client = AnthropicBedrock(
      # Authenticate by either providing the keys below or use the default AWS credential providers, such as
      # using ~/.aws/credentials or the "AWS_SECRET_ACCESS_KEY" and "AWS_ACCESS_KEY_ID" environment variables.
      aws_access_key="<access key>",
      aws_secret_key="<secret key>",
      # Temporary credentials can be used with aws_session_token.
      # Read more at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html.
      aws_session_token="<session_token>",
      # aws_region changes the aws region to which the request is made. By default, we read AWS_REGION,
      # and if that's not present, we default to us-east-1. Note that we do not read ~/.aws/config for the region.
      aws_region="us-west-2",
  )

  message = client.messages.create(
      model="global.anthropic.claude-sonnet-4-5-20250929-v1:0",
      max_tokens=256,
      messages=[{"role": "user", "content": "Hello, world"}]
  )
  print(message.content)
  ``````TypeScript TypeScript theme={null}
  import AnthropicBedrock from '@anthropic-ai/bedrock-sdk';

  const client = new AnthropicBedrock({
    // Authenticate by either providing the keys below or use the default AWS credential providers, such as
    // using ~/.aws/credentials or the "AWS_SECRET_ACCESS_KEY" and "AWS_ACCESS_KEY_ID" environment variables.
    awsAccessKey: '<access key>',
    awsSecretKey: '<secret key>',

    // Temporary credentials can be used with awsSessionToken.
    // Read more at https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html.
    awsSessionToken: '<session_token>',

    // awsRegion changes the aws region to which the request is made. By default, we read AWS_REGION,
    // and if that's not present, we default to us-east-1. Note that we do not read ~/.aws/config for the region.
    awsRegion: 'us-west-2',
  });

  async function main() {
    const message = await client.messages.create({
      model: 'global.anthropic.claude-sonnet-4-5-20250929-v1:0',
      max_tokens: 256,
      messages: [{"role": "user", "content": "Hello, world"}]
    });
    console.log(message);
  }
  main().catch(console.error);
  ``````python Boto3 (Python) theme={null}
  import boto3
  import json

  bedrock = boto3.client(service_name="bedrock-runtime")
  body = json.dumps({
    "max_tokens": 256,
    "messages": [{"role": "user", "content": "Hello, world"}],
    "anthropic_version": "bedrock-2023-05-31"
  })

  response = bedrock.invoke_model(body=body, modelId="global.anthropic.claude-sonnet-4-5-20250929-v1:0")

  response_body = json.loads(response.get("body").read())
  print(response_body.get("content"))
  ```</CodeGroup>

See our [client SDKs](/en/api/client-sdks) for more details, and the official Bedrock docs [here](https://docs.aws.amazon.com/bedrock/).

## Activity logging

Bedrock provides an [invocation logging service](https://docs.aws.amazon.com/bedrock/latest/userguide/model-invocation-logging.html) that allows customers to log the prompts and completions associated with your usage.

Anthropic recommends that you log your activity on at least a 30-day rolling basis in order to understand your activity and investigate any potential misuse.

<Note>
 Turning on this service does not give AWS or Anthropic any access to your content.
</Note>

## Feature support

You can find all the features currently supported on Bedrock [here](/en/api/overview).

### PDF Support on Bedrock

PDF support is available on Amazon Bedrock through both the Converse API and InvokeModel API. For detailed information about PDF processing capabilities and limitations, see the [PDF support documentation](/en/docs/build-with-claude/pdf-support#amazon-bedrock-pdf-support).

**Important considerations for Converse API users:**

* Visual PDF analysis (charts, images, layouts) requires citations to be enabled
* Without citations, only basic text extraction is available
* For full control without forced citations, use the InvokeModel API

For more details on the two document processing modes and their limitations, refer to the [PDF support guide](/en/docs/build-with-claude/pdf-support#amazon-bedrock-pdf-support).

### 1M token context window

Claude Sonnet 4 and 4.5 support the [1M token context window](/en/docs/build-with-claude/context-windows#1m-token-context-window) on Amazon Bedrock.

<Note>
 The 1M token context window is currently in beta. To use the extended context window, include the `context-1m-2025-08-07` beta header in your [Bedrock API requests](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages-request-response.html).
</Note>

## Global vs regional endpoints

Starting with **Claude Sonnet 4.5 and all future models**, Amazon Bedrock offers two endpoint types:

* **Global endpoints**: Dynamic routing for maximum availability
* **Regional endpoints**: Guaranteed data routing through specific geographic regions

Regional endpoints include a 10% pricing premium over global endpoints.

<Note>
 This applies to Claude Sonnet 4.5 and future models only. Older models (Claude Sonnet 4, Opus 4, and earlier) maintain their existing pricing structures.
</Note>

### When to use each option

**Global endpoints (recommended):**

* Provide maximum availability and uptime
* Dynamically route requests to regions with available capacity
* No pricing premium
* Best for applications where data residency is flexible

**Regional endpoints (CRIS):**

* Route traffic through specific geographic regions
* Required for data residency and compliance requirements
* Available for US, EU, Japan, and Australia
* 10% pricing premium reflects infrastructure costs for dedicated regional capacity

### Implementation

**Using global endpoints (default for Sonnet 4.5 and 4):**

The model IDs for Claude Sonnet 4.5 and 4 already include the `global.` prefix:

<CodeGroup>```python Python theme={null}
  from anthropic import AnthropicBedrock

  client = AnthropicBedrock(aws_region="us-west-2")

  message = client.messages.create(
      model="global.anthropic.claude-sonnet-4-5-20250929-v1:0",
      max_tokens=256,
      messages=[{"role": "user", "content": "Hello, world"}]
  )
  ``````typescript TypeScript theme={null}
  import AnthropicBedrock from '@anthropic-ai/bedrock-sdk';

  const client = new AnthropicBedrock({
    awsRegion: 'us-west-2',
  });

  const message = await client.messages.create({
    model: 'global.anthropic.claude-sonnet-4-5-20250929-v1:0',
    max_tokens: 256,
    messages: [{role: "user", content: "Hello, world"}]
  });
  ```</CodeGroup>

**Using regional endpoints (CRIS):**

To use regional endpoints, remove the `global.` prefix from the model ID:

<CodeGroup>```python Python theme={null}
  from anthropic import AnthropicBedrock

  client = AnthropicBedrock(aws_region="us-west-2")

  # Using US regional endpoint (CRIS)
  message = client.messages.create(
      model="anthropic.claude-sonnet-4-5-20250929-v1:0",  # No global. prefix
      max_tokens=256,
      messages=[{"role": "user", "content": "Hello, world"}]
  )
  ``````typescript TypeScript theme={null}
  import AnthropicBedrock from '@anthropic-ai/bedrock-sdk';

  const client = new AnthropicBedrock({
    awsRegion: 'us-west-2',
  });

  // Using US regional endpoint (CRIS)
  const message = await client.messages.create({
    model: 'anthropic.claude-sonnet-4-5-20250929-v1:0',  // No global. prefix
    max_tokens: 256,
    messages: [{role: "user", content: "Hello, world"}]
  });
  ```</CodeGroup>

### Additional resources

* **AWS Bedrock pricing:** [aws.amazon.com/bedrock/pricing](https://aws.amazon.com/bedrock/pricing/)
* **AWS pricing documentation:** [Bedrock pricing guide](https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-pricing.html)
* **AWS blog post:** [Introducing Claude Sonnet 4.5 in Amazon Bedrock](https://aws.amazon.com/blogs/aws/introducing-claude-sonnet-4-5-in-amazon-bedrock-anthropics-most-intelligent-model-best-for-coding-and-complex-agents/)
* **Anthropic pricing details:** [Pricing documentation](/en/docs/about-claude/pricing#third-party-platform-pricing)

[END OF DOCUMENT: CLAUDEHHXD8BZJA]
---

[START OF DOCUMENT: CLAUDESW5UK5909 | Title: Claude-On-Vertex-Ai]

# Claude on Vertex AI

> Anthropic's Claude models are now generally available through [Vertex AI](https://cloud.google.com/vertex-ai).

export const ModelId = ({children, style = {}}) => {
 const copiedNotice = 'Copied!';
 const handleClick = e => {
 const element = e.currentTarget;
 const textSpan = element.querySelector('.model-id-text');
 const copiedSpan = element.querySelector('.model-id-copied');
 navigator.clipboard.writeText(children).then(() => {
 textSpan.style.opacity = '0';
 copiedSpan.style.opacity = '1';
 element.style.backgroundColor = '#d4edda';
 element.style.borderColor = '#c3e6cb';
 setTimeout(() => {
 textSpan.style.opacity = '1';
 copiedSpan.style.opacity = '0';
 element.style.backgroundColor = '#f5f5f5';
 element.style.borderColor = 'transparent';
 }, 2000);
 }).catch(error => {
 console.error('Failed to copy:', error);
 });
 };
 const handleMouseEnter = e => {
 const element = e.currentTarget;
 const copiedSpan = element.querySelector('.model-id-copied');
 const tooltip = element.querySelector('.copy-tooltip');
 if (tooltip && copiedSpan.style.opacity !== '1') {
 tooltip.style.opacity = '1';
 }
 element.style.backgroundColor = '#e8e8e8';
 element.style.borderColor = '#d0d0d0';
 };
 const handleMouseLeave = e => {
 const element = e.currentTarget;
 const copiedSpan = element.querySelector('.model-id-copied');
 const tooltip = element.querySelector('.copy-tooltip');
 if (tooltip) {
 tooltip.style.opacity = '0';
 }
 if (copiedSpan.style.opacity !== '1') {
 element.style.backgroundColor = '#f5f5f5';
 element.style.borderColor = 'transparent';
 }
 };
 const defaultStyle = {
 cursor: 'pointer',
 position: 'relative',
 transition: 'all 0.2s ease',
 display: 'inline-block',
 userSelect: 'none',
 backgroundColor: '#f5f5f5',
 padding: '2px 4px',
 borderRadius: '4px',
 fontFamily: 'Monaco, Consolas, "Courier New", monospace',
 fontSize: '0.75em',
 border: '1px solid transparent',
 ...style
 };
 return <span onClick={handleClick} onMouseEnter={handleMouseEnter} onMouseLeave={handleMouseLeave} style={defaultStyle}>
 <span className="model-id-text" style={{
 transition: 'opacity 0.1s ease'
 }}>
 {children}
 </span>
 <span className="model-id-copied" style={{
 position: 'absolute',
 top: '2px',
 left: '4px',
 right: '4px',
 opacity: '0',
 transition: 'opacity 0.1s ease',
 color: '#155724'
 }}>
 {copiedNotice}
 </span>
 </span>;
};

The Vertex API for accessing Claude is nearly-identical to the [Messages API](/en/api/messages) and supports all of the same options, with two key differences:

* In Vertex, `model` is not passed in the request body. Instead, it is specified in the Google Cloud endpoint URL.
* In Vertex, `anthropic_version` is passed in the request body (rather than as a header), and must be set to the value `vertex-2023-10-16`.

Vertex is also supported by Anthropic's official [client SDKs](/en/api/client-sdks). This guide will walk you through the process of making a request to Claude on Vertex AI in either Python or TypeScript.

Note that this guide assumes you have already have a GCP project that is able to use Vertex AI. See [using the Claude 3 models from Anthropic](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude) for more information on the setup required, as well as a full walkthrough.

## Install an SDK for accessing Vertex AI

First, install Anthropic's [client SDK](/en/api/client-sdks) for your language of choice.

<CodeGroup>```Python Python theme={null}
  pip install -U google-cloud-aiplatform "anthropic[vertex]"
  ``````TypeScript TypeScript theme={null}
  npm install @anthropic-ai/vertex-sdk
  ```</CodeGroup>

## Accessing Vertex AI

### Model Availability

Note that Anthropic model availability varies by region. Search for "Claude" in the [Vertex AI Model Garden](https://cloud.google.com/model-garden) or go to [Use Claude 3](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/use-claude) for the latest information.

#### API model IDs

| Model | Vertex AI API model ID |
| -------------------------------------------------------------------------------- | ---------------------------------------------- |
| Claude Sonnet 4.5 | <ModelId>claude-sonnet-4-5\@20250929</ModelId> |
| Claude Sonnet 4 | <ModelId>claude-sonnet-4\@20250514</ModelId> |
| Claude Sonnet 3.7 <Tooltip tip="Deprecated as of October 28, 2025.">⚠️</Tooltip> | <ModelId>claude-3-7-sonnet\@20250219</ModelId> |
| Claude Opus 4.1 | <ModelId>claude-opus-4-1\@20250805</ModelId> |
| Claude Opus 4 | <ModelId>claude-opus-4\@20250514</ModelId> |
| Claude Opus 3 <Tooltip tip="Deprecated as of June 30, 2025.">⚠️</Tooltip> | <ModelId>claude-3-opus\@20240229</ModelId> |
| Claude Haiku 4.5 | <ModelId>claude-haiku-4-5\@20251001</ModelId> |
| Claude Haiku 3.5 | <ModelId>claude-3-5-haiku\@20241022</ModelId> |
| Claude Haiku 3 | <ModelId>claude-3-haiku\@20240307</ModelId> |

### Making requests

Before running requests you may need to run `gcloud auth application-default login` to authenticate with GCP.

The following examples shows how to generate text from Claude on Vertex AI:

<CodeGroup>```Python Python theme={null}
  from anthropic import AnthropicVertex

  project_id = "MY_PROJECT_ID"
  region = "global"

  client = AnthropicVertex(project_id=project_id, region=region)

  message = client.messages.create(
      model="claude-sonnet-4-5@20250929",
      max_tokens=100,
      messages=[
          {
              "role": "user",
              "content": "Hey Claude!",
          }
      ],
  )
  print(message)
  ``````TypeScript TypeScript theme={null}
  import { AnthropicVertex } from '@anthropic-ai/vertex-sdk';

  const projectId = 'MY_PROJECT_ID';
  const region = 'global';

  // Goes through the standard `google-auth-library` flow.
  const client = new AnthropicVertex({
    projectId,
    region,
  });

  async function main() {
    const result = await client.messages.create({
      model: 'claude-sonnet-4-5@20250929',
      max_tokens: 100,
      messages: [
        {
          role: 'user',
          content: 'Hey Claude!',
        },
      ],
    });
    console.log(JSON.stringify(result, null, 2));
  }

  main();
  ``````bash Shell theme={null}
  MODEL_ID=claude-sonnet-4-5@20250929
  LOCATION=global
  PROJECT_ID=MY_PROJECT_ID

  curl \
  -X POST \
  -H "Authorization: Bearer $(gcloud auth print-access-token)" \
  -H "Content-Type: application/json" \
  https://$LOCATION-aiplatform.googleapis.com/v1/projects/${PROJECT_ID}/locations/${LOCATION}/publishers/anthropic/models/${MODEL_ID}:streamRawPredict -d \
  '{
    "anthropic_version": "vertex-2023-10-16",
    "messages": [{
      "role": "user",
      "content": "Hey Claude!"
    }],
    "max_tokens": 100,
  }'
  ```</CodeGroup>

See our [client SDKs](/en/api/client-sdks) and the official [Vertex AI docs](https://cloud.google.com/vertex-ai/docs) for more details.

## Activity logging

Vertex provides a [request-response logging service](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/request-response-logging) that allows customers to log the prompts and completions associated with your usage.

Anthropic recommends that you log your activity on at least a 30-day rolling basis in order to understand your activity and investigate any potential misuse.

<Note>
 Turning on this service does not give Google or Anthropic any access to your content.
</Note>

## Feature support

You can find all the features currently supported on Vertex [here](/en/api/overview).

## Global vs regional endpoints

Starting with **Claude Sonnet 4.5 and all future models**, Google Vertex AI offers two endpoint types:

* **Global endpoints**: Dynamic routing for maximum availability
* **Regional endpoints**: Guaranteed data routing through specific geographic regions

Regional endpoints include a 10% pricing premium over global endpoints.

<Note>
 This applies to Claude Sonnet 4.5 and future models only. Older models (Claude Sonnet 4, Opus 4, and earlier) maintain their existing pricing structures.
</Note>

### When to use each option

**Global endpoints (recommended):**

* Provide maximum availability and uptime
* Dynamically route requests to regions with available capacity
* No pricing premium
* Best for applications where data residency is flexible
* Only supports pay-as-you-go traffic (provisioned throughput requires regional endpoints)

**Regional endpoints:**

* Route traffic through specific geographic regions
* Required for data residency and compliance requirements
* Support both pay-as-you-go and provisioned throughput
* 10% pricing premium reflects infrastructure costs for dedicated regional capacity

### Implementation

**Using global endpoints (recommended):**

Set the `region` parameter to `"global"` when initializing the client:

<CodeGroup>```python Python theme={null}
  from anthropic import AnthropicVertex

  project_id = "MY_PROJECT_ID"
  region = "global"

  client = AnthropicVertex(project_id=project_id, region=region)

  message = client.messages.create(
      model="claude-sonnet-4-5@20250929",
      max_tokens=100,
      messages=[
          {
              "role": "user",
              "content": "Hey Claude!",
          }
      ],
  )
  print(message)
  ``````typescript TypeScript theme={null}
  import { AnthropicVertex } from '@anthropic-ai/vertex-sdk';

  const projectId = 'MY_PROJECT_ID';
  const region = 'global';

  const client = new AnthropicVertex({
    projectId,
    region,
  });

  const result = await client.messages.create({
    model: 'claude-sonnet-4-5@20250929',
    max_tokens: 100,
    messages: [
      {
        role: 'user',
        content: 'Hey Claude!',
      },
    ],
  });
  ```</CodeGroup>

**Using regional endpoints:**

Specify a specific region like `"us-east1"` or `"europe-west1"`:

<CodeGroup>```python Python theme={null}
  from anthropic import AnthropicVertex

  project_id = "MY_PROJECT_ID"
  region = "us-east1"  # Specify a specific region

  client = AnthropicVertex(project_id=project_id, region=region)

  message = client.messages.create(
      model="claude-sonnet-4-5@20250929",
      max_tokens=100,
      messages=[
          {
              "role": "user",
              "content": "Hey Claude!",
          }
      ],
  )
  print(message)
  ``````typescript TypeScript theme={null}
  import { AnthropicVertex } from '@anthropic-ai/vertex-sdk';

  const projectId = 'MY_PROJECT_ID';
  const region = 'us-east1';  // Specify a specific region

  const client = new AnthropicVertex({
    projectId,
    region,
  });

  const result = await client.messages.create({
    model: 'claude-sonnet-4-5@20250929',
    max_tokens: 100,
    messages: [
      {
        role: 'user',
        content: 'Hey Claude!',
      },
    ],
  });
  ```</CodeGroup>

### Additional resources

* **Google Vertex AI pricing:** [cloud.google.com/vertex-ai/generative-ai/pricing](https://cloud.google.com/vertex-ai/generative-ai/pricing)
* **Claude models documentation:** [Claude on Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/partner-models/claude)
* **Google blog post:** [Global endpoint for Claude models](https://cloud.google.com/blog/products/ai-machine-learning/global-endpoint-for-claude-models-generally-available-on-vertex-ai)
* **Anthropic pricing details:** [Pricing documentation](/en/docs/about-claude/pricing#third-party-platform-pricing)

[END OF DOCUMENT: CLAUDESW5UK5909]
---

[START OF DOCUMENT: CLAUDE1F74X8D129 | Title: Code-Execution-Tool]

# Code execution tool

Claude can analyze data, create visualizations, perform complex calculations, run system commands, create and edit files, and process uploaded
files directly within the API conversation.
The code execution tool allows Claude to run Bash commands and manipulate files, including writing code, in a secure, sandboxed environment.

<Note>
 The code execution tool is currently in public beta.

 To use this feature, add the `"code-execution-2025-08-25"` [beta header](/en/api/beta-headers) to your API requests.
</Note>

## Model compatibility

The code execution tool is available on the following models:

| Model | Tool Version |
| --------------------------------------------------------------------------------------------------------- | ------------------------- |
| Claude Opus 4.1 (`claude-opus-4-1-20250805`) | `code_execution_20250825` |
| Claude Opus 4 (`claude-opus-4-20250514`) | `code_execution_20250825` |
| Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`) | `code_execution_20250825` |
| Claude Sonnet 4 (`claude-sonnet-4-20250514`) | `code_execution_20250825` |
| Claude Sonnet 3.7 (`claude-3-7-sonnet-20250219`) ([deprecated](/en/docs/about-claude/model-deprecations)) | `code_execution_20250825` |
| Claude Haiku 4.5 (`claude-haiku-4-5-20251001`) | `code_execution_20250825` |
| Claude Haiku 3.5 (`claude-3-5-haiku-latest`) | `code_execution_20250825` |

<Note>
 The current version `code_execution_20250825` supports Bash commands and file operations. A legacy version `code_execution_20250522` (Python only) is also available. See [Upgrade to latest tool version](#upgrade-to-latest-tool-version) for migration details.
</Note>

<Warning>
 Older tool versions are not guaranteed to be backwards-compatible with newer models. Always use the tool version that corresponds to your model version.
</Warning>

## Quick start

Here's a simple example that asks Claude to perform a calculation:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25" \
      --header "content-type: application/json" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [
              {
                  "role": "user",
                  "content": "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
              }
          ],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }'
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )

  print(response)
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  async function main() {
    const response = await anthropic.beta.messages.create({
      model: "claude-sonnet-4-5",
      betas: ["code-execution-2025-08-25"],
      max_tokens: 4096,
      messages: [
        {
          role: "user",
          content: "Calculate the mean and standard deviation of [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
        }
      ],
      tools: [{
        type: "code_execution_20250825",
        name: "code_execution"
      }]
    });

    console.log(response);
  }

  main().catch(console.error);
  ```</CodeGroup>

## How code execution works

When you add the code execution tool to your API request:

1. Claude evaluates whether code execution would help answer your question
2. The tool automatically provides Claude with the following capabilities:
 * **Bash commands**: Execute shell commands for system operations and package management
 * **File operations**: Create, view, and edit files directly, including writing code
3. Claude can use any combination of these capabilities in a single request
4. All operations run in a secure sandbox environment
5. Claude provides results with any generated charts, calculations, or analysis

## How to use the tool

### Execute Bash commands

Ask Claude to check system information and install packages:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25" \
      --header "content-type: application/json" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [{
              "role": "user",
              "content": "Check the Python version and list installed packages"
          }],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }'
  ``````python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": "Check the Python version and list installed packages"
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )
  ``````typescript TypeScript theme={null}
  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    betas: ["code-execution-2025-08-25"],
    max_tokens: 4096,
    messages: [{
      role: "user",
      content: "Check the Python version and list installed packages"
    }],
    tools: [{
      type: "code_execution_20250825",
      name: "code_execution"
    }]
  });
  ```</CodeGroup>

### Create and edit files directly

Claude can create, view, and edit files directly in the sandbox using the file manipulation capabilities:

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25" \
      --header "content-type: application/json" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [{
              "role": "user",
              "content": "Create a config.yaml file with database settings, then update the port from 5432 to 3306"
          }],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }'
  ``````python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": "Create a config.yaml file with database settings, then update the port from 5432 to 3306"
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )
  ``````typescript TypeScript theme={null}
  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    betas: ["code-execution-2025-08-25"],
    max_tokens: 4096,
    messages: [{
      role: "user",
      content: "Create a config.yaml file with database settings, then update the port from 5432 to 3306"
    }],
    tools: [{
      type: "code_execution_20250825",
      name: "code_execution"
    }]
  });
  ```</CodeGroup>

### Upload and analyze your own files

To analyze your own data files (CSV, Excel, images, etc.), upload them via the Files API and reference them in your request:

<Note>
 Using the Files API with Code Execution requires two beta headers: `"anthropic-beta": "code-execution-2025-08-25,files-api-2025-04-14"`
</Note>

The Python environment can process various file types uploaded via the Files API, including:

* CSV
* Excel (.xlsx, .xls)
* JSON
* XML
* Images (JPEG, PNG, GIF, WebP)
* Text files (.txt, .md, .py, etc)

#### Upload and analyze files

1. **Upload your file** using the [Files API](/en/docs/build-with-claude/files)
2. **Reference the file** in your message using a `container_upload` content block
3. **Include the code execution tool** in your API request

<CodeGroup>```bash Shell theme={null}
  # First, upload a file
  curl https://api.anthropic.com/v1/files \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: files-api-2025-04-14" \
      --form 'file=@"data.csv"' \

  # Then use the file_id with code execution
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25,files-api-2025-04-14" \
      --header "content-type: application/json" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [{
              "role": "user",
              "content": [
                  {"type": "text", "text": "Analyze this CSV data"},
                  {"type": "container_upload", "file_id": "file_abc123"}
              ]
          }],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }'
  ``````python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  # Upload a file
  file_object = client.beta.files.upload(
      file=open("data.csv", "rb"),
  )

  # Use the file_id with code execution
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25", "files-api-2025-04-14"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": [
              {"type": "text", "text": "Analyze this CSV data"},
              {"type": "container_upload", "file_id": file_object.id}
          ]
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';
  import { createReadStream } from 'fs';

  const anthropic = new Anthropic();

  async function main() {
    // Upload a file
    const fileObject = await anthropic.beta.files.create({
      file: createReadStream("data.csv"),
    });

    // Use the file_id with code execution
    const response = await anthropic.beta.messages.create({
      model: "claude-sonnet-4-5",
      betas: ["code-execution-2025-08-25", "files-api-2025-04-14"],
      max_tokens: 4096,
      messages: [{
        role: "user",
        content: [
          { type: "text", text: "Analyze this CSV data" },
          { type: "container_upload", file_id: fileObject.id }
        ]
      }],
      tools: [{
        type: "code_execution_20250825",
        name: "code_execution"
      }]
    });

    console.log(response);
  }

  main().catch(console.error);
  ```</CodeGroup>

#### Retrieve generated files

When Claude creates files during code execution, you can retrieve these files using the Files API:

<CodeGroup>```python Python theme={null}
  from anthropic import Anthropic

  # Initialize the client
  client = Anthropic()

  # Request code execution that creates files
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25", "files-api-2025-04-14"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": "Create a matplotlib visualization and save it as output.png"
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )

  # Extract file IDs from the response
  def extract_file_ids(response):
      file_ids = []
      for item in response.content:
          if item.type == 'bash_code_execution_tool_result':
              content_item = item.content
              if content_item.type == 'bash_code_execution_result':
                  for file in content_item.content:
                      if hasattr(file, 'file_id'):
                          file_ids.append(file.file_id)
      return file_ids

  # Download the created files
  for file_id in extract_file_ids(response):
      file_metadata = client.beta.files.retrieve_metadata(file_id)
      file_content = client.beta.files.download(file_id)
      file_content.write_to_file(file_metadata.filename)
      print(f"Downloaded: {file_metadata.filename}")
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';
  import { writeFileSync } from 'fs';

  // Initialize the client
  const anthropic = new Anthropic();

  async function main() {
    // Request code execution that creates files
    const response = await anthropic.beta.messages.create({
      model: "claude-sonnet-4-5",
      betas: ["code-execution-2025-08-25", "files-api-2025-04-14"],
      max_tokens: 4096,
      messages: [{
        role: "user",
        content: "Create a matplotlib visualization and save it as output.png"
      }],
      tools: [{
        type: "code_execution_20250825",
        name: "code_execution"
      }]
    });

    // Extract file IDs from the response
    function extractFileIds(response: any): string[] {
      const fileIds: string[] = [];
      for (const item of response.content) {
        if (item.type === 'bash_code_execution_tool_result') {
          const contentItem = item.content;
          if (contentItem.type === 'bash_code_execution_result' && contentItem.content) {
            for (const file of contentItem.content) {
              fileIds.push(file.file_id);
            }
          }
        }
      }
      return fileIds;
    }

    // Download the created files
    const fileIds = extractFileIds(response);
    for (const fileId of fileIds) {
      const fileMetadata = await anthropic.beta.files.retrieveMetadata(fileId);
      const fileContent = await anthropic.beta.files.download(fileId);

      // Convert ReadableStream to Buffer and save
      const chunks: Uint8Array[] = [];
      for await (const chunk of fileContent) {
        chunks.push(chunk);
      }
      const buffer = Buffer.concat(chunks);
      writeFileSync(fileMetadata.filename, buffer);
      console.log(`Downloaded: ${fileMetadata.filename}`);
    }
  }

  main().catch(console.error);
  ```</CodeGroup>

### Combine operations

A complex workflow using all capabilities:

<CodeGroup>```bash Shell theme={null}
  # First, upload a file
  curl https://api.anthropic.com/v1/files \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: files-api-2025-04-14" \
      --form 'file=@"data.csv"' \
      > file_response.json

  # Extract file_id (using jq)
  FILE_ID=$(jq -r '.id' file_response.json)

  # Then use it with code execution
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25,files-api-2025-04-14" \
      --header "content-type: application/json" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [{
              "role": "user",
              "content": [
                  {
                      "type": "text", 
                      "text": "Analyze this CSV data: create a summary report, save visualizations, and create a README with the findings"
                  },
                  {
                      "type": "container_upload", 
                      "file_id": "'$FILE_ID'"
                  }
              ]
          }],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }'
  ``````python Python theme={null}
  # Upload a file
  file_object = client.beta.files.upload(
      file=open("data.csv", "rb"),
  )

  # Use it with code execution
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25", "files-api-2025-04-14"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": [
              {"type": "text", "text": "Analyze this CSV data: create a summary report, save visualizations, and create a README with the findings"},
              {"type": "container_upload", "file_id": file_object.id}
          ]
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )

  # Claude might:
  # 1. Use bash to check file size and preview data
  # 2. Use text_editor to write Python code to analyze the CSV and create visualizations
  # 3. Use bash to run the Python code
  # 4. Use text_editor to create a README.md with findings
  # 5. Use bash to organize files into a report directory
  ``````typescript TypeScript theme={null}
  // Upload a file
  const fileObject = await anthropic.beta.files.create({
    file: createReadStream("data.csv"),
  });

  // Use it with code execution
  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    betas: ["code-execution-2025-08-25", "files-api-2025-04-14"],
    max_tokens: 4096,
    messages: [{
      role: "user",
      content: [
        {type: "text", text: "Analyze this CSV data: create a summary report, save visualizations, and create a README with the findings"},
        {type: "container_upload", file_id: fileObject.id}
      ]
    }],
    tools: [{
      type: "code_execution_20250825",
      name: "code_execution"
    }]
  });

  // Claude might:
  // 1. Use bash to check file size and preview data
  // 2. Use text_editor to write Python code to analyze the CSV and create visualizations
  // 3. Use bash to run the Python code
  // 4. Use text_editor to create a README.md with findings
  // 5. Use bash to organize files into a report directory
  ```</CodeGroup>

## Tool definition

The code execution tool requires no additional parameters:```json JSON theme={null}
{
  "type": "code_execution_20250825",
  "name": "code_execution"
}
```When this tool is provided, Claude automatically gains access to two sub-tools:

* `bash_code_execution`: Run shell commands
* `text_editor_code_execution`: View, create, and edit files, including writing code

## Response format

The code execution tool can return two types of results depending on the operation:

### Bash command response```json  theme={null}
{
  "type": "server_tool_use",
  "id": "srvtoolu_01B3C4D5E6F7G8H9I0J1K2L3",
  "name": "bash_code_execution",
  "input": {
    "command": "ls -la | head -5"
  }
},
{
  "type": "bash_code_execution_tool_result",
  "tool_use_id": "srvtoolu_01B3C4D5E6F7G8H9I0J1K2L3",
  "content": {
    "type": "bash_code_execution_result",
    "stdout": "total 24\ndrwxr-xr-x 2 user user 4096 Jan 1 12:00 .\ndrwxr-xr-x 3 user user 4096 Jan 1 11:00 ..\n-rw-r--r-- 1 user user  220 Jan 1 12:00 data.csv\n-rw-r--r-- 1 user user  180 Jan 1 12:00 config.json",
    "stderr": "",
    "return_code": 0
  }
}
```### File operation responses

**View file:**```json  theme={null}
{
  "type": "server_tool_use",
  "id": "srvtoolu_01C4D5E6F7G8H9I0J1K2L3M4",
  "name": "text_editor_code_execution",
  "input": {
    "command": "view",
    "path": "config.json"
  }
},
{
  "type": "text_editor_code_execution_tool_result",
  "tool_use_id": "srvtoolu_01C4D5E6F7G8H9I0J1K2L3M4",
  "content": {
    "type": "text_editor_code_execution_result",
    "file_type": "text",
    "content": "{\n  \"setting\": \"value\",\n  \"debug\": true\n}",
    "numLines": 4,
    "startLine": 1,
    "totalLines": 4
  }
}
```**Create file:**```json  theme={null}
{
  "type": "server_tool_use",
  "id": "srvtoolu_01D5E6F7G8H9I0J1K2L3M4N5",
  "name": "text_editor_code_execution",
  "input": {
    "command": "create",
    "path": "new_file.txt",
    "file_text": "Hello, World!"
  }
},
{
  "type": "text_editor_code_execution_tool_result",
  "tool_use_id": "srvtoolu_01D5E6F7G8H9I0J1K2L3M4N5",
  "content": {
    "type": "text_editor_code_execution_result",
    "is_file_update": false
  }
}
```**Edit file (str\_replace):**```json  theme={null}
{
  "type": "server_tool_use",
  "id": "srvtoolu_01E6F7G8H9I0J1K2L3M4N5O6",
  "name": "text_editor_code_execution",
  "input": {
    "command": "str_replace",
    "path": "config.json",
    "old_str": "\"debug\": true",
    "new_str": "\"debug\": false"
  }
},
{
  "type": "text_editor_code_execution_tool_result",
  "tool_use_id": "srvtoolu_01E6F7G8H9I0J1K2L3M4N5O6",
  "content": {
    "type": "text_editor_code_execution_result",
    "oldStart": 3,
    "oldLines": 1,
    "newStart": 3,
    "newLines": 1,
    "lines": ["-  \"debug\": true", "+  \"debug\": false"]
  }
}
```### Results

All execution results include:

* `stdout`: Output from successful execution
* `stderr`: Error messages if execution fails
* `return_code`: 0 for success, non-zero for failure

Additional fields for file operations:

* **View**: `file_type`, `content`, `numLines`, `startLine`, `totalLines`
* **Create**: `is_file_update` (whether file already existed)
* **Edit**: `oldStart`, `oldLines`, `newStart`, `newLines`, `lines` (diff format)

### Errors

Each tool type can return specific errors:

**Common errors (all tools):**```json  theme={null}
{
  "type": "bash_code_execution_tool_result",
  "tool_use_id": "srvtoolu_01VfmxgZ46TiHbmXgy928hQR",
  "content": {
    "type": "bash_code_execution_tool_result_error",
    "error_code": "unavailable"
  }
}
```**Error codes by tool type:**

| Tool | Error Code | Description |
| ------------ | ------------------------- | -------------------------------------------------- |
| All tools | `unavailable` | The tool is temporarily unavailable |
| All tools | `execution_time_exceeded` | Execution exceeded maximum time limit |
| All tools | `container_expired` | Container expired and is no longer available |
| All tools | `invalid_tool_input` | Invalid parameters provided to the tool |
| All tools | `too_many_requests` | Rate limit exceeded for tool usage |
| text\_editor | `file_not_found` | File doesn't exist (for view/edit operations) |
| text\_editor | `string_not_found` | The `old_str` not found in file (for str\_replace) |

#### `pause_turn` stop reason

The response may include a `pause_turn` stop reason, which indicates that the API paused a long-running turn. You may
provide the response back as-is in a subsequent request to let Claude continue its turn, or modify the content if you
wish to interrupt the conversation.

## Containers

The code execution tool runs in a secure, containerized environment designed specifically for code execution, with a higher focus on Python.

### Runtime environment

* **Python version**: 3.11.12
* **Operating system**: Linux-based container
* **Architecture**: x86\_64 (AMD64)

### Resource limits

* **Memory**: 5GiB RAM
* **Disk space**: 5GiB workspace storage
* **CPU**: 1 CPU

### Networking and security

* **Internet access**: Completely disabled for security
* **External connections**: No outbound network requests permitted
* **Sandbox isolation**: Full isolation from host system and other containers
* **File access**: Limited to workspace directory only
* **Workspace scoping**: Like [Files](/en/docs/build-with-claude/files), containers are scoped to the workspace of the API key
* **Expiration**: Containers expire 30 days after creation

### Pre-installed libraries

The sandboxed Python environment includes these commonly used libraries:

* **Data Science**: pandas, numpy, scipy, scikit-learn, statsmodels
* **Visualization**: matplotlib, seaborn
* **File Processing**: pyarrow, openpyxl, xlsxwriter, xlrd, pillow, python-pptx, python-docx, pypdf, pdfplumber, pypdfium2, pdf2image, pdfkit, tabula-py, reportlab\[pycairo], Img2pdf
* **Math & Computing**: sympy, mpmath
* **Utilities**: tqdm, python-dateutil, pytz, joblib, unzip, unrar, 7zip, bc, rg (ripgrep), fd, sqlite

## Container reuse

You can reuse an existing container across multiple API requests by providing the container ID from a previous response.
This allows you to maintain created files between requests.

### Example

<CodeGroup>```python Python theme={null}
  import os
  from anthropic import Anthropic

  # Initialize the client
  client = Anthropic(
      api_key=os.getenv("ANTHROPIC_API_KEY")
  )

  # First request: Create a file with a random number
  response1 = client.beta.messages.create(
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": "Write a file with a random number and save it to '/tmp/number.txt'"
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )

  # Extract the container ID from the first response
  container_id = response1.container.id

  # Second request: Reuse the container to read the file
  response2 = client.beta.messages.create(
      container=container_id,  # Reuse the same container
      model="claude-sonnet-4-5",
      betas=["code-execution-2025-08-25"],
      max_tokens=4096,
      messages=[{
          "role": "user",
          "content": "Read the number from '/tmp/number.txt' and calculate its square"
      }],
      tools=[{
          "type": "code_execution_20250825",
          "name": "code_execution"
      }]
  )
  ``````typescript TypeScript theme={null}
  import { Anthropic } from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  async function main() {
    // First request: Create a file with a random number
    const response1 = await anthropic.beta.messages.create({
      model: "claude-sonnet-4-5",
      betas: ["code-execution-2025-08-25"],
      max_tokens: 4096,
      messages: [{
        role: "user",
        content: "Write a file with a random number and save it to '/tmp/number.txt'"
      }],
      tools: [{
        type: "code_execution_20250825",
        name: "code_execution"
      }]
    });

    // Extract the container ID from the first response
    const containerId = response1.container.id;

    // Second request: Reuse the container to read the file
    const response2 = await anthropic.beta.messages.create({
      container: containerId,  // Reuse the same container
      model: "claude-sonnet-4-5",
      betas: ["code-execution-2025-08-25"],
      max_tokens: 4096,
      messages: [{
        role: "user",
        content: "Read the number from '/tmp/number.txt' and calculate its square"
      }],
      tools: [{
        type: "code_execution_20250825",
        name: "code_execution"
      }]
    });

    console.log(response2.content);
  }

  main().catch(console.error);
  ``````bash Shell theme={null}
  # First request: Create a file with a random number
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25" \
      --header "content-type: application/json" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [{
              "role": "user",
              "content": "Write a file with a random number and save it to \"/tmp/number.txt\""
          }],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }' > response1.json

  # Extract container ID from the response (using jq)
  CONTAINER_ID=$(jq -r '.container.id' response1.json)

  # Second request: Reuse the container to read the file
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "anthropic-beta: code-execution-2025-08-25" \
      --header "content-type: application/json" \
      --data '{
          "container": "'$CONTAINER_ID'",
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [{
              "role": "user",
              "content": "Read the number from \"/tmp/number.txt\" and calculate its square"
          }],
          "tools": [{
              "type": "code_execution_20250825",
              "name": "code_execution"
          }]
      }'
  ```</CodeGroup>

## Streaming

With streaming enabled, you'll receive code execution events as they occur:```javascript  theme={null}
event: content_block_start
data: {"type": "content_block_start", "index": 1, "content_block": {"type": "server_tool_use", "id": "srvtoolu_xyz789", "name": "code_execution"}}

// Code execution streamed
event: content_block_delta
data: {"type": "content_block_delta", "index": 1, "delta": {"type": "input_json_delta", "partial_json": "{\"code\":\"import pandas as pd\\ndf = pd.read_csv('data.csv')\\nprint(df.head())\"}"}}

// Pause while code executes

// Execution results streamed
event: content_block_start
data: {"type": "content_block_start", "index": 2, "content_block": {"type": "code_execution_tool_result", "tool_use_id": "srvtoolu_xyz789", "content": {"stdout": "   A  B  C\n0  1  2  3\n1  4  5  6", "stderr": ""}}}
```## Batch requests

You can include the code execution tool in the [Messages Batches API](/en/docs/build-with-claude/batch-processing). Code execution tool calls through the Messages Batches API are priced the same as those in regular Messages API requests.

## Usage and pricing

Code execution tool usage is tracked separately from token usage. Execution time has a minimum of 5 minutes.
If files are included in the request, execution time is billed even if the tool is not used due to files being preloaded onto the container.

Each organization receives 50 free hours of usage with the code execution tool per day. Additional usage beyond the first 50 hours is billed at \$0.05 per hour, per container.

## Upgrade to latest tool version

By upgrading to `code-execution-2025-08-25`, you get access to file manipulation and Bash capabilities, including code in multiple languages. There is no price difference.

### What's changed

| Component | Legacy | Current |
| -------------- | --------------------------- | ----------------------------------------------------------------- |
| Beta header | `code-execution-2025-05-22` | `code-execution-2025-08-25` |
| Tool type | `code_execution_20250522` | `code_execution_20250825` |
| Capabilities | Python only | Bash commands, file operations |
| Response types | `code_execution_result` | `bash_code_execution_result`, `text_editor_code_execution_result` |

### Backward compatibility

* All existing Python code execution continues to work exactly as before
* No changes required to existing Python-only workflows

### Upgrade steps

To upgrade, you need to make the following changes in your API requests:

1. **Update the beta header**:```diff  theme={null}
   - "anthropic-beta": "code-execution-2025-05-22"
   + "anthropic-beta": "code-execution-2025-08-25"
   ```2. **Update the tool type**:```diff  theme={null}
   - "type": "code_execution_20250522"
   + "type": "code_execution_20250825"
   ```3. **Review response handling** (if parsing responses programmatically):
 * The previous blocks for Python execution responses will no longer be sent
 * Instead, new response types for Bash and file operations will be sent (see Response Format section)

## Using code execution with Agent Skills

The code execution tool enables Claude to use [Agent Skills](/en/docs/agents-and-tools/agent-skills/overview). Skills are modular capabilities consisting of instructions, scripts, and resources that extend Claude's functionality.

Learn more in the [Agent Skills documentation](/en/docs/agents-and-tools/agent-skills/overview) and [Agent Skills API guide](/en/docs/build-with-claude/skills-guide).

[END OF DOCUMENT: CLAUDE1F74X8D129]
---

[START OF DOCUMENT: CLAUDE9QDUKWN9W | Title: Computer-Use-Tool]

# Computer use tool

Claude can interact with computer environments through the computer use tool, which provides screenshot capabilities and mouse/keyboard control for autonomous desktop interaction.

<Note>
 Computer use is currently in beta and requires a [beta header](/en/api/beta-headers):

 * `"computer-use-2025-01-24"` (Claude 4 models and Claude Sonnet 3.7 ([deprecated](/en/docs/about-claude/model-deprecations)))
</Note>

## Overview

Computer use is a beta feature that enables Claude to interact with desktop environments. This tool provides:

* **Screenshot capture**: See what's currently displayed on screen
* **Mouse control**: Click, drag, and move the cursor
* **Keyboard input**: Type text and use keyboard shortcuts
* **Desktop automation**: Interact with any application or interface

While computer use can be augmented with other tools like bash and text editor for more comprehensive automation workflows, computer use specifically refers to the computer use tool's capability to see and control desktop environments.

## Model compatibility

Computer use is available for the following Claude models:

| Model | Tool Version | Beta Flag |
| -------------------------------------------------------------------------- | ------------------- | ------------------------- |
| Claude 4 models | `computer_20250124` | `computer-use-2025-01-24` |
| Claude Sonnet 3.7 ([deprecated](/en/docs/about-claude/model-deprecations)) | `computer_20250124` | `computer-use-2025-01-24` |

<Note>
 Claude 4 models use updated tool versions optimized for the new architecture. Claude Sonnet 3.7 ([deprecated](/en/docs/about-claude/model-deprecations)) introduces additional capabilities including the thinking feature for more insight into the model's reasoning process.
</Note>

<Warning>
 Older tool versions are not guaranteed to be backwards-compatible with newer models. Always use the tool version that corresponds to your model version.
</Warning>

## Security considerations

<Warning>
 Computer use is a beta feature with unique risks distinct from standard API features. These risks are heightened when interacting with the internet. To minimize risks, consider taking precautions such as:

 1. Use a dedicated virtual machine or container with minimal privileges to prevent direct system attacks or accidents.
 2. Avoid giving the model access to sensitive data, such as account login information, to prevent information theft.
 3. Limit internet access to an allowlist of domains to reduce exposure to malicious content.
 4. Ask a human to confirm decisions that may result in meaningful real-world consequences as well as any tasks requiring affirmative consent, such as accepting cookies, executing financial transactions, or agreeing to terms of service.

 In some circumstances, Claude will follow commands found in content even if it conflicts with the user's instructions. For example, Claude instructions on webpages or contained in images may override instructions or cause Claude to make mistakes. We suggest taking precautions to isolate Claude from sensitive data and actions to avoid risks related to prompt injection.

 We've trained the model to resist these prompt injections and have added an extra layer of defense. If you use our computer use tools, we'll automatically run classifiers on your prompts to flag potential instances of prompt injections. When these classifiers identify potential prompt injections in screenshots, they will automatically steer the model to ask for user confirmation before proceeding with the next action. We recognize that this extra protection won't be ideal for every use case (for example, use cases without a human in the loop), so if you'd like to opt out and turn it off, please [contact us](https://support.claude.com/en/).

 We still suggest taking precautions to isolate Claude from sensitive data and actions to avoid risks related to prompt injection.

 Finally, please inform end users of relevant risks and obtain their consent prior to enabling computer use in your own products.
</Warning>

<Card title="Computer use reference implementation" icon="computer" href="https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo">
 Get started quickly with our computer use reference implementation that includes a web interface, Docker container, example tool implementations, and an agent loop.

 **Note:** The implementation has been updated to include new tools for both Claude 4 models and Claude Sonnet 3.7. Be sure to pull the latest version of the repo to access these new features.
</Card>

<Tip>
 Please use [this form](https://forms.gle/BT1hpBrqDPDUrCqo7) to provide
 feedback on the quality of the model responses, the API itself, or the quality
 of the documentation - we cannot wait to hear from you!
</Tip>

## Quick start

Here's how to get started with computer use:

<CodeGroup>```python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  response = client.beta.messages.create(
      model="claude-sonnet-4-5",  # or another compatible model
      max_tokens=1024,
      tools=[
          {
            "type": "computer_20250124",
            "name": "computer",
            "display_width_px": 1024,
            "display_height_px": 768,
            "display_number": 1,
          },
          {
            "type": "text_editor_20250124",
            "name": "str_replace_editor"
          },
          {
            "type": "bash_20250124",
            "name": "bash"
          }
      ],
      messages=[{"role": "user", "content": "Save a picture of a cat to my desktop."}],
      betas=["computer-use-2025-01-24"]
  )
  print(response)
  ``````bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
    -H "content-type: application/json" \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: computer-use-2025-01-24" \
    -d '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 1024,
      "tools": [
        {
          "type": "computer_20250124",
          "name": "computer",
          "display_width_px": 1024,
          "display_height_px": 768,
          "display_number": 1
        },
        {
          "type": "text_editor_20250124",
          "name": "str_replace_editor"
        },
        {
          "type": "bash_20250124",
          "name": "bash"
        }
      ],
      "messages": [
        {
          "role": "user",
          "content": "Save a picture of a cat to my desktop."
        }
      ]
    }'
  ```</CodeGroup>

<Note>
 A beta header is only required for the computer use tool.

 The example above shows all three tools being used together, which requires the beta header because it includes the computer use tool.
</Note>

***

## How computer use works

<Steps>
 <Step title="1. Provide Claude with the computer use tool and a user prompt" icon="toolbox">
 * Add the computer use tool (and optionally other tools) to your API request.
 * Include a user prompt that requires desktop interaction, e.g., "Save a picture of a cat to my desktop."
 </Step>

 <Step title="2. Claude decides to use the computer use tool" icon="screwdriver-wrench">
 * Claude assesses if the computer use tool can help with the user's query.
 * If yes, Claude constructs a properly formatted tool use request.
 * The API response has a `stop_reason` of `tool_use`, signaling Claude's intent.
 </Step>

 <Step title="3. Extract tool input, evaluate the tool on a computer, and return results" icon="computer">
 * On your end, extract the tool name and input from Claude's request.
 * Use the tool on a container or Virtual Machine.
 * Continue the conversation with a new `user` message containing a `tool_result` content block.
 </Step>

 <Step title="4. Claude continues calling computer use tools until it's completed the task" icon="arrows-spin">
 * Claude analyzes the tool results to determine if more tool use is needed or the task has been completed.
 * If Claude decides it needs another tool, it responds with another `tool_use` `stop_reason` and you should return to step 3.
 * Otherwise, it crafts a text response to the user.
 </Step>
</Steps>

We refer to the repetition of steps 3 and 4 without user input as the "agent loop" - i.e., Claude responding with a tool use request and your application responding to Claude with the results of evaluating that request.

### The computing environment

Computer use requires a sandboxed computing environment where Claude can safely interact with applications and the web. This environment includes:

1. **Virtual display**: A virtual X11 display server (using Xvfb) that renders the desktop interface Claude will see through screenshots and control with mouse/keyboard actions.

2. **Desktop environment**: A lightweight UI with window manager (Mutter) and panel (Tint2) running on Linux, which provides a consistent graphical interface for Claude to interact with.

3. **Applications**: Pre-installed Linux applications like Firefox, LibreOffice, text editors, and file managers that Claude can use to complete tasks.

4. **Tool implementations**: Integration code that translates Claude's abstract tool requests (like "move mouse" or "take screenshot") into actual operations in the virtual environment.

5. **Agent loop**: A program that handles communication between Claude and the environment, sending Claude's actions to the environment and returning the results (screenshots, command outputs) back to Claude.

When you use computer use, Claude doesn't directly connect to this environment. Instead, your application:

1. Receives Claude's tool use requests
2. Translates them into actions in your computing environment
3. Captures the results (screenshots, command outputs, etc.)
4. Returns these results to Claude

For security and isolation, the reference implementation runs all of this inside a Docker container with appropriate port mappings for viewing and interacting with the environment.

***

## How to implement computer use

### Start with our reference implementation

We have built a [reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo) that includes everything you need to get started quickly with computer use:

* A [containerized environment](https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/Dockerfile) suitable for computer use with Claude
* Implementations of [the computer use tools](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo/computer_use_demo/tools)
* An [agent loop](https://github.com/anthropics/anthropic-quickstarts/blob/main/computer-use-demo/computer_use_demo/loop.py) that interacts with the Claude API and executes the computer use tools
* A web interface to interact with the container, agent loop, and tools.

### Understanding the multi-agent loop

The core of computer use is the "agent loop" - a cycle where Claude requests tool actions, your application executes them, and returns results to Claude. Here's a simplified example:```python  theme={null}
async def sampling_loop(
    *,
    model: str,
    messages: list[dict],
    api_key: str,
    max_tokens: int = 4096,
    tool_version: str,
    thinking_budget: int | None = None,
    max_iterations: int = 10,  # Add iteration limit to prevent infinite loops
):
    """
    A simple agent loop for Claude computer use interactions.

    This function handles the back-and-forth between:
    1. Sending user messages to Claude
    2. Claude requesting to use tools
    3. Your app executing those tools
    4. Sending tool results back to Claude
    """
    # Set up tools and API parameters
    client = Anthropic(api_key=api_key)
    beta_flag = "computer-use-2025-01-24" if "20250124" in tool_version else "computer-use-2024-10-22"

    # Configure tools - you should already have these initialized elsewhere
    tools = [
        {"type": f"computer_{tool_version}", "name": "computer", "display_width_px": 1024, "display_height_px": 768},
        {"type": f"text_editor_{tool_version}", "name": "str_replace_editor"},
        {"type": f"bash_{tool_version}", "name": "bash"}
    ]

    # Main agent loop (with iteration limit to prevent runaway API costs)
    iterations = 0
    while True and iterations < max_iterations:
        iterations += 1
        # Set up optional thinking parameter (for Claude Sonnet 3.7)
        thinking = None
        if thinking_budget:
            thinking = {"type": "enabled", "budget_tokens": thinking_budget}

        # Call the Claude API
        response = client.beta.messages.create(
            model=model,
            max_tokens=max_tokens,
            messages=messages,
            tools=tools,
            betas=[beta_flag],
            thinking=thinking
        )

        # Add Claude's response to the conversation history
        response_content = response.content
        messages.append({"role": "assistant", "content": response_content})

        # Check if Claude used any tools
        tool_results = []
        for block in response_content:
            if block.type == "tool_use":
                # In a real app, you would execute the tool here
                # For example: result = run_tool(block.name, block.input)
                result = {"result": "Tool executed successfully"}

                # Format the result for Claude
                tool_results.append({
                    "type": "tool_result",
                    "tool_use_id": block.id,
                    "content": result
                })

        # If no tools were used, Claude is done - return the final messages
        if not tool_results:
            return messages

        # Add tool results to messages for the next iteration with Claude
        messages.append({"role": "user", "content": tool_results})
```The loop continues until either Claude responds without requesting any tools (task completion) or the maximum iteration limit is reached. This safeguard prevents potential infinite loops that could result in unexpected API costs.

<Warning>
 When using the computer use tool, you must include the appropriate beta flag for your model version:

 <AccordionGroup>
 <Accordion title="Claude 4 models">
 When using `computer_20250124`, include this beta flag:```
      "betas": ["computer-use-2025-01-24"]
      ```</Accordion>

 <Accordion title="Claude Sonnet 3.7">
 When using `computer_20250124`, include this beta flag:```
      "betas": ["computer-use-2025-01-24"]
      ```</Accordion>
 </AccordionGroup>
</Warning>

We recommend trying the reference implementation out before reading the rest of this documentation.

### Optimize model performance with prompting

Here are some tips on how to get the best quality outputs:

1. Specify simple, well-defined tasks and provide explicit instructions for each step.
2. Claude sometimes assumes outcomes of its actions without explicitly checking their results. To prevent this you can prompt Claude with `After each step, take a screenshot and carefully evaluate if you have achieved the right outcome. Explicitly show your thinking: "I have evaluated step X..." If not correct, try again. Only when you confirm a step was executed correctly should you move on to the next one.`
3. Some UI elements (like dropdowns and scrollbars) might be tricky for Claude to manipulate using mouse movements. If you experience this, try prompting the model to use keyboard shortcuts.
4. For repeatable tasks or UI interactions, include example screenshots and tool calls of successful outcomes in your prompt.
5. If you need the model to log in, provide it with the username and password in your prompt inside xml tags like `<robot_credentials>`. Using computer use within applications that require login increases the risk of bad outcomes as a result of prompt injection. Please review our [guide on mitigating prompt injections](/en/docs/test-and-evaluate/strengthen-guardrails/mitigate-jailbreaks) before providing the model with login credentials.

<Tip>
 If you repeatedly encounter a clear set of issues or know in advance the tasks
 Claude will need to complete, use the system prompt to provide Claude with
 explicit tips or instructions on how to do the tasks successfully.
</Tip>

### System prompts

When one of the Anthropic-defined tools is requested via the Claude API, a computer use-specific system prompt is generated. It's similar to the [tool use system prompt](/en/docs/agents-and-tools/tool-use/implement-tool-use#tool-use-system-prompt) but starts with:

> You have access to a set of functions you can use to answer the user's question. This includes access to a sandboxed computing environment. You do NOT currently have the ability to inspect files or interact with external resources, except by invoking the below functions.

As with regular tool use, the user-provided `system_prompt` field is still respected and used in the construction of the combined system prompt.

### Available actions

The computer use tool supports these actions:

**Basic actions (all versions)**

* **screenshot** - Capture the current display
* **left\_click** - Click at coordinates `[x, y]`
* **type** - Type text string
* **key** - Press key or key combination (e.g., "ctrl+s")
* **mouse\_move** - Move cursor to coordinates

**Enhanced actions (`computer_20250124`)**
Available in Claude 4 models and Claude Sonnet 3.7:

* **scroll** - Scroll in any direction with amount control
* **left\_click\_drag** - Click and drag between coordinates
* **right\_click**, **middle\_click** - Additional mouse buttons
* **double\_click**, **triple\_click** - Multiple clicks
* **left\_mouse\_down**, **left\_mouse\_up** - Fine-grained click control
* **hold\_key** - Hold a key while performing other actions
* **wait** - Pause between actions

<Accordion title="Example actions">```json  theme={null}
  // Take a screenshot
  {
    "action": "screenshot"
  }

  // Click at position
  {
    "action": "left_click",
    "coordinate": [500, 300]
  }

  // Type text
  {
    "action": "type",
    "text": "Hello, world!"
  }

  // Scroll down (Claude 4/3.7)
  {
    "action": "scroll",
    "coordinate": [500, 400],
    "scroll_direction": "down",
    "scroll_amount": 3
  }
  ```</Accordion>

### Tool parameters

| Parameter | Required | Description |
| ------------------- | -------- | --------------------------------------------------------- |
| `type` | Yes | Tool version (`computer_20250124` or `computer_20241022`) |
| `name` | Yes | Must be "computer" |
| `display_width_px` | Yes | Display width in pixels |
| `display_height_px` | Yes | Display height in pixels |
| `display_number` | No | Display number for X11 environments |

<Warning>
 Keep display resolution at or below 1280x800 (WXGA) for best performance. Higher resolutions may cause accuracy issues due to [image resizing](/en/docs/build-with-claude/vision#evaluate-image-size).
</Warning>

<Note>
 **Important**: The computer use tool must be explicitly executed by your application - Claude cannot execute it directly. You are responsible for implementing the screenshot capture, mouse movements, keyboard inputs, and other actions based on Claude's requests.
</Note>

### Enable thinking capability in Claude 4 models and Claude Sonnet 3.7

Claude Sonnet 3.7 introduced a new "thinking" capability that allows you to see the model's reasoning process as it works through complex tasks. This feature helps you understand how Claude is approaching a problem and can be particularly valuable for debugging or educational purposes.

To enable thinking, add a `thinking` parameter to your API request:```json  theme={null}
"thinking": {
  "type": "enabled",
  "budget_tokens": 1024
}
```The `budget_tokens` parameter specifies how many tokens Claude can use for thinking. This is subtracted from your overall `max_tokens` budget.

When thinking is enabled, Claude will return its reasoning process as part of the response, which can help you:

1. Understand the model's decision-making process
2. Identify potential issues or misconceptions
3. Learn from Claude's approach to problem-solving
4. Get more visibility into complex multi-step operations

Here's an example of what thinking output might look like:```
[Thinking]
I need to save a picture of a cat to the desktop. Let me break this down into steps:

1. First, I'll take a screenshot to see what's on the desktop
2. Then I'll look for a web browser to search for cat images
3. After finding a suitable image, I'll need to save it to the desktop

Let me start by taking a screenshot to see what's available...
```### Augmenting computer use with other tools

The computer use tool can be combined with other tools to create more powerful automation workflows. This is particularly useful when you need to:

* Execute system commands ([bash tool](/en/docs/agents-and-tools/tool-use/bash-tool))
* Edit configuration files or scripts ([text editor tool](/en/docs/agents-and-tools/tool-use/text-editor-tool))
* Integrate with custom APIs or services (custom tools)

<CodeGroup>```bash Shell theme={null}
  curl https://api.anthropic.com/v1/messages \
    -H "content-type: application/json" \
    -H "x-api-key: $ANTHROPIC_API_KEY" \
    -H "anthropic-version: 2023-06-01" \
    -H "anthropic-beta: computer-use-2025-01-24" \
    -d '{
      "model": "claude-sonnet-4-5",
      "max_tokens": 2000,
      "tools": [
        {
          "type": "computer_20250124",
          "name": "computer",
          "display_width_px": 1024,
          "display_height_px": 768,
          "display_number": 1
        },
        {
          "type": "text_editor_20250124",
          "name": "str_replace_editor"
        },
        {
          "type": "bash_20250124",
          "name": "bash"
        },
        {
          "name": "get_weather",
          "description": "Get the current weather in a given location",
          "input_schema": {
            "type": "object",
            "properties": {
              "location": {
                "type": "string",
                "description": "The city and state, e.g. San Francisco, CA"
              },
              "unit": {
                "type": "string",
                "enum": ["celsius", "fahrenheit"],
                "description": "The unit of temperature, either 'celsius' or 'fahrenheit'"
              }
            },
            "required": ["location"]
          }
        }
      ],
      "messages": [
        {
          "role": "user",
          "content": "Find flights from San Francisco to a place with warmer weather."
        }
      ],
      "thinking": {
        "type": "enabled",
        "budget_tokens": 1024
      }
    }'
  ``````Python Python theme={null}
  import anthropic

  client = anthropic.Anthropic()

  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=1024,
      tools=[
          {
            "type": "computer_20250124",
            "name": "computer",
            "display_width_px": 1024,
            "display_height_px": 768,
            "display_number": 1,
          },
          {
            "type": "text_editor_20250124",
            "name": "str_replace_editor"
          },
          {
            "type": "bash_20250124",
            "name": "bash"
          },
          {
            "name": "get_weather",
            "description": "Get the current weather in a given location",
            "input_schema": {
              "type": "object",
              "properties": {
                "location": {
                  "type": "string",
                  "description": "The city and state, e.g. San Francisco, CA"
                },
                "unit": {
                  "type": "string",
                  "enum": ["celsius", "fahrenheit"],
                  "description": "The unit of temperature, either 'celsius' or 'fahrenheit'"
                }
              },
              "required": ["location"]
            }
          },
      ],
      messages=[{"role": "user", "content": "Find flights from San Francisco to a place with warmer weather."}],
      betas=["computer-use-2025-01-24"],
      thinking={"type": "enabled", "budget_tokens": 1024},
  )
  print(response)
  ``````TypeScript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic();

  const message = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 1024,
    tools: [
        {
          type: "computer_20250124",
          name: "computer",
          display_width_px: 1024,
          display_height_px: 768,
          display_number: 1,
        },
        {
          type: "text_editor_20250124",
          name: "str_replace_editor"
        },
        {
          type: "bash_20250124",
          name: "bash"
        },
        {
          name: "get_weather",
          description: "Get the current weather in a given location",
          input_schema: {
            type: "object",
            properties: {
              location: {
                type: "string",
                description: "The city and state, e.g. San Francisco, CA"
              },
              unit: {
                type: "string",
                enum: ["celsius", "fahrenheit"],
                description: "The unit of temperature, either 'celsius' or 'fahrenheit'"
              }
            },
            required: ["location"]
          }
        },
    ],
    messages: [{ role: "user", content: "Find flights from San Francisco to a place with warmer weather." }],
    betas: ["computer-use-2025-01-24"],
    thinking: { type: "enabled", budget_tokens: 1024 },
  });
  console.log(message);
  ``````java Java theme={null}
  import java.util.List;
  import java.util.Map;

  import com.anthropic.client.AnthropicClient;
  import com.anthropic.client.okhttp.AnthropicOkHttpClient;
  import com.anthropic.core.JsonValue;
  import com.anthropic.models.beta.messages.BetaMessage;
  import com.anthropic.models.beta.messages.MessageCreateParams;
  import com.anthropic.models.beta.messages.BetaToolBash20250124;
  import com.anthropic.models.beta.messages.BetaToolComputerUse20250124;
  import com.anthropic.models.beta.messages.BetaToolTextEditor20250124;
  import com.anthropic.models.beta.messages.BetaThinkingConfigEnabled;
  import com.anthropic.models.beta.messages.BetaThinkingConfigParam;
  import com.anthropic.models.beta.messages.BetaTool;

  public class MultipleToolsExample {

      public static void main(String[] args) {
          AnthropicClient client = AnthropicOkHttpClient.fromEnv();

          MessageCreateParams params = MessageCreateParams.builder()
                  .model("claude-sonnet-4-5")
                  .maxTokens(1024)
                  .addTool(BetaToolComputerUse20250124.builder()
                          .displayWidthPx(1024)
                          .displayHeightPx(768)
                          .displayNumber(1)
                          .build())
                  .addTool(BetaToolTextEditor20250124.builder()
                          .build())
                  .addTool(BetaToolBash20250124.builder()
                          .build())
                  .addTool(BetaTool.builder()
                          .name("get_weather")
                          .description("Get the current weather in a given location")
                          .inputSchema(BetaTool.InputSchema.builder()
                                  .properties(
                                          JsonValue.from(
                                                  Map.of(
                                                          "location", Map.of(
                                                                  "type", "string",
                                                                  "description", "The city and state, e.g. San Francisco, CA"
                                                          ),
                                                          "unit", Map.of(
                                                                  "type", "string",
                                                                  "enum", List.of("celsius", "fahrenheit"),
                                                                  "description", "The unit of temperature, either 'celsius' or 'fahrenheit'"
                                                          )
                                                  )
                                          ))
                                  .build()
                          )
                          .build())
                  .thinking(BetaThinkingConfigParam.ofEnabled(
                          BetaThinkingConfigEnabled.builder()
                                  .budgetTokens(1024)
                                  .build()
                  ))
                  .addUserMessage("Find flights from San Francisco to a place with warmer weather.")
                  .addBeta("computer-use-2025-01-24")
                  .build();

          BetaMessage message = client.beta().messages().create(params);
          System.out.println(message);
      }
  }
  ```</CodeGroup>

### Build a custom computer use environment

The [reference implementation](https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo) is meant to help you get started with computer use. It includes all of the components needed have Claude use a computer. However, you can build your own environment for computer use to suit your needs. You'll need:

* A virtualized or containerized environment suitable for computer use with Claude
* An implementation of at least one of the Anthropic-defined computer use tools
* An agent loop that interacts with the Claude API and executes the `tool_use` results using your tool implementations
* An API or UI that allows user input to start the agent loop

#### Implement the computer use tool

The computer use tool is implemented as a schema-less tool. When using this tool, you don't need to provide an input schema as with other tools; the schema is built into Claude's model and can't be modified.

<Steps>
 <Step title="Set up your computing environment">
 Create a virtual display or connect to an existing display that Claude will interact with. This typically involves setting up Xvfb (X Virtual Framebuffer) or similar technology.
 </Step>

 <Step title="Implement action handlers">
 Create functions to handle each action type that Claude might request:```python  theme={null}
    def handle_computer_action(action_type, params):
        if action_type == "screenshot":
            return capture_screenshot()
        elif action_type == "left_click":
            x, y = params["coordinate"]
            return click_at(x, y)
        elif action_type == "type":
            return type_text(params["text"])
        # ... handle other actions
    ```</Step>

 <Step title="Process Claude's tool calls">
 Extract and execute tool calls from Claude's responses:```python  theme={null}
    for content in response.content:
        if content.type == "tool_use":
            action = content.input["action"]
            result = handle_computer_action(action, content.input)
            
            # Return result to Claude
            tool_result = {
                "type": "tool_result",
                "tool_use_id": content.id,
                "content": result
            }
    ```</Step>

 <Step title="Implement the agent loop">
 Create a loop that continues until Claude completes the task:```python  theme={null}
    while True:
        response = client.beta.messages.create(...)
        
        # Check if Claude used any tools
        tool_results = process_tool_calls(response)
        
        if not tool_results:
            # No more tool use, task complete
            break
            
        # Continue conversation with tool results
        messages.append({"role": "user", "content": tool_results})
    ```</Step>
</Steps>

#### Handle errors

When implementing the computer use tool, various errors may occur. Here's how to handle them:

<AccordionGroup>
 <Accordion title="Screenshot capture failure">
 If screenshot capture fails, return an appropriate error message:```json  theme={null}
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01A09q90qw90lq917835lq9",
          "content": "Error: Failed to capture screenshot. Display may be locked or unavailable.",
          "is_error": true
        }
      ]
    }
    ```</Accordion>

 <Accordion title="Invalid coordinates">
 If Claude provides coordinates outside the display bounds:```json  theme={null}
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01A09q90qw90lq917835lq9",
          "content": "Error: Coordinates (1200, 900) are outside display bounds (1024x768).",
          "is_error": true
        }
      ]
    }
    ```</Accordion>

 <Accordion title="Action execution failure">
 If an action fails to execute:```json  theme={null}
    {
      "role": "user",
      "content": [
        {
          "type": "tool_result",
          "tool_use_id": "toolu_01A09q90qw90lq917835lq9",
          "content": "Error: Failed to perform click action. The application may be unresponsive.",
          "is_error": true
        }
      ]
    }
    ```</Accordion>
</AccordionGroup>

#### Follow implementation best practices

<AccordionGroup>
 <Accordion title="Use appropriate display resolution">
 Set display dimensions that match your use case while staying within recommended limits:

 * For general desktop tasks: 1024x768 or 1280x720
 * For web applications: 1280x800 or 1366x768
 * Avoid resolutions above 1920x1080 to prevent performance issues
 </Accordion>

 <Accordion title="Implement proper screenshot handling">
 When returning screenshots to Claude:

 * Encode screenshots as base64 PNG or JPEG
 * Consider compressing large screenshots to improve performance
 * Include relevant metadata like timestamp or display state
 </Accordion>

 <Accordion title="Add action delays">
 Some applications need time to respond to actions:```python  theme={null}
    def click_and_wait(x, y, wait_time=0.5):
        click_at(x, y)
        time.sleep(wait_time)  # Allow UI to update
    ```</Accordion>

 <Accordion title="Validate actions before execution">
 Check that requested actions are safe and valid:```python  theme={null}
    def validate_action(action_type, params):
        if action_type == "left_click":
            x, y = params.get("coordinate", (0, 0))
            if not (0 <= x < display_width and 0 <= y < display_height):
                return False, "Coordinates out of bounds"
        return True, None
    ```</Accordion>

 <Accordion title="Log actions for debugging">
 Keep a log of all actions for troubleshooting:```python  theme={null}
    import logging

    def log_action(action_type, params, result):
        logging.info(f"Action: {action_type}, Params: {params}, Result: {result}")
    ```</Accordion>
</AccordionGroup>

***

## Understand computer use limitations

The computer use functionality is in beta. While Claude's capabilities are cutting edge, developers should be aware of its limitations:

1. **Latency**: the current computer use latency for human-AI interactions may be too slow compared to regular human-directed computer actions. We recommend focusing on use cases where speed isn't critical (e.g., background information gathering, automated software testing) in trusted environments.
2. **Computer vision accuracy and reliability**: Claude may make mistakes or hallucinate when outputting specific coordinates while generating actions. Claude Sonnet 3.7 introduces the thinking capability that can help you understand the model's reasoning and identify potential issues.
3. **Tool selection accuracy and reliability**: Claude may make mistakes or hallucinate when selecting tools while generating actions or take unexpected actions to solve problems. Additionally, reliability may be lower when interacting with niche applications or multiple applications at once. We recommend that users prompt the model carefully when requesting complex tasks.
4. **Scrolling reliability**: Claude Sonnet 3.7 introduced dedicated scroll actions with direction control that improves reliability. The model can now explicitly scroll in any direction (up/down/left/right) by a specified amount.
5. **Spreadsheet interaction**: Mouse clicks for spreadsheet interaction have improved in Claude Sonnet 3.7 with the addition of more precise mouse control actions like `left_mouse_down`, `left_mouse_up`, and new modifier key support. Cell selection can be more reliable by using these fine-grained controls and combining modifier keys with clicks.
6. **Account creation and content generation on social and communications platforms**: While Claude will visit websites, we are limiting its ability to create accounts or generate and share content or otherwise engage in human impersonation across social media websites and platforms. We may update this capability in the future.
7. **Vulnerabilities**: Vulnerabilities like jailbreaking or prompt injection may persist across frontier AI systems, including the beta computer use API. In some circumstances, Claude will follow commands found in content, sometimes even in conflict with the user's instructions. For example, Claude instructions on webpages or contained in images may override instructions or cause Claude to make mistakes. We recommend:
 a. Limiting computer use to trusted environments such as virtual machines or containers with minimal privileges
 b. Avoiding giving computer use access to sensitive accounts or data without strict oversight
 c. Informing end users of relevant risks and obtaining their consent before enabling or requesting permissions necessary for computer use features in your applications
8. **Inappropriate or illegal actions**: Per Anthropic's terms of service, you must not employ computer use to violate any laws or our Acceptable Use Policy.

Always carefully review and verify Claude's computer use actions and logs. Do not use Claude for tasks requiring perfect precision or sensitive user information without human oversight.

***

## Pricing

Computer use follows the standard [tool use pricing](/en/docs/agents-and-tools/tool-use/overview#pricing). When using the computer use tool:

**System prompt overhead**: The computer use beta adds 466-499 tokens to the system prompt

**Computer use tool token usage**:

| Model | Input tokens per tool definition |
| -------------------------------------------------------------------------- | -------------------------------- |
| Claude 4.x models | 735 tokens |
| Claude Sonnet 3.7 ([deprecated](/en/docs/about-claude/model-deprecations)) | 735 tokens |

**Additional token consumption**:

* Screenshot images (see [Vision pricing](/en/docs/build-with-claude/vision))
* Tool execution results returned to Claude

<Note>
 If you're also using bash or text editor tools alongside computer use, those tools have their own token costs as documented in their respective pages.
</Note>

## Next steps

<CardGroup cols={2}>
 <Card title="Reference implementation" icon="github" href="https://github.com/anthropics/anthropic-quickstarts/tree/main/computer-use-demo">
 Get started quickly with our complete Docker-based implementation
 </Card>

 <Card title="Tool documentation" icon="toolbox" href="/en/docs/agents-and-tools/tool-use/overview">
 Learn more about tool use and creating custom tools
 </Card>
</CardGroup>

[END OF DOCUMENT: CLAUDE9QDUKWN9W]
---

[START OF DOCUMENT: CLAUDE1WX3OH4GDJ | Title: Context-Editing]

# Context editing

> Automatically manage conversation context as it grows with context editing.

<Note>
 Context editing is currently in beta with support for tool result clearing and thinking block clearing. To enable it, use the beta header `context-management-2025-06-27` in your API requests.

 Please reach out through our [feedback form](https://forms.gle/YXC2EKGMhjN1c4L88) to share your feedback on this feature.
</Note>

## Overview

Context editing allows you to automatically manage conversation context as it grows, helping you optimize costs and stay within context window limits. The API provides different strategies for managing context:

* **Tool result clearing** (`clear_tool_uses_20250919`): Automatically clears tool use/result pairs when conversation context exceeds your configured threshold
* **Thinking block clearing** (`clear_thinking_20251015`): Manages [thinking blocks](/en/docs/build-with-claude/extended-thinking) by clearing older thinking blocks from previous turns

Each strategy can be configured independently and applied together to optimize your specific use case.

## Context editing strategies

### Tool result clearing

The `clear_tool_uses_20250919` strategy clears tool results when conversation context grows beyond your configured threshold. When activated, the API automatically clears the oldest tool results in chronological order, replacing them with placeholder text to let Claude know the tool result was removed. By default, only tool results are cleared. You can optionally clear both tool results and tool calls (the tool use parameters) by setting `clear_tool_inputs` to true.

### Thinking block clearing

The `clear_thinking_20251015` strategy manages `thinking` blocks in conversations when extended thinking is enabled. This strategy automatically clears older thinking blocks from previous turns.

<Tip>
 **Default behavior**: When extended thinking is enabled without configuring the `clear_thinking_20251015` strategy, the API automatically keeps only the thinking blocks from the last assistant turn (equivalent to `keep: {type: "thinking_turns", value: 1}`).

 To maximize cache hits, preserve all thinking blocks by setting `keep: "all"`.
</Tip>

<Note>
 An assistant conversation turn may include multiple content blocks (e.g. when using tools) and multiple thinking blocks (e.g. with [interleaved thinking](/en/docs/build-with-claude/extended-thinking#interleaved-thinking)).
</Note>

<Tip>
 **Context editing happens server-side**

 Context editing is applied **server-side** before the prompt reaches Claude. Your client application maintains the full, unmodified conversation history—you do not need to sync your client state with the edited version. Continue managing your full conversation history locally as you normally would.
</Tip>

<Tip>
 **Context editing and prompt caching**

 Context editing's interaction with [prompt caching](/en/docs/build-with-claude/prompt-caching) varies by strategy:

 * **Tool result clearing**: Invalidates cached prompt prefixes when content is cleared. To account for this, we recommend clearing enough tokens to make the cache invalidation worthwhile. Use the `clear_at_least` parameter to ensure a minimum number of tokens is cleared each time. You'll incur cache write costs each time content is cleared, but subsequent requests can reuse the newly cached prefix.

 * **Thinking block clearing**: When thinking blocks are **kept** in context (not cleared), the prompt cache is preserved, enabling cache hits and reducing input token costs. When thinking blocks are **cleared**, the cache is invalidated at the point where clearing occurs. Configure the `keep` parameter based on whether you want to prioritize cache performance or context window availability.
</Tip>

## Supported models

Context editing is available on:

* Claude Opus 4.1 (`claude-opus-4-1-20250805`)
* Claude Opus 4 (`claude-opus-4-20250514`)
* Claude Sonnet 4.5 (`claude-sonnet-4-5-20250929`)
* Claude Sonnet 4 (`claude-sonnet-4-20250514`)
* Claude Haiku 4.5 (`claude-haiku-4-5-20251001`)

## Tool result clearing usage

The simplest way to enable tool result clearing is to specify only the strategy type, as all other [configuration options](#configuration-options-for-tool-result-clearing) will use their default values:

<CodeGroup>```bash cURL theme={null}
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "content-type: application/json" \
      --header "anthropic-beta: context-management-2025-06-27" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [
              {
                  "role": "user",
                  "content": "Search for recent developments in AI"
              }
          ],
          "tools": [
              {
                  "type": "web_search_20250305",
                  "name": "web_search"
              }
          ],
          "context_management": {
              "edits": [
                  {"type": "clear_tool_uses_20250919"}
              ]
          }
      }'
  ``````python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=4096,
      messages=[
          {
              "role": "user",
              "content": "Search for recent developments in AI"
          }
      ],
      tools=[
          {
              "type": "web_search_20250305",
              "name": "web_search"
          }
      ],
      betas=["context-management-2025-06-27"],
      context_management={
          "edits": [
              {"type": "clear_tool_uses_20250919"}
          ]
      }
  )
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 4096,
    messages: [
      {
        role: "user",
        content: "Search for recent developments in AI"
      }
    ],
    tools: [
      {
        type: "web_search_20250305",
        name: "web_search"
      }
    ],
    context_management: {
      edits: [
        { type: "clear_tool_uses_20250919" }
      ]
    },
    betas: ["context-management-2025-06-27"]
  });
  ```</CodeGroup>

### Advanced configuration

You can customize the tool result clearing behavior with additional parameters:

<CodeGroup>```bash cURL theme={null}
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "content-type: application/json" \
      --header "anthropic-beta: context-management-2025-06-27" \
      --data '{
          "model": "claude-sonnet-4-5",
          "max_tokens": 4096,
          "messages": [
              {
                  "role": "user",
                  "content": "Create a simple command line calculator app using Python"
              }
          ],
          "tools": [
              {
                  "type": "text_editor_20250728",
                  "name": "str_replace_based_edit_tool",
                  "max_characters": 10000
              },
              {
                  "type": "web_search_20250305",
                  "name": "web_search",
                  "max_uses": 3
              }
          ],
          "context_management": {
              "edits": [
                  {
                      "type": "clear_tool_uses_20250919",
                      "trigger": {
                          "type": "input_tokens",
                          "value": 30000
                      },
                      "keep": {
                          "type": "tool_uses",
                          "value": 3
                      },
                      "clear_at_least": {
                          "type": "input_tokens",
                          "value": 5000
                      },
                      "exclude_tools": ["web_search"]
                  }
              ]
          }
      }'
  ``````python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=4096,
      messages=[
          {
              "role": "user",
              "content": "Create a simple command line calculator app using Python"
          }
      ],
      tools=[
          {
              "type": "text_editor_20250728",
              "name": "str_replace_based_edit_tool",
              "max_characters": 10000
          },
          {
              "type": "web_search_20250305",
              "name": "web_search",
              "max_uses": 3
          }
      ],
      betas=["context-management-2025-06-27"],
      context_management={
          "edits": [
              {
                  "type": "clear_tool_uses_20250919",
                  # Trigger clearing when threshold is exceeded
                  "trigger": {
                      "type": "input_tokens",
                      "value": 30000
                  },
                  # Number of tool uses to keep after clearing
                  "keep": {
                      "type": "tool_uses",
                      "value": 3
                  },
                  # Optional: Clear at least this many tokens
                  "clear_at_least": {
                      "type": "input_tokens",
                      "value": 5000
                  },
                  # Exclude these tools from being cleared
                  "exclude_tools": ["web_search"]
              }
          ]
      }
  )
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 4096,
    messages: [
      {
        role: "user",
        content: "Create a simple command line calculator app using Python"
      }
    ],
    tools: [
      {
        type: "text_editor_20250728",
        name: "str_replace_based_edit_tool",
        max_characters: 10000
      },
      {
        type: "web_search_20250305",
        name: "web_search",
        max_uses: 3
      }
    ],
    betas: ["context-management-2025-06-27"],
    context_management: {
      edits: [
        {
          type: "clear_tool_uses_20250919",
          // Trigger clearing when threshold is exceeded
          trigger: {
            type: "input_tokens",
            value: 30000
          },
          // Number of tool uses to keep after clearing
          keep: {
            type: "tool_uses",
            value: 3
          },
          // Optional: Clear at least this many tokens
          clear_at_least: {
            type: "input_tokens",
            value: 5000
          },
          // Exclude these tools from being cleared
          exclude_tools: ["web_search"]
        }
      ]
    }
  });
  ```</CodeGroup>

## Thinking block clearing usage

Enable thinking block clearing to manage context and prompt caching effectively when extended thinking is enabled:

<CodeGroup>```bash cURL theme={null}
  curl https://api.anthropic.com/v1/messages \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "content-type: application/json" \
      --header "anthropic-beta: context-management-2025-06-27" \
      --data '{
          "model": "claude-sonnet-4-5-20250929",
          "max_tokens": 1024,
          "messages": [...],
          "thinking": {
              "type": "enabled",
              "budget_tokens": 10000
          },
          "context_management": {
              "edits": [
                  {
                      "type": "clear_thinking_20251015",
                      "keep": {
                          "type": "thinking_turns",
                          "value": 2
                      }
                  }
              ]
          }
      }'
  ``````python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5-20250929",
      max_tokens=1024,
      messages=[...],
      thinking={
          "type": "enabled",
          "budget_tokens": 10000
      },
      betas=["context-management-2025-06-27"],
      context_management={
          "edits": [
              {
                  "type": "clear_thinking_20251015",
                  "keep": {
                      "type": "thinking_turns",
                      "value": 2
                  }
              }
          ]
      }
  )
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5-20250929",
    max_tokens: 1024,
    messages: [...],
    thinking: {
      type: "enabled",
      budget_tokens: 10000
    },
    betas: ["context-management-2025-06-27"],
    context_management: {
      edits: [
        {
          type: "clear_thinking_20251015",
          keep: {
            type: "thinking_turns",
            value: 2
          }
        }
      ]
    }
  });
  ```</CodeGroup>

### Configuration options for thinking block clearing

The `clear_thinking_20251015` strategy supports the following configuration:

| Configuration option | Default | Description |
| -------------------- | ------------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `keep` | `{type: "thinking_turns", value: 1}` | Defines how many recent assistant turns with thinking blocks to preserve. Use `{type: "thinking_turns", value: N}` where N must be > 0 to keep the last N turns, or `"all"` to keep all thinking blocks. |

**Example configurations:**```json  theme={null}
// Keep thinking blocks from the last 3 assistant turns
{
  "type": "clear_thinking_20251015",
  "keep": {
    "type": "thinking_turns",
    "value": 3
  }
}

// Keep all thinking blocks (maximizes cache hits)
{
  "type": "clear_thinking_20251015",
  "keep": "all"
}
```### Combining strategies

You can use both thinking block clearing and tool result clearing together:

<Note>
 When using multiple strategies, the `clear_thinking_20251015` strategy must be listed first in the `edits` array.
</Note>

<CodeGroup>```python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5-20250929",
      max_tokens=1024,
      messages=[...],
      thinking={
          "type": "enabled",
          "budget_tokens": 10000
      },
      tools=[...],
      betas=["context-management-2025-06-27"],
      context_management={
          "edits": [
              {
                  "type": "clear_thinking_20251015",
                  "keep": {
                      "type": "thinking_turns",
                      "value": 2
                  }
              },
              {
                  "type": "clear_tool_uses_20250919",
                  "trigger": {
                      "type": "input_tokens",
                      "value": 50000
                  },
                  "keep": {
                      "type": "tool_uses",
                      "value": 5
                  }
              }
          ]
      }
  )
  ``````typescript TypeScript theme={null}
  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5-20250929",
    max_tokens: 1024,
    messages: [...],
    thinking: {
      type: "enabled",
      budget_tokens: 10000
    },
    tools: [...],
    betas: ["context-management-2025-06-27"],
    context_management: {
      edits: [
        {
          type: "clear_thinking_20251015",
          keep: {
            type: "thinking_turns",
            value: 2
          }
        },
        {
          type: "clear_tool_uses_20250919",
          trigger: {
            type: "input_tokens",
            value: 50000
          },
          keep: {
            type: "tool_uses",
            value: 5
          }
        }
      ]
    }
  });
  ```</CodeGroup>

## Configuration options for tool result clearing

| Configuration option | Default | Description |
| -------------------- | -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `trigger` | 100,000 input tokens | Defines when the context editing strategy activates. Once the prompt exceeds this threshold, clearing will begin. You can specify this value in either `input_tokens` or `tool_uses`. |
| `keep` | 3 tool uses | Defines how many recent tool use/result pairs to keep after clearing occurs. The API removes the oldest tool interactions first, preserving the most recent ones. |
| `clear_at_least` | None | Ensures a minimum number of tokens is cleared each time the strategy activates. If the API can't clear at least the specified amount, the strategy will not be applied. This helps determine if context clearing is worth breaking your prompt cache. |
| `exclude_tools` | None | List of tool names whose tool uses and results should never be cleared. Useful for preserving important context. |
| `clear_tool_inputs` | `false` | Controls whether the tool call parameters are cleared along with the tool results. By default, only the tool results are cleared while keeping Claude's original tool calls visible. |

## Context editing response

You can see which context edits were applied to your request using the `context_management` response field, along with helpful statistics about the content and input tokens cleared.```json Response theme={null}
{
    "id": "msg_013Zva2CMHLNnXjNJJKqJ2EF",
    "type": "message",
    "role": "assistant",
    "content": [...],
    "usage": {...},
    "context_management": {
        "applied_edits": [
            // When using `clear_thinking_20251015`
            {
                "type": "clear_thinking_20251015",
                "cleared_thinking_turns": 3,
                "cleared_input_tokens": 15000
            },
            // When using `clear_tool_uses_20250919`
            {
                "type": "clear_tool_uses_20250919",
                "cleared_tool_uses": 8,
                "cleared_input_tokens": 50000
            }
        ]
    }
}
```For streaming responses, the context edits will be included in the final `message_delta` event:```json Streaming Response theme={null}
{
    "type": "message_delta",
    "delta": {
        "stop_reason": "end_turn",
        "stop_sequence": null
    },
    "usage": {
        "output_tokens": 1024
    },
    "context_management": {
        "applied_edits": [...]
    }
}
```## Token counting

The [token counting](/en/docs/build-with-claude/token-counting) endpoint supports context management, allowing you to preview how many tokens your prompt will use after context editing is applied.

<CodeGroup>```bash cURL theme={null}
  curl https://api.anthropic.com/v1/messages/count_tokens \
      --header "x-api-key: $ANTHROPIC_API_KEY" \
      --header "anthropic-version: 2023-06-01" \
      --header "content-type: application/json" \
      --header "anthropic-beta: context-management-2025-06-27" \
      --data '{
          "model": "claude-sonnet-4-5",
          "messages": [
              {
                  "role": "user",
                  "content": "Continue our conversation..."
              }
          ],
          "tools": [...],
          "context_management": {
              "edits": [
                  {
                      "type": "clear_tool_uses_20250919",
                      "trigger": {
                          "type": "input_tokens",
                          "value": 30000
                      },
                      "keep": {
                          "type": "tool_uses",
                          "value": 5
                      }
                  }
              ]
          }
      }'
  ``````python Python theme={null}
  response = client.beta.messages.count_tokens(
      model="claude-sonnet-4-5",
      messages=[
          {
              "role": "user",
              "content": "Continue our conversation..."
          }
      ],
      tools=[...],  # Your tool definitions
      betas=["context-management-2025-06-27"],
      context_management={
          "edits": [
              {
                  "type": "clear_tool_uses_20250919",
                  "trigger": {
                      "type": "input_tokens",
                      "value": 30000
                  },
                  "keep": {
                      "type": "tool_uses",
                      "value": 5
                  }
              }
          ]
      }
  )

  print(f"Original tokens: {response.context_management['original_input_tokens']}")
  print(f"After clearing: {response.input_tokens}")
  print(f"Savings: {response.context_management['original_input_tokens'] - response.input_tokens} tokens")
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  const response = await anthropic.beta.messages.countTokens({
    model: "claude-sonnet-4-5",
    messages: [
      {
        role: "user",
        content: "Continue our conversation..."
      }
    ],
    tools: [...],  // Your tool definitions
    betas: ["context-management-2025-06-27"],
    context_management: {
      edits: [
        {
          type: "clear_tool_uses_20250919",
          trigger: {
            type: "input_tokens",
            value: 30000
          },
          keep: {
            type: "tool_uses",
            value: 5
          }
        }
      ]
    }
  });

  console.log(`Original tokens: ${response.context_management?.original_input_tokens}`);
  console.log(`After clearing: ${response.input_tokens}`);
  console.log(`Savings: ${(response.context_management?.original_input_tokens || 0) - response.input_tokens} tokens`);
  ```</CodeGroup>```json Response theme={null}
{
    "input_tokens": 25000,
    "context_management": {
        "original_input_tokens": 70000
    }
}
```The response shows both the final token count after context management is applied (`input_tokens`) and the original token count before any clearing occurred (`original_input_tokens`).

## Using with the Memory Tool

Context editing can be combined with the [memory tool](/en/docs/agents-and-tools/tool-use/memory-tool). When your conversation context approaches the configured clearing threshold, Claude receives an automatic warning to preserve important information. This enables Claude to save tool results or context to its memory files before they're cleared from the conversation history.

This combination allows you to:

* **Preserve important context**: Claude can write essential information from tool results to memory files before those results are cleared
* **Maintain long-running workflows**: Enable agentic workflows that would otherwise exceed context limits by offloading information to persistent storage
* **Access information on demand**: Claude can look up previously cleared information from memory files when needed, rather than keeping everything in the active context window

For example, in a file editing workflow where Claude performs many operations, Claude can summarize completed changes to memory files as the context grows. When tool results are cleared, Claude retains access to that information through its memory system and can continue working effectively.

To use both features together, enable them in your API request:

<CodeGroup>```python Python theme={null}
  response = client.beta.messages.create(
      model="claude-sonnet-4-5",
      max_tokens=4096,
      messages=[...],
      tools=[
          {
              "type": "memory_20250818",
              "name": "memory"
          },
          # Your other tools
      ],
      betas=["context-management-2025-06-27"],
      context_management={
          "edits": [
              {"type": "clear_tool_uses_20250919"}
          ]
      }
  )
  ``````typescript TypeScript theme={null}
  import Anthropic from '@anthropic-ai/sdk';

  const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
  });

  const response = await anthropic.beta.messages.create({
    model: "claude-sonnet-4-5",
    max_tokens: 4096,
    messages: [...],
    tools: [
      {
        type: "memory_20250818",
        name: "memory"
      },
      // Your other tools
    ],
    betas: ["context-management-2025-06-27"],
    context_management: {
      edits: [
        { type: "clear_tool_uses_20250919" }
      ]
    }
  });
  ```</CodeGroup>

[END OF DOCUMENT: CLAUDE1WX3OH4GDJ]
---

